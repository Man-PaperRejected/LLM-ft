{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB数据集，评论情感二分类微调\n",
    "\n",
    "1. 完成一个简单的模型下游任务训练过程\n",
    "2. 简单了解hf 一些常用库的用法\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载数据集\n",
    "使用 datasets 库加载 imdb 数据集。这个库会自动下载并缓存数据。\n",
    "也可以下载到本地，这样可以防止网络问题导致代码执行失败（尽管已经下载到缓存）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 25000 examples [00:00, 152530.13 examples/s]\n",
      "Generating test split: 25000 examples [00:00, 169834.63 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = \"/data/Dataset/LLM-dataset/imdb\" # 替换为本地目录，或者使用\"imdb\"自动下载\n",
    "raw_dataset = load_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里简单介绍一下数据集加载函数\n",
    "- load_dataset(path, name=None, data_dir=None, data_files=None, split=None, cache_dir=None, download_mode=None, ignore_verifications=False, keep_in_memory=False, features=None, **kwargs)\n",
    "  - 功能:  这是最核心的函数，用于从 Hugging Face Hub、本地文件或自定义脚本加载数据集。\n",
    "  - 参数:\n",
    "    - path:  数据集的名称 (例如 “rotten_tomatoes”, “glue”, “csv”, “json”) 或本地文件路径。\n",
    "    - name:  数据集的配置名称 (如果数据集有多个配置，例如 “glue” 数据集的 “cola”, “sst2” 等)。\n",
    "    - data_dir:  本地数据目录 (如果数据集存储在本地)。\n",
    "    - data_files:  指定要加载的数据文件 (例如 [\"train.csv\", \"test.csv\"])。  可以是字符串或字符串列表。\n",
    "    - split:  指定要加载的数据集分割 (例如 “train”, “validation”, “test”, “train[:10%]” 表示训练集的前 10%)。  可以是字符串或字符串列表。\n",
    "    - cache_dir:  指定数据集缓存目录。\n",
    "    - download_mode:  指定下载模式 (例如 “reuse_dataset_if_exists”, “force_redownload”)。\n",
    "    - ignore_verifications:  是否忽略数据集校验。\n",
    "    - keep_in_memory:  是否将数据集保存在内存中。\n",
    "    - features:  指定数据集的特征 (例如 Features({\"text\": Value(\"string\"), \"label\": ClassLabel(names=[\"neg\", \"pos\"])}))。\n",
    "    - **kwargs:  传递给数据集构建脚本的其他参数。\n",
    "  - 返回值:  一个 Dataset 对象 (如果只加载一个分割) 或一个 DatasetDict 对象 (如果加载多个分割)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "})\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 25000\n",
      "})\n",
      "{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.', 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "'''可以直接print 看一下数据集的概况'''\n",
    "print(raw_dataset)\n",
    "\n",
    "'''由于返回的是train 和 test 两个split 也可以直接像字典一样进行索引'''\n",
    "print(type(raw_dataset['train']))\n",
    "print(raw_dataset['train'])\n",
    "\n",
    "print(raw_dataset['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理 (Tokenization)\n",
    "接着我们需要加载 模型对应的分词器对数据进行处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1045, 12524, 1045, 2572, 8025, 1011, 3756, 2013, 2026, 2678, 3573, 2138, 1997, 2035, 1996, 6704, 2008, 5129, 2009, 2043, 2009, 2001, 2034, 2207, 1999, 3476, 1012, 1045, 2036, 2657, 2008, 2012, 2034, 2009, 2001, 8243, 2011, 1057, 1012, 1055, 1012, 8205, 2065, 2009, 2412, 2699, 2000, 4607, 2023, 2406, 1010, 3568, 2108, 1037, 5470, 1997, 3152, 2641, 1000, 6801, 1000, 1045, 2428, 2018, 2000, 2156, 2023, 2005, 2870, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 5436, 2003, 8857, 2105, 1037, 2402, 4467, 3689, 3076, 2315, 14229, 2040, 4122, 2000, 4553, 2673, 2016, 2064, 2055, 2166, 1012, 1999, 3327, 2016, 4122, 2000, 3579, 2014, 3086, 2015, 2000, 2437, 2070, 4066, 1997, 4516, 2006, 2054, 1996, 2779, 25430, 14728, 2245, 2055, 3056, 2576, 3314, 2107, 2004, 1996, 5148, 2162, 1998, 2679, 3314, 1999, 1996, 2142, 2163, 1012, 1999, 2090, 4851, 8801, 1998, 6623, 7939, 4697, 3619, 1997, 8947, 2055, 2037, 10740, 2006, 4331, 1010, 2016, 2038, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"/data/Weights/bert/bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "\n",
    "def preprocess(instance):\n",
    "    return tokenizer(instance[\"text\"], truncation=True, padding=\"max_length\", max_length=160) \n",
    "\n",
    "res = preprocess(raw_dataset[\"train\"][0])\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集处理函数\n",
    "- map(function, batched=False, num_proc=None, remove_columns=None, keep_in_memory=False, load_from_cache_file=True, cache_file_name=None, writer_batch_size=1000, features=None, disable_nullable=False, desc=None)\n",
    "  - 功能:  将一个函数应用于数据集的每个样本。  这是最常用的数据处理函数。\n",
    "  - 参数:\n",
    "    - function:  要应用于每个样本的函数。  该函数应该接受一个字典作为输入 (表示一个样本)，并返回一个字典作为输出 (表示处理后的样本)。\n",
    "    - batched:  是否以批处理模式应用函数。  如果为 True，则函数将接收一个字典列表作为输入 (表示一批样本)。\n",
    "    - num_proc:  用于并行处理的进程数。\n",
    "    - remove_columns:  要从数据集中删除的列的名称列表。\n",
    "    - keep_in_memory:  是否将处理后的数据集保存在内存中。\n",
    "    - load_from_cache_file:  是否从缓存文件加载处理后的数据集。\n",
    "    - cache_file_name:  缓存文件的名称。\n",
    "    - writer_batch_size:  写入缓存文件的批处理大小。\n",
    "    - features:  指定处理后的数据集的特征。\n",
    "    - disable_nullable:  是否禁用可空列。\n",
    "    - desc:  进度条的描述。\n",
    "  - 返回值:  一个处理后的 Dataset 对象。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 25000/25000 [00:04<00:00, 5242.23 examples/s]\n",
      "Map: 100%|██████████| 25000/25000 [00:04<00:00, 5308.21 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = raw_dataset.map(preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 由于hf 中Trainer的一些特殊规定，数据集的标签key值规定为 “labels”，所以我们需要修改数据集中的colnum names；\n",
    "2. 此外，由于我们已经使用分词器得到token id 所以text数据已经没用了，可以remove掉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"label\",\"labels\")\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置数据集格式为 PyTorch tensors (如果使用 TensorFlow 则设置为 \"tf\")\n",
    "tokenized_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  加载预训练模型\n",
    "加载带有适合下游任务头部的模型。对于文本分类，我们使用 AutoModelForSequenceClassification。其就是在最后加了层MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /data/Weights/bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "num_labels = 2\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint,num_labels=num_labels)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  定义评估指标\n",
    "选择适合任务的评估指标。对于分类任务，常用准确率 (Accuracy)、F1 分数等。\n",
    "\n",
    "这里要注意evaluate.load是要联网下载代码的，注意网络问题（会卡很久）。当然对于一些简单的指标计算也可以自己写，但要注意以下要求"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compute_metrics (Callable[[EvalPrediction], Dict], optional) \n",
    "- 用于在评估时计算指标的函数。\n",
    "- 必须接受一个EvalPrediction并返回一个字典，其中键为字符串，值为指标值。\n",
    "- 注意：当传入的TrainingArgs中batch_eval_metrics设置为True时，您的compute_metrics函数必须接受一个布尔类型的compute_result参数。\n",
    "- 这将在最后一个评估批次后触发，以通知函数需要计算并返回全局汇总统计信息，而不是累积批次级别的统计信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**class transformers.EvalPrediction**\n",
    "\n",
    "( predictions: typing.Union[numpy.ndarray, tuple[numpy.ndarray]]label_ids: typing.Union[numpy.ndarray, tuple[numpy.ndarray]]inputs: typing.Union[numpy.ndarray, tuple[numpy.ndarray], NoneType] = Nonelosses: typing.Union[numpy.ndarray, tuple[numpy.ndarray], NoneType] = None )\n",
    "\n",
    "Parameters:\n",
    "\n",
    "- `predictions` (np.ndarray) — Predictions of the model.\n",
    "- `label_ids` (np.ndarray) — Targets to be matched.\n",
    "- `inputs` (np.ndarray, optional) — Input data passed to the model.\n",
    "- `losses` (np.ndarray, optional) — Loss values computed during evaluation.\n",
    "\n",
    "Evaluation output (always contains labels), to be used to compute metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationModule(name: \"accuracy\", module_type: \"metric\", features: {'predictions': Value(dtype='int32', id=None), 'references': Value(dtype='int32', id=None)}, usage: \"\"\"\n",
       "Args:\n",
       "    predictions (`list` of `int`): Predicted labels.\n",
       "    references (`list` of `int`): Ground truth labels.\n",
       "    normalize (`boolean`): If set to False, returns the number of correctly classified samples. Otherwise, returns the fraction of correctly classified samples. Defaults to True.\n",
       "    sample_weight (`list` of `float`): Sample weights Defaults to None.\n",
       "\n",
       "Returns:\n",
       "    accuracy (`float` or `int`): Accuracy score. Minimum possible value is 0. Maximum possible value is 1.0, or the number of examples input, if `normalize` is set to `True`.. A higher score means higher accuracy.\n",
       "\n",
       "Examples:\n",
       "\n",
       "    Example 1-A simple example\n",
       "        >>> accuracy_metric = evaluate.load(\"accuracy\")\n",
       "        >>> results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0])\n",
       "        >>> print(results)\n",
       "        {'accuracy': 0.5}\n",
       "\n",
       "    Example 2-The same as Example 1, except with `normalize` set to `False`.\n",
       "        >>> accuracy_metric = evaluate.load(\"accuracy\")\n",
       "        >>> results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0], normalize=False)\n",
       "        >>> print(results)\n",
       "        {'accuracy': 3.0}\n",
       "\n",
       "    Example 3-The same as Example 1, except with `sample_weight` set.\n",
       "        >>> accuracy_metric = evaluate.load(\"accuracy\")\n",
       "        >>> results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0], sample_weight=[0.5, 2, 0.7, 0.5, 9, 0.4])\n",
       "        >>> print(results)\n",
       "        {'accuracy': 0.8778625954198473}\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_path = \"/data/cache/hf/modules/evaluate_modules/metrics/evaluate-metric--accuracy/f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14/accuracy.py\"\n",
    "\n",
    "metric = evaluate.load(metric_path) \n",
    "metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里简单实现了一个指标计算的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute(references, predictions):\n",
    "\n",
    "    n_references = len(references)\n",
    "    n_predictions = len(predictions)\n",
    "\n",
    "    if n_references != n_predictions:\n",
    "        raise ValueError(\"Input lists 'references' and 'predictions' must have the same length.\")\n",
    "\n",
    "    correct_count = 0\n",
    "    for i in range(n_references):\n",
    "        if references[i] == predictions[i]:\n",
    "            correct_count += 1\n",
    "\n",
    "    accuracy = correct_count / n_references\n",
    "\n",
    "    return {'accuracy': accuracy}\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型训练\n",
    "在最后会详细的列出TrainingArguments的所有参数并，附上每个参数的意义\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results/bert-epoch5\",              # 输出目录，保存模型和日志\n",
    "    eval_strategy=\"epoch\",         # 每个 epoch 结束后进行评估\n",
    "    save_strategy=\"epoch\",               # 每个 epoch 结束后保存模型\n",
    "    learning_rate=2e-5,                  # 学习率\n",
    "    per_device_train_batch_size=16,      # 训练批次大小\n",
    "    per_device_eval_batch_size=16,       # 评估批次大小\n",
    "    num_train_epochs=5,                  # 训练轮数\n",
    "    weight_decay=0.01,                   # 权重衰减\n",
    "    load_best_model_at_end=True,         # 训练结束后加载最佳模型\n",
    "    metric_for_best_model=\"accuracy\",    # 以 accuracy 指标判断最佳模型 (需与 compute_metrics 返回的 key 匹配)\n",
    "    report_to=\"tensorboard\",             # 可以选择保存日志到 tensorboard, wandb 等\n",
    "    save_total_limit=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-04 22:54:46,393] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda3/envs/llm/compiler_compat/ld: cannot find -lcufile: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3910' max='3910' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3910/3910 16:57, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.342100</td>\n",
       "      <td>0.241491</td>\n",
       "      <td>0.899640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.180400</td>\n",
       "      <td>0.255857</td>\n",
       "      <td>0.901880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.113100</td>\n",
       "      <td>0.347929</td>\n",
       "      <td>0.899880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.056700</td>\n",
       "      <td>0.433937</td>\n",
       "      <td>0.899120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.037600</td>\n",
       "      <td>0.481276</td>\n",
       "      <td>0.899200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/data/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/data/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/data/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/data/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3910, training_loss=0.13811987757377917, metrics={'train_runtime': 1018.8572, 'train_samples_per_second': 122.686, 'train_steps_per_second': 3.838, 'total_flos': 1.02777756e+16, 'train_loss': 0.13811987757377917, 'epoch': 5.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们查看训练日志可以发现，非常明显的过拟合现象，train—loss下降，eval-loss上升。\n",
    "\n",
    "大概率是因为进行了模型的全量微调，且数据集过小导致的。那么我们简单尝试一下只微调部分层（pooler层和classifier）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /data/Weights/bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- bert.pooler.dense.weight\n",
      "- bert.pooler.dense.bias\n",
      "- classifier.weight\n",
      "- classifier.bias\n",
      "\n",
      "Total parameters: 109483778\n",
      "Trainable parameters: 592130\n",
      "Percentage trainable: 0.54%\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint,num_labels=num_labels)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if 'classifier' in name or 'pooler' in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "        \n",
    "# 检查一下模型可训练参数\n",
    "\n",
    "total_params = 0\n",
    "trainable_params = 0\n",
    "for name, param in model.named_parameters():\n",
    "    total_params += param.numel()\n",
    "    if param.requires_grad:\n",
    "        trainable_params += param.numel()\n",
    "        print(f\"- {name}\")\n",
    "\n",
    "print(f\"\\nTotal parameters: {total_params}\")\n",
    "print(f\"Trainable parameters: {trainable_params}\")\n",
    "print(f\"Percentage trainable: {trainable_params / total_params * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3910' max='3910' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3910/3910 14:26, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.602900</td>\n",
       "      <td>0.481677</td>\n",
       "      <td>0.786480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.470900</td>\n",
       "      <td>0.441038</td>\n",
       "      <td>0.800040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.465500</td>\n",
       "      <td>0.427341</td>\n",
       "      <td>0.806680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.449700</td>\n",
       "      <td>0.423719</td>\n",
       "      <td>0.807800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.443400</td>\n",
       "      <td>0.422067</td>\n",
       "      <td>0.808640</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/data/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/data/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/data/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/data/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3910, training_loss=0.4802803332238551, metrics={'train_runtime': 867.0357, 'train_samples_per_second': 144.169, 'train_steps_per_second': 4.51, 'total_flos': 1.02777756e+16, 'train_loss': 0.4802803332238551, 'epoch': 5.0})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以发现至少训练是正常的，可以逐步增加可训练的参数，或者使用peft库进行高效微调"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型推理\n",
    "\n",
    "这里使用hf 的pipeline来进行模型的推理。对于使用标准 hf 训练得到的模型权重，直接传入目录即可。如果是经过其他包装的模型需要重新定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "custom_pipeline = pipeline(\n",
    "    \"text-classification\",  # 任务类型\n",
    "    model=\"./results/bert-epoch5/checkpoint-3910\",\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative', 'positive']\n"
     ]
    }
   ],
   "source": [
    "# 使用pipeline进行预测\n",
    "result = custom_pipeline([\"I hate this movie\",\"i love this movie\"])\n",
    "\n",
    "def postprocess(res):\n",
    "    Res_dict = {\"LABEL_1\":\"positive\", \"LABEL_0\":\"negative\"}\n",
    "    \n",
    "    return Res_dict[res['label']]\n",
    "\n",
    "\n",
    "presult = list(map(postprocess, result))\n",
    "\n",
    "print(presult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace TrainingArguments 参数解析\n",
    "\n",
    "\n",
    "## 1. 基础配置参数\n",
    "\n",
    "| 参数名 | 类型 | 默认值 | 说明 |\n",
    "|-------|-----|-------|------|\n",
    "| output_dir | str | \"trainer_output\" | 输出目录，用于保存模型预测结果和检查点 |\n",
    "| overwrite_output_dir | bool | False | 是否覆盖输出目录的内容，用于在已有检查点的目录中继续训练 |\n",
    "| do_train | bool | False | 是否运行训练，主要由训练/评估脚本使用 |\n",
    "| do_eval | bool | None | 是否在验证集上运行评估，当eval_strategy不为\"no\"时会自动设为True |\n",
    "| do_predict | bool | False | 是否在测试集上运行预测，主要由训练/评估脚本使用 |\n",
    "| run_name | str | 等同于output_dir | 运行描述符，通常用于wandb、mlflow和comet日志记录 |\n",
    "| disable_tqdm | bool | None | 是否禁用进度条，默认在日志级别为warn或更低时为True |\n",
    "| remove_unused_columns | bool | True | 是否自动移除模型前向方法中未使用的列 |\n",
    "| seed | int | 42 | 训练开始时设置的随机种子 |\n",
    "| data_seed | int | None | 数据采样器使用的随机种子，若未设置则使用与seed相同的值 |\n",
    "| use_cpu | bool | False | 是否使用CPU。若为False，会优先使用CUDA或MPS设备 |\n",
    "| full_determinism | bool | False | 若为True，则调用enable_full_determinism而非set_seed以确保分布式训练中的可复现性 |\n",
    "\n",
    "## 2. 训练控制参数\n",
    "\n",
    "| 参数名 | 类型 | 默认值 | 说明 |\n",
    "|-------|-----|-------|------|\n",
    "| per_device_train_batch_size | int | 8 | 每个GPU/TPU核心/CPU的训练批量大小 |\n",
    "| per_device_eval_batch_size | int | 8 | 每个GPU/TPU核心/CPU的评估批量大小 |\n",
    "| gradient_accumulation_steps | int | 1 | 在执行反向/更新传递之前累积梯度的更新步骤数 |\n",
    "| eval_accumulation_steps | int | None | 在将结果移至CPU之前累积输出张量的预测步骤数 |\n",
    "| max_steps | int | -1 | 若为正数，表示要执行的训练步骤总数，会覆盖num_train_epochs |\n",
    "| num_train_epochs | float | 3.0 | 要执行的训练周期总数 |\n",
    "| max_grad_norm | float | 1.0 | 最大梯度范数（用于梯度裁剪） |\n",
    "| group_by_length | bool | False | 是否将训练数据集中大致相同长度的样本分组（提高效率） |\n",
    "| length_column_name | str | \"length\" | 预计算长度的列名 |\n",
    "| dataloader_drop_last | bool | False | 如果数据集长度不能被批量大小整除，是否丢弃最后一个不完整批次 |\n",
    "| dataloader_num_workers | int | 0 | 用于数据加载的子进程数量 |\n",
    "| dataloader_pin_memory | bool | True | 是否在数据加载器中使用内存锁定 |\n",
    "| dataloader_persistent_workers | bool | False | 数据集消耗完后是否保持工作进程活跃 |\n",
    "| dataloader_prefetch_factor | int | None | 每个工作进程提前加载的批次数 |\n",
    "| auto_find_batch_size | bool | False | 是否通过指数衰减自动寻找适合内存的批量大小 |\n",
    "| neftune_noise_alpha | float | None | 激活NEFTune噪声嵌入的参数，可显著提高指令微调的模型性能 |\n",
    "\n",
    "## 3. 评估相关参数\n",
    "\n",
    "| 参数名 | 类型 | 默认值 | 说明 |\n",
    "|-------|-----|-------|------|\n",
    "| eval_strategy | str/IntervalStrategy | \"no\" | 训练期间的评估策略：\"no\"(不评估)、\"steps\"(每eval_steps评估)、\"epoch\"(每个epoch结束时评估) |\n",
    "| prediction_loss_only | bool | False | 在执行评估和生成预测时，是否仅返回损失 |\n",
    "| eval_steps | int/float | None | 如果eval_strategy=\"steps\"，则为两次评估之间的更新步骤数，未设置则与logging_steps相同 |\n",
    "| eval_delay | float | None | 首次评估前等待的轮次或步骤数，取决于eval_strategy |\n",
    "| load_best_model_at_end | bool | False | 训练结束时是否加载找到的最佳模型 |\n",
    "| metric_for_best_model | str | None | 与load_best_model_at_end结合使用，指定用于比较不同模型的指标 |\n",
    "| greater_is_better | bool | None | 与load_best_model_at_end和metric_for_best_model结合使用，指定更好的模型是否应具有更大的指标 |\n",
    "| include_inputs_for_metrics | bool | False | 已弃用，使用include_for_metrics替代 |\n",
    "| include_for_metrics | List[str] | [] | 指定metrics计算时需要的额外数据，如\"inputs\"、\"loss\"等 |\n",
    "| eval_do_concat_batches | bool | True | 是否在批次间递归连接输入/损失/标签/预测结果 |\n",
    "| batch_eval_metrics | bool | False | 是否在每个批次结束时调用compute_metrics以累积统计信息，而非将所有评估logits保存在内存中 |\n",
    "| eval_on_start | bool | False | 是否在训练前执行评估步骤（检查验证步骤是否正常工作） |\n",
    "| eval_use_gather_object | bool | False | 是否从所有设备递归收集嵌套列表/元组/字典中的对象 |\n",
    "\n",
    "## 4. 优化器与学习率相关参数\n",
    "\n",
    "| 参数名 | 类型 | 默认值 | 说明 |\n",
    "|-------|-----|-------|------|\n",
    "| learning_rate | float | 5e-5 | AdamW优化器的初始学习率 |\n",
    "| weight_decay | float | 0 | 在AdamW优化器中应用于所有层的权重衰减（除了所有偏置和LayerNorm权重） |\n",
    "| adam_beta1 | float | 0.9 | AdamW优化器的beta1超参数 |\n",
    "| adam_beta2 | float | 0.999 | AdamW优化器的beta2超参数 |\n",
    "| adam_epsilon | float | 1e-8 | AdamW优化器的epsilon超参数 |\n",
    "| lr_scheduler_type | str/SchedulerType | \"linear\" | 要使用的调度器类型 |\n",
    "| lr_scheduler_kwargs | dict | {} | 传递给lr_scheduler的额外参数 |\n",
    "| warmup_ratio | float | 0.0 | 用于从0到learning_rate的线性预热的总训练步骤的比例 |\n",
    "| warmup_steps | int | 0 | 用于从0到learning_rate的线性预热的步骤数，会覆盖warmup_ratio的效果 |\n",
    "| optim | str/OptimizerNames | \"adamw_torch\" | 要使用的优化器，如\"adamw_hf\"、\"adamw_torch\"、\"adamw_torch_fused\"等 |\n",
    "| optim_args | str | None | 传递给优化器的可选参数 |\n",
    "| optim_target_modules | str/List[str] | None | 要优化的目标模块，目前用于GaLore和APOLLO算法 |\n",
    "\n",
    "## 5. 保存与加载相关参数\n",
    "\n",
    "| 参数名 | 类型 | 默认值 | 说明 |\n",
    "|-------|-----|-------|------|\n",
    "| save_strategy | str/SaveStrategy | \"steps\" | 训练期间的检查点保存策略：\"no\"(不保存)、\"epoch\"(每个epoch末保存)、\"steps\"(每save_steps保存)、\"best\"(在达到新的best_metric时保存) |\n",
    "| save_steps | int/float | 500 | 若save_strategy=\"steps\"，则为两次检查点保存之间的更新步骤数 |\n",
    "| save_total_limit | int | None | 如果传递了值，将限制检查点的总数，删除output_dir中较旧的检查点 |\n",
    "| save_safetensors | bool | True | 使用safetensors而非默认torch.load和torch.save进行状态字典的保存和加载 |\n",
    "| save_on_each_node | bool | False | 在多节点分布式训练中，是否在每个节点上保存模型和检查点 |\n",
    "| save_only_model | bool | False | 检查点时是否只保存模型，而不保存优化器、调度器和RNG状态 |\n",
    "| restore_callback_states_from_checkpoint | bool | False | 是否从检查点恢复回调状态 |\n",
    "| resume_from_checkpoint | str | None | 指向包含有效检查点的文件夹路径，用于恢复训练 |\n",
    "| ignore_data_skip | bool | False | 恢复训练时，是否跳过数据到之前训练的相同阶段 |\n",
    "\n",
    "## 6. 分布式训练参数\n",
    "\n",
    "| 参数名 | 类型 | 默认值 | 说明 |\n",
    "|-------|-----|-------|------|\n",
    "| local_rank | int | -1 | 分布式训练期间的进程排名 |\n",
    "| ddp_backend | str | None | 分布式训练使用的后端，必须是\"nccl\"、\"mpi\"、\"ccl\"、\"gloo\"、\"hccl\"之一 |\n",
    "| tpu_num_cores | int | None | 在TPU上训练时的TPU核心数量 |\n",
    "| ddp_find_unused_parameters | bool | None | 使用分布式训练时传递给DistributedDataParallel的find_unused_parameters标志的值 |\n",
    "| ddp_bucket_cap_mb | int | None | 使用分布式训练时传递给DistributedDataParallel的bucket_cap_mb标志的值 |\n",
    "| ddp_broadcast_buffers | bool | None | 使用分布式训练时传递给DistributedDataParallel的broadcast_buffers标志的值 |\n",
    "| ddp_timeout | int | 1800 | torch.distributed.init_process_group调用的超时设置 |\n",
    "| fsdp | bool/str/List[FSDPOption] | \"\" | 使用PyTorch分布式并行训练（仅在分布式训练中） |\n",
    "| fsdp_config | str/dict | None | 与FSDP一起使用的配置 |\n",
    "| deepspeed | str/dict | None | 使用Deepspeed，实验性功能 |\n",
    "| accelerator_config | str/dict/AcceleratorConfig | None | 用于内部Accelerator实现的配置 |\n",
    "| split_batches | bool | None | 加速器是否应该将数据加载器生成的批次在设备间拆分 |\n",
    "| average_tokens_across_devices | bool | False | 是否跨设备平均令牌数，用于精确损失计算 |\n",
    "\n",
    "## 7. 精度与性能参数\n",
    "\n",
    "| 参数名 | 类型 | 默认值 | 说明 |\n",
    "|-------|-----|-------|------|\n",
    "| bf16 | bool | False | 是否使用bf16 16位（混合）精度训练而非32位训练 |\n",
    "| fp16 | bool | False | 是否使用fp16 16位（混合）精度训练而非32位训练 |\n",
    "| fp16_opt_level | str | \"O1\" | 用于fp16训练的Apex AMP优化级别 |\n",
    "| fp16_backend | str | \"auto\" | 已弃用，使用half_precision_backend替代 |\n",
    "| half_precision_backend | str | \"auto\" | 用于混合精度训练的后端 |\n",
    "| bf16_full_eval | bool | False | 是否使用完全的bfloat16评估而非32位 |\n",
    "| fp16_full_eval | bool | False | 是否使用完全的float16评估而非32位 |\n",
    "| tf32 | bool | None | 是否启用TF32模式，在Ampere及更新的GPU架构中可用 |\n",
    "| jit_mode_eval | bool | False | 是否使用PyTorch jit trace进行推理 |\n",
    "| use_ipex | bool | False | 在可用时是否使用Intel PyTorch扩展 |\n",
    "| torch_compile | bool | False | 是否使用PyTorch 2.0的torch.compile编译模型 |\n",
    "| torch_compile_backend | str | None | 在torch.compile中使用的后端 |\n",
    "| torch_compile_mode | str | None | 在torch.compile中使用的模式 |\n",
    "| gradient_checkpointing | bool | False | 是否使用梯度检查点以节省内存（以较慢的反向传递为代价） |\n",
    "| gradient_checkpointing_kwargs | dict | None | 传递给gradient_checkpointing_enable方法的关键字参数 |\n",
    "| torch_empty_cache_steps | int | None | 在调用torch.<device>.empty_cache()之前等待的步骤数 |\n",
    "| use_liger_kernel | bool | False | 是否启用Liger Kernel进行LLM模型训练，可提高多GPU训练吞吐量并减少内存使用 |\n",
    "\n",
    "## 8. 日志与报告参数\n",
    "\n",
    "| 参数名 | 类型 | 默认值 | 说明 |\n",
    "|-------|-----|-------|------|\n",
    "| log_level | str | \"passive\" | 主进程上使用的日志级别：\"debug\"、\"info\"、\"warning\"、\"error\"、\"critical\"或\"passive\" |\n",
    "| log_level_replica | str | \"warning\" | 副本上使用的日志级别 |\n",
    "| log_on_each_node | bool | True | 在多节点分布式训练中，是否在每个节点上记录日志 |\n",
    "| logging_dir | str | None | TensorBoard日志目录，默认为output_dir/runs/CURRENT_DATETIME_HOSTNAME |\n",
    "| logging_strategy | str/IntervalStrategy | \"steps\" | 训练期间的日志策略：\"no\"、\"epoch\"或\"steps\" |\n",
    "| logging_first_step | bool | False | 是否记录第一个global_step |\n",
    "| logging_steps | int/float | 500 | 如果logging_strategy=\"steps\"，则为两次日志之间的更新步骤数 |\n",
    "| logging_nan_inf_filter | bool | True | 是否过滤日志记录的nan和inf损失 |\n",
    "| report_to | str/List[str] | \"all\" | 报告结果和日志的集成列表，支持多种平台如wandb、tensorboard等 |\n",
    "| skip_memory_metrics | bool | True | 是否跳过向指标添加内存分析器报告 |\n",
    "| include_tokens_per_second | bool | None | 是否计算每秒每设备令牌数的训练速度指标 |\n",
    "| include_num_input_tokens_seen | bool | None | 是否跟踪整个训练过程中看到的输入令牌数 |\n",
    "\n",
    "## 9. 集成与推送参数\n",
    "\n",
    "| 参数名 | 类型 | 默认值 | 说明 |\n",
    "|-------|-----|-------|------|\n",
    "| push_to_hub | bool | False | 是否在每次保存模型时将模型推送到Hub |\n",
    "| hub_model_id | str | None | 与本地output_dir保持同步的仓库名称 |\n",
    "| hub_strategy | str/HubStrategy | \"every_save\" | 定义何时以及什么内容被推送到Hub |\n",
    "| hub_token | str | None | 用于将模型推送到Hub的令牌 |\n",
    "| hub_private_repo | bool | None | 是否将仓库设为私有 |\n",
    "| hub_always_push | bool | False | 是否在上一次推送未完成时跳过推送检查点 |\n",
    "\n",
    "## 10. 其他工具性参数\n",
    "\n",
    "| 参数名 | 类型 | 默认值 | 说明 |\n",
    "|-------|-----|-------|------|\n",
    "| debug | str/List[DebugOption] | \"\" | 启用一个或多个调试功能，实验性功能 |\n",
    "| past_index | int | -1 | 一些模型可以利用过去的隐藏状态进行预测 |\n",
    "| ray_scope | str | \"last\" | 使用Ray进行超参数搜索时使用的范围 |\n",
    "| label_names | List[str] | None | 输入字典中对应标签的键列表 |\n",
    "| label_smoothing_factor | float | 0.0 | 标签平滑因子 |\n",
    "| torchdynamo | str | None | TorchDynamo的后端编译器，如\"eager\"、\"inductor\"等 |\n",
    "| use_mps_device | bool | False | 已弃用。若可用，将使用mps设备（类似于cuda设备） |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace Trainer 参数解析\n",
    "\n",
    "\n",
    "## 1. 模型相关参数\n",
    "\n",
    "| 参数名 | 类型 | 默认值 | 说明 |\n",
    "|-------|-----|-------|------|\n",
    "| model | PreTrainedModel 或 torch.nn.Module | 必选 | 用于训练、评估或预测的模型。如果未提供，则必须提供model_init |\n",
    "| model_init | Callable[[], PreTrainedModel] | None | 实例化模型的函数。如果提供，每次调用train将从此函数返回的新模型实例开始 |\n",
    "| place_model_on_device | bool | True | 是否自动将模型放置在设备上。当使用模型并行或DeepSpeed时会设为False |\n",
    "\n",
    "## 2. 数据处理参数\n",
    "\n",
    "| 参数名 | 类型 | 默认值 | 说明 |\n",
    "|-------|-----|-------|------|\n",
    "| data_collator | DataCollator | None | 将数据集元素列表组合成批次的函数。如果未提供，且无processing_class，将默认使用default_data_collator |\n",
    "| train_dataset | Dataset或IterableDataset | None | 用于训练的数据集。如果是datasets.Dataset，不被model.forward()接受的列会自动移除 |\n",
    "| eval_dataset | Dataset或Dict[str, Dataset] | None | 用于评估的数据集。如果是字典，将对每个数据集进行评估，并在指标名称前加上字典键 |\n",
    "| processing_class | PreTrainedTokenizerBase等 | None | 用于处理数据的类。如果提供，将用于自动处理模型输入，并与模型一起保存 |\n",
    "\n",
    "## 3. 训练配置参数\n",
    "\n",
    "| 参数名 | 类型 | 默认值 | 说明 |\n",
    "|-------|-----|-------|------|\n",
    "| args | TrainingArguments | 基本实例 | 用于调整训练的参数。如果未提供，将默认使用output_dir设置为当前目录中tmp_trainer的基本实例 |\n",
    "| compute_loss_func | Callable | None | 接受模型原始输出、标签和累积批次中项目总数的函数，并返回损失 |\n",
    "\n",
    "## 4. 优化器和调度器参数\n",
    "\n",
    "| 参数名 | 类型 | 默认值 | 说明 |\n",
    "|-------|-----|-------|------|\n",
    "| optimizers | Tuple[Optimizer, LRScheduler] | (None, None) | 包含优化器和调度器的元组。默认使用AdamW和get_linear_schedule_with_warmup |\n",
    "| optimizer_cls_and_kwargs | Tuple[Type[Optimizer], Dict[str, Any]] | None | 包含优化器类和关键字参数的元组。覆盖args中的optim和optim_args，与optimizers参数不兼容 |\n",
    "\n",
    "## 5. 评估和指标参数\n",
    "\n",
    "| 参数名 | 类型 | 默认值 | 说明 |\n",
    "|-------|-----|-------|------|\n",
    "| compute_metrics | Callable[[EvalPrediction], Dict] | None | 用于计算评估指标的函数。必须接受EvalPrediction并返回字符串到指标值的字典 |\n",
    "| preprocess_logits_for_metrics | Callable | None | 在每个评估步骤缓存logits之前预处理它们的函数。必须接受两个张量(logits和labels)并返回处理后的logits |\n",
    "\n",
    "## 6. 回调参数\n",
    "\n",
    "| 参数名 | 类型 | 默认值 | 说明 |\n",
    "|-------|-----|-------|------|\n",
    "| callbacks | List[TrainerCallback] | None | 用于自定义训练循环的回调列表。会添加到默认回调列表中 |\n",
    "\n",
    "## 7. 重要属性\n",
    "\n",
    "| 属性名 | 类型 | 说明 |\n",
    "|-------|-----|------|\n",
    "| model | PreTrainedModel | 始终指向核心模型。如果使用transformers模型，它将是PreTrainedModel的子类 |\n",
    "| model_wrapped | Module | 始终指向最外层的模型。如果原始模型被一个或多个其他模块包装，这是应该用于前向传播的模型 |\n",
    "| is_model_parallel | bool | 模型是否已切换到模型并行模式(与数据并行不同，这意味着模型的某些层在不同的GPU上拆分) |\n",
    "| is_in_train | bool | 模型当前是否正在运行train(例如，在train期间调用evaluate时) |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face Tokenizer 方法和属性全解析\n",
    "\n",
    "## 1. 核心方法\n",
    "\n",
    "| 方法名 | 说明 | 常用参数 |\n",
    "|-------|------|---------|\n",
    "| `__call__` | 主要的tokenization方法，支持单条或多条文本 | `text`, `text_pair`, `padding`, `truncation`, `max_length`, `return_tensors` |\n",
    "| `encode` | 将文本转换为token IDs | `text`, `text_pair`, `add_special_tokens`, `padding`, `truncation` |\n",
    "| `decode` | 将token IDs转换回文本 | `token_ids`, `skip_special_tokens`, `clean_up_tokenization_spaces` |\n",
    "| `tokenize` | 将文本分割为tokens | `text`, `text_pair`, `add_special_tokens` |\n",
    "| `batch_encode_plus` | 批量处理文本(与`__call__(batch_text)`相同) | `batch_text_or_text_pairs`, `padding`, `truncation`, `max_length` |\n",
    "| `encode_plus` | 获取文本的完整编码信息 | 与`__call__`相同 |\n",
    "| `save_pretrained` | 保存tokenizer到目录 | `save_directory`, `legacy_format`, `filename_prefix` |\n",
    "| `from_pretrained` | 从预训练模型或目录加载tokenizer(静态方法) | `pretrained_model_name_or_path`, `cache_dir`, `use_fast` |\n",
    "\n",
    "## 2. 转换方法\n",
    "\n",
    "| 方法名 | 说明 | 常用参数 |\n",
    "|-------|------|---------|\n",
    "| `convert_tokens_to_ids` | 将tokens转换为IDs | `tokens` |\n",
    "| `convert_ids_to_tokens` | 将IDs转换为tokens | `ids`, `skip_special_tokens` |\n",
    "| `convert_tokens_to_string` | 将tokens列表转换为单个字符串 | `tokens` |\n",
    "| `prepare_for_model` | 为模型准备编码输入 | `ids`, `pair_ids`, `add_special_tokens` |\n",
    "| `build_inputs_with_special_tokens` | 在输入中添加特殊tokens | `token_ids_0`, `token_ids_1` |\n",
    "\n",
    "## 3. 辅助方法\n",
    "\n",
    "| 方法名 | 说明 | 常用参数 |\n",
    "|-------|------|---------|\n",
    "| `pad` | 对已编码的输入执行填充 | `encoded_inputs`, `padding_strategy`, `max_length`, `pad_to_multiple_of` |\n",
    "| `truncate` | 截断已编码的输入 | `encoded_inputs`, `max_length`, `truncation_strategy` |\n",
    "| `add_special_tokens` | 处理特殊tokens | `encoded_inputs`, `return_tensors` |\n",
    "| `create_token_type_ids_from_sequences` | 为序列对创建token类型IDs | `token_ids_0`, `token_ids_1` |\n",
    "| `num_special_tokens_to_add` | 计算添加的特殊tokens数量 | `pair` |\n",
    "\n",
    "## 4. 关键属性\n",
    "\n",
    "| 属性名 | 类型 | 说明 |\n",
    "|-------|------|------|\n",
    "| `vocab_size` | int | 词汇表大小 |\n",
    "| `model_max_length` | int | 模型支持的最大序列长度 |\n",
    "| `padding_side` | str | 填充位置(\"left\"或\"right\") |\n",
    "| `truncation_side` | str | 截断位置(\"left\"或\"right\") |\n",
    "| `all_special_tokens` | list | 所有特殊tokens列表 |\n",
    "| `special_tokens_map` | dict | 特殊token类型到值的映射 |\n",
    "| `vocab` | dict | 词汇表(token到ID的映射) |\n",
    "| `added_tokens_encoder` | dict | 额外添加的tokens编码器 |\n",
    "| `added_tokens_decoder` | dict | 额外添加的tokens解码器 |\n",
    "\n",
    "## 5. `__call__`方法主要参数\n",
    "\n",
    "| 参数名 | 类型 | 默认值 | 说明 |\n",
    "|-------|------|-------|------|\n",
    "| `text` | str/List[str] | 必需 | 要编码的文本或文本列表 |\n",
    "| `text_pair` | str/List[str] | None | 配对文本(用于句子对任务) |\n",
    "| `padding` | bool/str | False | 填充策略(\"max_length\"、\"longest\"或布尔值) |\n",
    "| `truncation` | bool/str | False | 截断策略(\"only_first\"、\"only_second\"、\"longest_first\"、\"do_not_truncate\"或布尔值) |\n",
    "| `max_length` | int | None | 序列的最大长度 |\n",
    "| `stride` | int | 0 | 当使用`return_overflowing_tokens=True`时的窗口滑动步长 |\n",
    "| `return_tensors` | str | None | 返回的张量类型(\"pt\"表示PyTorch，\"tf\"表示TensorFlow，\"np\"表示NumPy) |\n",
    "| `return_token_type_ids` | bool | None | 是否返回token类型IDs |\n",
    "| `return_attention_mask` | bool | None | 是否返回注意力掩码 |\n",
    "| `return_overflowing_tokens` | bool | False | 是否返回溢出tokens |\n",
    "| `return_special_tokens_mask` | bool | False | 是否返回特殊token掩码 |\n",
    "| `return_offsets_mapping` | bool | False | 是否返回字符偏移映射 |\n",
    "| `return_length` | bool | False | 是否返回tokens长度 |\n",
    "| `verbose` | bool | True | 是否打印警告信息 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
