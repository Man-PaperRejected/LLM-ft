{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RACE数据集，英语阅读理解多项选择QA\n",
    "\n",
    "1. 使用GPT2模型，通过生成的方式完成多项选择QA任务\n",
    "2. 使用P-Tuning v2 进行模型微调\n",
    "3. 使用COT来增强模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载数据集\n",
    "使用 datasets 库加载 imdb 数据集。这个库会自动下载并缓存数据。\n",
    "也可以下载到本地，这样可以防止网络问题导致代码执行失败（尽管已经下载到缓存）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda3/envs/llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = \"/data/Dataset/LLM-dataset/race\"\n",
    "raw_dataset = load_dataset(dataset_name, \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['example_id', 'article', 'answer', 'question', 'options'],\n",
      "        num_rows: 4934\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['example_id', 'article', 'answer', 'question', 'options'],\n",
      "        num_rows: 87866\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['example_id', 'article', 'answer', 'question', 'options'],\n",
      "        num_rows: 4887\n",
      "    })\n",
      "})\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "Dataset({\n",
      "    features: ['example_id', 'article', 'answer', 'question', 'options'],\n",
      "    num_rows: 87866\n",
      "})\n",
      "{'example_id': 'high19088.txt', 'article': 'Last week I talked with some of my students about what they wanted to do after they graduated, and what kind of job prospects  they thought they had.\\nGiven that I teach students who are training to be doctors, I was surprised do find that most thought that they would not be able to get the jobs they wanted without \"outside help\". \"What kind of help is that?\" I asked, expecting them to tell me that they would need a   or family friend to help them out.\\n\"Surgery ,\" one replied.\\nI was pretty alarmed by that response. It seems that the graduates of today are increasingly willing to go under the knife to get ahead of others when it comes to getting a job .\\nOne girl told me that she was considering surgery to increase her height. \"They break your legs, put in special extending screws, and slowly expand the gap between the two ends of the bone as it re-grows, you can get at least 5 cm taller!\"\\nAt that point, I was shocked. I am short, I can\\'t deny that, but I don\\'t think I would put myself through months of agony just to be a few centimetres taller. I don\\'t even bother to wear shoes with thick soles, as I\\'m not trying to hide the fact that I am just not tall!\\nIt seems to me that there is a trend towards wanting \"perfection\" , and that is an ideal that just does not exist in reality.\\nNo one is born perfect, yet magazines, TV shows and movies present images of thin, tall, beautiful people as being the norm. Advertisements for slimming aids, beauty treatments and cosmetic surgery clinics fill the pages of newspapers, further creating an idea that \"perfection\" is a requirement, and that it must be purchased, no matter what the cost. In my opinion, skills, rather than appearance, should determine how successful a person is in his/her chosen career.', 'answer': 'C', 'question': 'We can know from the passage that the author works as a_.', 'options': ['doctor', 'model', 'teacher', 'reporter']}\n"
     ]
    }
   ],
   "source": [
    "'''可以直接print 看一下数据集的概况'''\n",
    "print(raw_dataset)\n",
    "\n",
    "'''由于返回的是train 和 test 两个split 也可以直接像字典一样进行索引'''\n",
    "print(type(raw_dataset['train']))\n",
    "print(raw_dataset['train'])\n",
    "\n",
    "print(raw_dataset['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理 (Tokenization)\n",
    "这里的处理稍微有所不一样，针对Decoder only的模型，我们期望其对答案进行生成\n",
    "1. 我们需要构造：article + question + options + answer的Prompt\n",
    "2. 对于labels，我们不计算answer之前的token预测loss，专注于答案生成的loss（也可以尝试计算所有token的loss）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[1102, 16886, 25, 5956, 1285, 314, 6619, 351, 617, 286, 616, 2444, 546, 644, 484, 2227, 284, 466, 706, 484, 18303, 11, 290, 644, 1611, 286, 1693, 13285, 220, 484, 1807, 484, 550, 13, 198, 15056, 326, 314, 4545, 2444, 508, 389, 3047, 284, 307, 7519, 11, 314, 373, 6655, 466, 1064, 326, 749, 1807, 326, 484, 561, 407, 307, 1498, 284, 651, 262, 3946, 484, 2227, 1231, 366, 43435, 1037, 1911, 366, 2061, 1611, 286, 1037, 318, 326, 1701, 314, 1965, 11, 12451, 606, 284, 1560, 502, 326, 484, 561, 761, 257, 220, 220, 393, 1641, 1545, 284, 1037, 606, 503, 13, 198, 1, 14214, 7076, 42911, 530, 8712, 13, 198, 40, 373, 2495, 32064, 416, 326, 2882, 13, 632, 2331, 326, 262, 19087, 286, 1909, 389, 6481, 4684, 284, 467, 739, 262, 9845, 284, 651, 4058, 286, 1854, 618, 340, 2058, 284, 1972, 257, 1693, 764, 198, 3198, 2576, 1297, 502, 326, 673, 373, 6402, 8185, 284, 2620, 607, 6001, 13, 366, 2990, 2270, 534, 7405, 11, 1234, 287, 2041, 16610, 23742, 11, 290, 6364, 4292, 262, 7625, 1022, 262, 734, 5645, 286, 262, 9970, 355, 340, 302, 12, 70, 8516, 11, 345, 460, 651, 379, 1551, 642, 12067, 25242, 2474, 198, 2953, 326, 966, 11, 314, 373, 11472, 13, 314, 716, 1790, 11, 314, 460, 470, 10129, 326, 11, 475, 314, 836, 470, 892, 314, 561, 1234, 3589, 832, 1933, 286, 35358, 655, 284, 307, 257, 1178, 1247, 38813, 411, 25242, 13, 314, 836, 470, 772, 11393, 284, 5806, 10012, 351, 6546, 1540, 274, 11, 355, 314, 1101, 407, 2111, 284, 7808, 262, 1109, 326, 314, 716, 655, 407, 7331, 0, 198, 1026, 2331, 284, 502, 326, 612, 318, 257, 5182, 3371, 10291, 366, 25833, 295, 1, 837, 290, 326, 318, 281, 7306, 326, 655, 857, 407, 2152, 287, 3950, 13, 198, 2949, 530, 318, 4642, 2818, 11, 1865, 16695, 11, 3195, 2523, 290, 6918, 1944, 4263, 286, 7888, 11, 7331, 11, 4950, 661, 355, 852, 262, 2593, 13, 1215, 11371, 329, 18862, 2229, 31378, 11, 8737, 13820, 290, 27284, 8185, 21434, 6070, 262, 5468, 286, 14741, 11, 2252, 4441, 281, 2126, 326, 366, 25833, 295, 1, 318, 257, 9079, 11, 290, 326, 340, 1276, 307, 8155, 11, 645, 2300, 644, 262, 1575, 13, 554, 616, 4459, 11, 4678, 11, 2138, 621, 5585, 11, 815, 5004, 703, 4388, 257, 1048, 318, 287, 465, 14, 372, 7147, 3451, 13, 220, 198, 25652, 25, 1135, 460, 760, 422, 262, 10066, 326, 262, 1772, 2499, 355, 257, 44807, 220, 198, 25811, 25, 198, 32, 25, 35580, 198, 33, 25, 19849, 198, 34, 25, 660, 3493, 198, 35, 25, 260, 26634, 13, 220, 198, 5492, 2922, 262, 1266, 3038, 329, 262, 1808, 1912, 319, 262, 2695, 287, 262, 4732, 11, 3280, 25, 34], [1102, 16886, 25, 5956, 1285, 314, 6619, 351, 617, 286, 616, 2444, 546, 644, 484, 2227, 284, 466, 706, 484, 18303, 11, 290, 644, 1611, 286, 1693, 13285, 220, 484, 1807, 484, 550, 13, 198, 15056, 326, 314, 4545, 2444, 508, 389, 3047, 284, 307, 7519, 11, 314, 373, 6655, 466, 1064, 326, 749, 1807, 326, 484, 561, 407, 307, 1498, 284, 651, 262, 3946, 484, 2227, 1231, 366, 43435, 1037, 1911, 366, 2061, 1611, 286, 1037, 318, 326, 1701, 314, 1965, 11, 12451, 606, 284, 1560, 502, 326, 484, 561, 761, 257, 220, 220, 393, 1641, 1545, 284, 1037, 606, 503, 13, 198, 1, 14214, 7076, 42911, 530, 8712, 13, 198, 40, 373, 2495, 32064, 416, 326, 2882, 13, 632, 2331, 326, 262, 19087, 286, 1909, 389, 6481, 4684, 284, 467, 739, 262, 9845, 284, 651, 4058, 286, 1854, 618, 340, 2058, 284, 1972, 257, 1693, 764, 198, 3198, 2576, 1297, 502, 326, 673, 373, 6402, 8185, 284, 2620, 607, 6001, 13, 366, 2990, 2270, 534, 7405, 11, 1234, 287, 2041, 16610, 23742, 11, 290, 6364, 4292, 262, 7625, 1022, 262, 734, 5645, 286, 262, 9970, 355, 340, 302, 12, 70, 8516, 11, 345, 460, 651, 379, 1551, 642, 12067, 25242, 2474, 198, 2953, 326, 966, 11, 314, 373, 11472, 13, 314, 716, 1790, 11, 314, 460, 470, 10129, 326, 11, 475, 314, 836, 470, 892, 314, 561, 1234, 3589, 832, 1933, 286, 35358, 655, 284, 307, 257, 1178, 1247, 38813, 411, 25242, 13, 314, 836, 470, 772, 11393, 284, 5806, 10012, 351, 6546, 1540, 274, 11, 355, 314, 1101, 407, 2111, 284, 7808, 262, 1109, 326, 314, 716, 655, 407, 7331, 0, 198, 1026, 2331, 284, 502, 326, 612, 318, 257, 5182, 3371, 10291, 366, 25833, 295, 1, 837, 290, 326, 318, 281, 7306, 326, 655, 857, 407, 2152, 287, 3950, 13, 198, 2949, 530, 318, 4642, 2818, 11, 1865, 16695, 11, 3195, 2523, 290, 6918, 1944, 4263, 286, 7888, 11, 7331, 11, 4950, 661, 355, 852, 262, 2593, 13, 1215, 11371, 329, 18862, 2229, 31378, 11, 8737, 13820, 290, 27284, 8185, 21434, 6070, 262, 5468, 286, 14741, 11, 2252, 4441, 281, 2126, 326, 366, 25833, 295, 1, 318, 257, 9079, 11, 290, 326, 340, 1276, 307, 8155, 11, 645, 2300, 644, 262, 1575, 13, 554, 616, 4459, 11, 4678, 11, 2138, 621, 5585, 11, 815, 5004, 703, 4388, 257, 1048, 318, 287, 465, 14, 372, 7147, 3451, 13, 220, 198, 25652, 25, 7085, 19087, 1909, 1210, 284, 27284, 8185, 284, 44807, 220, 198, 25811, 25, 198, 32, 25, 3876, 563, 257, 1365, 582, 14, 8580, 198, 33, 25, 9423, 462, 257, 2746, 198, 34, 25, 1136, 281, 4621, 625, 1854, 287, 1693, 12, 20088, 889, 198, 35, 25, 1078, 974, 517, 21099, 3808, 13, 220, 198, 5492, 2922, 262, 1266, 3038, 329, 262, 1808, 1912, 319, 262, 2695, 287, 262, 4732, 11, 3280, 25, 34], [1102, 16886, 25, 5956, 1285, 314, 6619, 351, 617, 286, 616, 2444, 546, 644, 484, 2227, 284, 466, 706, 484, 18303, 11, 290, 644, 1611, 286, 1693, 13285, 220, 484, 1807, 484, 550, 13, 198, 15056, 326, 314, 4545, 2444, 508, 389, 3047, 284, 307, 7519, 11, 314, 373, 6655, 466, 1064, 326, 749, 1807, 326, 484, 561, 407, 307, 1498, 284, 651, 262, 3946, 484, 2227, 1231, 366, 43435, 1037, 1911, 366, 2061, 1611, 286, 1037, 318, 326, 1701, 314, 1965, 11, 12451, 606, 284, 1560, 502, 326, 484, 561, 761, 257, 220, 220, 393, 1641, 1545, 284, 1037, 606, 503, 13, 198, 1, 14214, 7076, 42911, 530, 8712, 13, 198, 40, 373, 2495, 32064, 416, 326, 2882, 13, 632, 2331, 326, 262, 19087, 286, 1909, 389, 6481, 4684, 284, 467, 739, 262, 9845, 284, 651, 4058, 286, 1854, 618, 340, 2058, 284, 1972, 257, 1693, 764, 198, 3198, 2576, 1297, 502, 326, 673, 373, 6402, 8185, 284, 2620, 607, 6001, 13, 366, 2990, 2270, 534, 7405, 11, 1234, 287, 2041, 16610, 23742, 11, 290, 6364, 4292, 262, 7625, 1022, 262, 734, 5645, 286, 262, 9970, 355, 340, 302, 12, 70, 8516, 11, 345, 460, 651, 379, 1551, 642, 12067, 25242, 2474, 198, 2953, 326, 966, 11, 314, 373, 11472, 13, 314, 716, 1790, 11, 314, 460, 470, 10129, 326, 11, 475, 314, 836, 470, 892, 314, 561, 1234, 3589, 832, 1933, 286, 35358, 655, 284, 307, 257, 1178, 1247, 38813, 411, 25242, 13, 314, 836, 470, 772, 11393, 284, 5806, 10012, 351, 6546, 1540, 274, 11, 355, 314, 1101, 407, 2111, 284, 7808, 262, 1109, 326, 314, 716, 655, 407, 7331, 0, 198, 1026, 2331, 284, 502, 326, 612, 318, 257, 5182, 3371, 10291, 366, 25833, 295, 1, 837, 290, 326, 318, 281, 7306, 326, 655, 857, 407, 2152, 287, 3950, 13, 198, 2949, 530, 318, 4642, 2818, 11, 1865, 16695, 11, 3195, 2523, 290, 6918, 1944, 4263, 286, 7888, 11, 7331, 11, 4950, 661, 355, 852, 262, 2593, 13, 1215, 11371, 329, 18862, 2229, 31378, 11, 8737, 13820, 290, 27284, 8185, 21434, 6070, 262, 5468, 286, 14741, 11, 2252, 4441, 281, 2126, 326, 366, 25833, 295, 1, 318, 257, 9079, 11, 290, 326, 340, 1276, 307, 8155, 11, 645, 2300, 644, 262, 1575, 13, 554, 616, 4459, 11, 4678, 11, 2138, 621, 5585, 11, 815, 5004, 703, 4388, 257, 1048, 318, 287, 465, 14, 372, 7147, 3451, 13, 220, 198, 25652, 25, 4821, 284, 262, 10066, 11, 262, 1772, 5804, 326, 44807, 220, 198, 25811, 25, 198, 32, 25, 47057, 815, 5001, 20187, 11, 4232, 262, 1575, 198, 33, 25, 270, 338, 826, 329, 19087, 284, 1265, 329, 1854, 284, 1037, 606, 503, 287, 10988, 329, 3946, 198, 34, 25, 270, 318, 530, 338, 5585, 2427, 286, 4678, 326, 1107, 6067, 287, 530, 338, 3451, 198, 35, 25, 11431, 389, 284, 8138, 329, 15850, 1862, 661, 287, 511, 6095, 329, 8185, 13, 220, 198, 5492, 2922, 262, 1266, 3038, 329, 262, 1808, 1912, 319, 262, 2695, 287, 262, 4732, 11, 3280, 25, 35], [1102, 16886, 25, 5956, 1285, 314, 6619, 351, 617, 286, 616, 2444, 546, 644, 484, 2227, 284, 466, 706, 484, 18303, 11, 290, 644, 1611, 286, 1693, 13285, 220, 484, 1807, 484, 550, 13, 198, 15056, 326, 314, 4545, 2444, 508, 389, 3047, 284, 307, 7519, 11, 314, 373, 6655, 466, 1064, 326, 749, 1807, 326, 484, 561, 407, 307, 1498, 284, 651, 262, 3946, 484, 2227, 1231, 366, 43435, 1037, 1911, 366, 2061, 1611, 286, 1037, 318, 326, 1701, 314, 1965, 11, 12451, 606, 284, 1560, 502, 326, 484, 561, 761, 257, 220, 220, 393, 1641, 1545, 284, 1037, 606, 503, 13, 198, 1, 14214, 7076, 42911, 530, 8712, 13, 198, 40, 373, 2495, 32064, 416, 326, 2882, 13, 632, 2331, 326, 262, 19087, 286, 1909, 389, 6481, 4684, 284, 467, 739, 262, 9845, 284, 651, 4058, 286, 1854, 618, 340, 2058, 284, 1972, 257, 1693, 764, 198, 3198, 2576, 1297, 502, 326, 673, 373, 6402, 8185, 284, 2620, 607, 6001, 13, 366, 2990, 2270, 534, 7405, 11, 1234, 287, 2041, 16610, 23742, 11, 290, 6364, 4292, 262, 7625, 1022, 262, 734, 5645, 286, 262, 9970, 355, 340, 302, 12, 70, 8516, 11, 345, 460, 651, 379, 1551, 642, 12067, 25242, 2474, 198, 2953, 326, 966, 11, 314, 373, 11472, 13, 314, 716, 1790, 11, 314, 460, 470, 10129, 326, 11, 475, 314, 836, 470, 892, 314, 561, 1234, 3589, 832, 1933, 286, 35358, 655, 284, 307, 257, 1178, 1247, 38813, 411, 25242, 13, 314, 836, 470, 772, 11393, 284, 5806, 10012, 351, 6546, 1540, 274, 11, 355, 314, 1101, 407, 2111, 284, 7808, 262, 1109, 326, 314, 716, 655, 407, 7331, 0, 198, 1026, 2331, 284, 502, 326, 612, 318, 257, 5182, 3371, 10291, 366, 25833, 295, 1, 837, 290, 326, 318, 281, 7306, 326, 655, 857, 407, 2152, 287, 3950, 13, 198, 2949, 530, 318, 4642, 2818, 11, 1865, 16695, 11, 3195, 2523, 290, 6918, 1944, 4263, 286, 7888, 11, 7331, 11, 4950, 661, 355, 852, 262, 2593, 13, 1215, 11371, 329, 18862, 2229, 31378, 11, 8737, 13820, 290, 27284, 8185, 21434, 6070, 262, 5468, 286, 14741, 11, 2252, 4441, 281, 2126, 326, 366, 25833, 295, 1, 318, 257, 9079, 11, 290, 326, 340, 1276, 307, 8155, 11, 645, 2300, 644, 262, 1575, 13, 554, 616, 4459, 11, 4678, 11, 2138, 621, 5585, 11, 815, 5004, 703, 4388, 257, 1048, 318, 287, 465, 14, 372, 7147, 3451, 13, 220, 198, 25652, 25, 13828, 6, 264, 262, 1266, 3670, 329, 262, 10066, 30, 13, 220, 198, 25811, 25, 198, 32, 25, 20917, 17701, 12632, 8192, 16038, 23600, 602, 198, 33, 25, 20917, 17701, 12632, 6803, 284, 39037, 329, 11625, 19161, 198, 34, 25, 20917, 17701, 12632, 6, 29525, 7994, 46161, 39037, 198, 35, 25, 20917, 17701, 12632, 15399, 257, 20615, 49465, 287, 15768, 12, 20088, 889, 13, 220, 198, 5492, 2922, 262, 1266, 3038, 329, 262, 1808, 1912, 319, 262, 2695, 287, 262, 4732, 11, 3280, 25, 33]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 34], [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 34], [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 35], [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 33]]}\n",
      "shape of input_ids :(4,462)\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "model_checkpoint = \"/data/Weights/gpt2/gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_checkpoint)\n",
    "'''\n",
    "这里记得设置padding_side,因为我们用的是生成的方式来完成QA任务, 对于自回归模型生成时一般使用left pad 防止在生成时被pad_token影响\n",
    "'''\n",
    "tokenizer.padding_side=\"left\"\n",
    "\n",
    "\n",
    "def preprocess(instances):\n",
    "    article = instances[\"article\"]\n",
    "    question = instances[\"question\"]\n",
    "    options = instances[\"options\"]\n",
    "    answer = instances['answer']\n",
    "\n",
    "    results = {\n",
    "        'input_ids': [],\n",
    "        'attention_mask': [],\n",
    "        'labels': []\n",
    "    }\n",
    "    \n",
    "    for art,ques,opt,ans in zip(article,question,options,answer):\n",
    "        # 构造prompt\n",
    "        opt_str = f\"\\nA:{opt[0]}\\nB:{opt[1]}\\nC:{opt[2]}\\nD:{opt[3]}\"\n",
    "        prompt = f\"contex:{art} \\nquestion:{ques} \\noptions:{opt_str}. \\nPlease select the best option for the question based on the content in the context, answer:\"        \n",
    "\n",
    "        tokenized_context = tokenizer(\n",
    "            prompt,\n",
    "            max_length=512,\n",
    "            padding=False,            \n",
    "            truncation=\"only_first\",  \n",
    "        ) \n",
    "        # answer之前的loss，不计算，设为-100\n",
    "        labels = [-100]*len(tokenized_context['input_ids'])\n",
    "        \n",
    "        # 对Answer进行encode，并补充进labels\n",
    "        tokenized_ans = tokenizer(\n",
    "            ans,\n",
    "            max_length=512,\n",
    "            padding=False,            \n",
    "            truncation=\"only_first\",  \n",
    "        ) \n",
    "        labels.extend(tokenized_ans['input_ids'])\n",
    "        \n",
    "        for k in tokenized_context.keys():\n",
    "            tokenized_context[k].extend(tokenized_ans[k])\n",
    "            \n",
    "        results['input_ids'].append(tokenized_context['input_ids'])\n",
    "        results['attention_mask'].append(tokenized_context['attention_mask'])\n",
    "        results['labels'].append(labels)\n",
    "    \n",
    "    return results\n",
    "\n",
    "c = preprocess(raw_dataset[\"train\"][0:4])\n",
    "\n",
    "print(c)\n",
    "print(f\"shape of input_ids :({len(c['input_ids'])},{len(c['input_ids'][0])})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contex:Last week I talked with some of my students about what they wanted to do after they graduated, and what kind of job prospects  they thought they had.\n",
      "Given that I teach students who are training to be doctors, I was surprised do find that most thought that they would not be able to get the jobs they wanted without \"outside help\". \"What kind of help is that?\" I asked, expecting them to tell me that they would need a   or family friend to help them out.\n",
      "\"Surgery ,\" one replied.\n",
      "I was pretty alarmed by that response. It seems that the graduates of today are increasingly willing to go under the knife to get ahead of others when it comes to getting a job .\n",
      "One girl told me that she was considering surgery to increase her height. \"They break your legs, put in special extending screws, and slowly expand the gap between the two ends of the bone as it re-grows, you can get at least 5 cm taller!\"\n",
      "At that point, I was shocked. I am short, I can't deny that, but I don't think I would put myself through months of agony just to be a few centimetres taller. I don't even bother to wear shoes with thick soles, as I'm not trying to hide the fact that I am just not tall!\n",
      "It seems to me that there is a trend towards wanting \"perfection\" , and that is an ideal that just does not exist in reality.\n",
      "No one is born perfect, yet magazines, TV shows and movies present images of thin, tall, beautiful people as being the norm. Advertisements for slimming aids, beauty treatments and cosmetic surgery clinics fill the pages of newspapers, further creating an idea that \"perfection\" is a requirement, and that it must be purchased, no matter what the cost. In my opinion, skills, rather than appearance, should determine how successful a person is in his/her chosen career. \n",
      "question:We can know from the passage that the author works as a_. \n",
      "options:\n",
      "A:doctor\n",
      "B:model\n",
      "C:teacher\n",
      "D:reporter. \n",
      "Please select the best option for the question based on the content in the context, answer:C\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(c[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|██████████| 4934/4934 [00:04<00:00, 1144.41 examples/s]\n",
      "Map (num_proc=4): 100%|██████████| 87866/87866 [01:02<00:00, 1411.50 examples/s]\n",
      "Map (num_proc=4): 100%|██████████| 4887/4887 [00:04<00:00, 1180.32 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# 进行批处理，这里我们直接在map函数中将文本信息全部删除即可\n",
    "tokenized_dataset = raw_dataset.map(preprocess, batched=True,remove_columns=raw_dataset[\"train\"].column_names, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1102,\n",
       "  16886,\n",
       "  25,\n",
       "  5956,\n",
       "  1285,\n",
       "  314,\n",
       "  6619,\n",
       "  351,\n",
       "  617,\n",
       "  286,\n",
       "  616,\n",
       "  2444,\n",
       "  546,\n",
       "  644,\n",
       "  484,\n",
       "  2227,\n",
       "  284,\n",
       "  466,\n",
       "  706,\n",
       "  484,\n",
       "  18303,\n",
       "  11,\n",
       "  290,\n",
       "  644,\n",
       "  1611,\n",
       "  286,\n",
       "  1693,\n",
       "  13285,\n",
       "  220,\n",
       "  484,\n",
       "  1807,\n",
       "  484,\n",
       "  550,\n",
       "  13,\n",
       "  198,\n",
       "  15056,\n",
       "  326,\n",
       "  314,\n",
       "  4545,\n",
       "  2444,\n",
       "  508,\n",
       "  389,\n",
       "  3047,\n",
       "  284,\n",
       "  307,\n",
       "  7519,\n",
       "  11,\n",
       "  314,\n",
       "  373,\n",
       "  6655,\n",
       "  466,\n",
       "  1064,\n",
       "  326,\n",
       "  749,\n",
       "  1807,\n",
       "  326,\n",
       "  484,\n",
       "  561,\n",
       "  407,\n",
       "  307,\n",
       "  1498,\n",
       "  284,\n",
       "  651,\n",
       "  262,\n",
       "  3946,\n",
       "  484,\n",
       "  2227,\n",
       "  1231,\n",
       "  366,\n",
       "  43435,\n",
       "  1037,\n",
       "  1911,\n",
       "  366,\n",
       "  2061,\n",
       "  1611,\n",
       "  286,\n",
       "  1037,\n",
       "  318,\n",
       "  326,\n",
       "  1701,\n",
       "  314,\n",
       "  1965,\n",
       "  11,\n",
       "  12451,\n",
       "  606,\n",
       "  284,\n",
       "  1560,\n",
       "  502,\n",
       "  326,\n",
       "  484,\n",
       "  561,\n",
       "  761,\n",
       "  257,\n",
       "  220,\n",
       "  220,\n",
       "  393,\n",
       "  1641,\n",
       "  1545,\n",
       "  284,\n",
       "  1037,\n",
       "  606,\n",
       "  503,\n",
       "  13,\n",
       "  198,\n",
       "  1,\n",
       "  14214,\n",
       "  7076,\n",
       "  42911,\n",
       "  530,\n",
       "  8712,\n",
       "  13,\n",
       "  198,\n",
       "  40,\n",
       "  373,\n",
       "  2495,\n",
       "  32064,\n",
       "  416,\n",
       "  326,\n",
       "  2882,\n",
       "  13,\n",
       "  632,\n",
       "  2331,\n",
       "  326,\n",
       "  262,\n",
       "  19087,\n",
       "  286,\n",
       "  1909,\n",
       "  389,\n",
       "  6481,\n",
       "  4684,\n",
       "  284,\n",
       "  467,\n",
       "  739,\n",
       "  262,\n",
       "  9845,\n",
       "  284,\n",
       "  651,\n",
       "  4058,\n",
       "  286,\n",
       "  1854,\n",
       "  618,\n",
       "  340,\n",
       "  2058,\n",
       "  284,\n",
       "  1972,\n",
       "  257,\n",
       "  1693,\n",
       "  764,\n",
       "  198,\n",
       "  3198,\n",
       "  2576,\n",
       "  1297,\n",
       "  502,\n",
       "  326,\n",
       "  673,\n",
       "  373,\n",
       "  6402,\n",
       "  8185,\n",
       "  284,\n",
       "  2620,\n",
       "  607,\n",
       "  6001,\n",
       "  13,\n",
       "  366,\n",
       "  2990,\n",
       "  2270,\n",
       "  534,\n",
       "  7405,\n",
       "  11,\n",
       "  1234,\n",
       "  287,\n",
       "  2041,\n",
       "  16610,\n",
       "  23742,\n",
       "  11,\n",
       "  290,\n",
       "  6364,\n",
       "  4292,\n",
       "  262,\n",
       "  7625,\n",
       "  1022,\n",
       "  262,\n",
       "  734,\n",
       "  5645,\n",
       "  286,\n",
       "  262,\n",
       "  9970,\n",
       "  355,\n",
       "  340,\n",
       "  302,\n",
       "  12,\n",
       "  70,\n",
       "  8516,\n",
       "  11,\n",
       "  345,\n",
       "  460,\n",
       "  651,\n",
       "  379,\n",
       "  1551,\n",
       "  642,\n",
       "  12067,\n",
       "  25242,\n",
       "  2474,\n",
       "  198,\n",
       "  2953,\n",
       "  326,\n",
       "  966,\n",
       "  11,\n",
       "  314,\n",
       "  373,\n",
       "  11472,\n",
       "  13,\n",
       "  314,\n",
       "  716,\n",
       "  1790,\n",
       "  11,\n",
       "  314,\n",
       "  460,\n",
       "  470,\n",
       "  10129,\n",
       "  326,\n",
       "  11,\n",
       "  475,\n",
       "  314,\n",
       "  836,\n",
       "  470,\n",
       "  892,\n",
       "  314,\n",
       "  561,\n",
       "  1234,\n",
       "  3589,\n",
       "  832,\n",
       "  1933,\n",
       "  286,\n",
       "  35358,\n",
       "  655,\n",
       "  284,\n",
       "  307,\n",
       "  257,\n",
       "  1178,\n",
       "  1247,\n",
       "  38813,\n",
       "  411,\n",
       "  25242,\n",
       "  13,\n",
       "  314,\n",
       "  836,\n",
       "  470,\n",
       "  772,\n",
       "  11393,\n",
       "  284,\n",
       "  5806,\n",
       "  10012,\n",
       "  351,\n",
       "  6546,\n",
       "  1540,\n",
       "  274,\n",
       "  11,\n",
       "  355,\n",
       "  314,\n",
       "  1101,\n",
       "  407,\n",
       "  2111,\n",
       "  284,\n",
       "  7808,\n",
       "  262,\n",
       "  1109,\n",
       "  326,\n",
       "  314,\n",
       "  716,\n",
       "  655,\n",
       "  407,\n",
       "  7331,\n",
       "  0,\n",
       "  198,\n",
       "  1026,\n",
       "  2331,\n",
       "  284,\n",
       "  502,\n",
       "  326,\n",
       "  612,\n",
       "  318,\n",
       "  257,\n",
       "  5182,\n",
       "  3371,\n",
       "  10291,\n",
       "  366,\n",
       "  25833,\n",
       "  295,\n",
       "  1,\n",
       "  837,\n",
       "  290,\n",
       "  326,\n",
       "  318,\n",
       "  281,\n",
       "  7306,\n",
       "  326,\n",
       "  655,\n",
       "  857,\n",
       "  407,\n",
       "  2152,\n",
       "  287,\n",
       "  3950,\n",
       "  13,\n",
       "  198,\n",
       "  2949,\n",
       "  530,\n",
       "  318,\n",
       "  4642,\n",
       "  2818,\n",
       "  11,\n",
       "  1865,\n",
       "  16695,\n",
       "  11,\n",
       "  3195,\n",
       "  2523,\n",
       "  290,\n",
       "  6918,\n",
       "  1944,\n",
       "  4263,\n",
       "  286,\n",
       "  7888,\n",
       "  11,\n",
       "  7331,\n",
       "  11,\n",
       "  4950,\n",
       "  661,\n",
       "  355,\n",
       "  852,\n",
       "  262,\n",
       "  2593,\n",
       "  13,\n",
       "  1215,\n",
       "  11371,\n",
       "  329,\n",
       "  18862,\n",
       "  2229,\n",
       "  31378,\n",
       "  11,\n",
       "  8737,\n",
       "  13820,\n",
       "  290,\n",
       "  27284,\n",
       "  8185,\n",
       "  21434,\n",
       "  6070,\n",
       "  262,\n",
       "  5468,\n",
       "  286,\n",
       "  14741,\n",
       "  11,\n",
       "  2252,\n",
       "  4441,\n",
       "  281,\n",
       "  2126,\n",
       "  326,\n",
       "  366,\n",
       "  25833,\n",
       "  295,\n",
       "  1,\n",
       "  318,\n",
       "  257,\n",
       "  9079,\n",
       "  11,\n",
       "  290,\n",
       "  326,\n",
       "  340,\n",
       "  1276,\n",
       "  307,\n",
       "  8155,\n",
       "  11,\n",
       "  645,\n",
       "  2300,\n",
       "  644,\n",
       "  262,\n",
       "  1575,\n",
       "  13,\n",
       "  554,\n",
       "  616,\n",
       "  4459,\n",
       "  11,\n",
       "  4678,\n",
       "  11,\n",
       "  2138,\n",
       "  621,\n",
       "  5585,\n",
       "  11,\n",
       "  815,\n",
       "  5004,\n",
       "  703,\n",
       "  4388,\n",
       "  257,\n",
       "  1048,\n",
       "  318,\n",
       "  287,\n",
       "  465,\n",
       "  14,\n",
       "  372,\n",
       "  7147,\n",
       "  3451,\n",
       "  13,\n",
       "  220,\n",
       "  198,\n",
       "  25652,\n",
       "  25,\n",
       "  1135,\n",
       "  460,\n",
       "  760,\n",
       "  422,\n",
       "  262,\n",
       "  10066,\n",
       "  326,\n",
       "  262,\n",
       "  1772,\n",
       "  2499,\n",
       "  355,\n",
       "  257,\n",
       "  44807,\n",
       "  220,\n",
       "  198,\n",
       "  25811,\n",
       "  25,\n",
       "  198,\n",
       "  32,\n",
       "  25,\n",
       "  35580,\n",
       "  198,\n",
       "  33,\n",
       "  25,\n",
       "  19849,\n",
       "  198,\n",
       "  34,\n",
       "  25,\n",
       "  660,\n",
       "  3493,\n",
       "  198,\n",
       "  35,\n",
       "  25,\n",
       "  260,\n",
       "  26634,\n",
       "  13,\n",
       "  220,\n",
       "  198,\n",
       "  5492,\n",
       "  2922,\n",
       "  262,\n",
       "  1266,\n",
       "  3038,\n",
       "  329,\n",
       "  262,\n",
       "  1808,\n",
       "  1912,\n",
       "  319,\n",
       "  262,\n",
       "  2695,\n",
       "  287,\n",
       "  262,\n",
       "  4732,\n",
       "  11,\n",
       "  3280,\n",
       "  25,\n",
       "  34],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'labels': [-100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  34]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置数据集格式为 PyTorch tensors (如果使用 TensorFlow 则设置为 \"tf\")\n",
    "tokenized_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DataCollatorForLanguageModeling` 是用于处理输入数据的动态填充（padding）和标签（labels）的生成，其会复制input_ids 作为labels，这里与我们的操作有所不一样，所有要自定义一个dataCollator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "class QADataCollator(DataCollatorForLanguageModeling):\n",
    "    def __call__(self, features):\n",
    "        batch = self.tokenizer.pad(\n",
    "            {\"input_ids\": [f[\"input_ids\"] for f in features]},\n",
    "            return_attention_mask=True,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        \n",
    "        labels = [f[\"labels\"] for f in features]\n",
    "        padded_labels = []\n",
    "        for i, label_seq in enumerate(labels):\n",
    "            # 计算需要填充的长度\n",
    "            pad_len = batch[\"input_ids\"].shape[1] - len(label_seq)\n",
    "            # 在 labels 的填充部分用 -100（PyTorch 会忽略这些位置的loss）\n",
    "            padded_seq = torch.cat([\n",
    "                torch.full((pad_len,), -100, dtype=torch.long),\n",
    "                torch.tensor(label_seq)\n",
    "                \n",
    "            ])\n",
    "            padded_labels.append(padded_seq)\n",
    "        batch[\"labels\"] = torch.stack(padded_labels)\n",
    "        \n",
    "        return batch\n",
    "\n",
    "data_collator = QADataCollator(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    "    pad_to_multiple_of=8,\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_64184/815751923.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(label_seq)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[50256, 50256, 50256,  ...,  3280,    25,    34],\n",
       "        [50256, 50256, 50256,  ...,  3280,    25,    34],\n",
       "        [ 1102, 16886,    25,  ...,  3280,    25,    35],\n",
       "        [50256, 50256, 50256,  ...,  3280,    25,    33]]), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
       "        [0, 0, 0,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [0, 0, 0,  ..., 1, 1, 1]]), 'labels': tensor([[-100, -100, -100,  ..., -100, -100,   34],\n",
       "        [-100, -100, -100,  ..., -100, -100,   34],\n",
       "        [-100, -100, -100,  ..., -100, -100,   35],\n",
       "        [-100, -100, -100,  ..., -100, -100,   33]])}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 传入四个数据进行查看\n",
    "batch = data_collator([tokenized_dataset['train'][i] for i in range(4)])\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  加载RoBERTa预训练模型\n",
    "加载带有适合下游任务头部的模型。对于文本分类，我们使用 AutoModelForSequenceClassification。其就是在最后加了层MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D(nf=2304, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=768)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D(nf=3072, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=3072)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_checkpoint)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(instances, model):\n",
    "    article = instances[\"article\"]\n",
    "    question = instances[\"question\"]\n",
    "    options = instances[\"options\"]\n",
    "    answer = instances['answer']\n",
    "    model = model.to('cuda')\n",
    "    \n",
    "    opt_str = f\"\\nA:{options[0]}\\nB:{options[1]}\\nC:{options[2]}\\nD:{options[3]}\"\n",
    "    prompt = f\"contex:{article} \\nquestion:{question} \\noptions:{opt_str}. \\nPlease select the best option for the question based on the content in the context, answer:\"     \n",
    "    results = tokenizer(\n",
    "        prompt,\n",
    "        max_length=512,\n",
    "        padding=False,            \n",
    "        truncation=\"only_first\",  \n",
    "        return_tensors=\"pt\"\n",
    "    )    \n",
    "    \n",
    "    outputs = model.generate(\n",
    "                        input_ids=results[\"input_ids\"].to(model.device),\n",
    "                        attention_mask=results[\"attention_mask\"].to(model.device),\n",
    "                        num_beams=1,\n",
    "                        max_new_tokens=1,  # 最大生成长度\n",
    "                        num_return_sequences=1,  # 返回1个候选\n",
    "                        do_sample=False,  # 启用随机采样\n",
    "                        top_p=1.0,                     \n",
    "                        temperature=1.0, \n",
    "                        pad_token_id = tokenizer.eos_token_id, \n",
    "                        eos_token_id = tokenizer.eos_token_id,\n",
    "                        )\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尝试一下，好像并无法生成答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contex:Last week I talked with some of my students about what they wanted to do after they graduated, and what kind of job prospects  they thought they had.\n",
      "Given that I teach students who are training to be doctors, I was surprised do find that most thought that they would not be able to get the jobs they wanted without \"outside help\". \"What kind of help is that?\" I asked, expecting them to tell me that they would need a   or family friend to help them out.\n",
      "\"Surgery ,\" one replied.\n",
      "I was pretty alarmed by that response. It seems that the graduates of today are increasingly willing to go under the knife to get ahead of others when it comes to getting a job .\n",
      "One girl told me that she was considering surgery to increase her height. \"They break your legs, put in special extending screws, and slowly expand the gap between the two ends of the bone as it re-grows, you can get at least 5 cm taller!\"\n",
      "At that point, I was shocked. I am short, I can't deny that, but I don't think I would put myself through months of agony just to be a few centimetres taller. I don't even bother to wear shoes with thick soles, as I'm not trying to hide the fact that I am just not tall!\n",
      "It seems to me that there is a trend towards wanting \"perfection\" , and that is an ideal that just does not exist in reality.\n",
      "No one is born perfect, yet magazines, TV shows and movies present images of thin, tall, beautiful people as being the norm. Advertisements for slimming aids, beauty treatments and cosmetic surgery clinics fill the pages of newspapers, further creating an idea that \"perfection\" is a requirement, and that it must be purchased, no matter what the cost. In my opinion, skills, rather than appearance, should determine how successful a person is in his/her chosen career. \n",
      "question:According to the passage, the author believes that_. \n",
      "options:\n",
      "A:everyone should purchase perfection, whatever the cost\n",
      "B:it's right for graduates to ask for others to help them out in hunting for jobs\n",
      "C:it is one's appearance instead of skills that really matters in one's career\n",
      "D:media are to blame for misleading young people in their seeking for surgery. \n",
      "Please select the best option for the question based on the content in the context, answer:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "respone = inference(raw_dataset['train'][2],model)\n",
    "print(respone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 462, 50257])\n"
     ]
    }
   ],
   "source": [
    "# 可以看看输出\n",
    "import torch\n",
    "\n",
    "dummy_input = tokenized_dataset['train'][0]\n",
    "for k, v in dummy_input.items():\n",
    "    dummy_input[k] = v.unsqueeze(0).to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**dummy_input)\n",
    "\n",
    "print(outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  定义评估指标\n",
    "这里做的是语言建模，是不需要compute_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT2模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results/GPT2-epoch5\",              # 输出目录，保存模型和日志\n",
    "    eval_strategy=\"epoch\",         # 每个 epoch 结束后进行评估\n",
    "    save_strategy=\"epoch\",               # 每个 epoch 结束后保存模型\n",
    "    learning_rate=2e-5,                  # 学习率\n",
    "    per_device_train_batch_size=16,      # 训练批次大小\n",
    "    per_device_eval_batch_size=16,       # 评估批次大小\n",
    "    num_train_epochs=5,                  # 训练轮数\n",
    "    weight_decay=0.01,                   # 权重衰减\n",
    "    load_best_model_at_end=True,         # 训练结束后加载最佳模型\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    report_to=\"tensorboard\",             # 可以选择 tensorboard, wandb 等\n",
    "    save_total_limit=2,\n",
    "    label_names = [\"labels\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里我们使用P-tuning v2进行微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PrefixTuningConfig, get_peft_model, TaskType, PeftModel\n",
    "\n",
    "# 定义 Prefix Tuning 配置（P-Tuning v2）\n",
    "peft_config = PrefixTuningConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    num_virtual_tokens=20,         \n",
    "    encoder_hidden_size=768,      \n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# 记得解冻lm_head的梯度计算\n",
    "for name, param in model.named_parameters():\n",
    "    if 'lm_head' in name:\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): GPT2LMHeadModel(\n",
      "    (transformer): GPT2Model(\n",
      "      (wte): Embedding(50257, 768)\n",
      "      (wpe): Embedding(1024, 768)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "      (h): ModuleList(\n",
      "        (0-11): 12 x GPT2Block(\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): GPT2Attention(\n",
      "            (c_attn): Conv1D(nf=2304, nx=768)\n",
      "            (c_proj): Conv1D(nf=768, nx=768)\n",
      "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): GPT2MLP(\n",
      "            (c_fc): Conv1D(nf=3072, nx=768)\n",
      "            (c_proj): Conv1D(nf=768, nx=3072)\n",
      "            (act): NewGELUActivation()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      "  )\n",
      "  (prompt_encoder): ModuleDict(\n",
      "    (default): PrefixEncoder(\n",
      "      (embedding): Embedding(20, 18432)\n",
      "    )\n",
      "  )\n",
      "  (word_embeddings): Embedding(50257, 768)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_64184/815751923.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(label_seq)\n",
      "/data/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='306' max='153' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [153/153 34:35]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.523458957672119, 'eval_runtime': 73.3823, 'eval_samples_per_second': 66.596, 'eval_steps_per_second': 2.085}\n"
     ]
    }
   ],
   "source": [
    "# 查看一下初始精度，并验证能否正常运行\n",
    "evalres = trainer.evaluate()\n",
    "print(evalres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_64184/815751923.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(label_seq)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13730' max='13730' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13730/13730 2:40:07, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.991900</td>\n",
       "      <td>1.785209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.850500</td>\n",
       "      <td>1.594922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.802900</td>\n",
       "      <td>1.538443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.784500</td>\n",
       "      <td>1.515149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.781900</td>\n",
       "      <td>1.509089</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_64184/815751923.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(label_seq)\n",
      "/data/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_64184/815751923.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(label_seq)\n",
      "/data/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_64184/815751923.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(label_seq)\n",
      "/data/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_64184/815751923.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(label_seq)\n",
      "/data/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_64184/815751923.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(label_seq)\n",
      "/data/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=13730, training_loss=0.9582138861693003, metrics={'train_runtime': 9607.9617, 'train_samples_per_second': 45.726, 'train_steps_per_second': 1.429, 'total_flos': 1.15016556503808e+17, 'train_loss': 0.9582138861693003, 'epoch': 5.0})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4090 * 2 训练约3h\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型推理\n",
    "\n",
    "这里测试一下能否正常生成答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contex:The rain had continued for a week and the flood had created a big river which were running by Nancy Brown's farm. As she tried to gather her cows to a higher ground, she slipped and hit her head on a fallen tree trunk. The fall made her unconscious for a moment or two. When she came to, Lizzie, one of her oldest and favorite cows, was licking her face. \n",
      "At that time, the water level on the farm was still rising. Nancy gathered all her strength to get up and began walking slowly with Lizzie. The rain had become much heavier, and the water in the field was now waist high. Nancy's pace got slower and slower because she felt a great pain in her head. Finally, all she could do was to throw her arm around Lizzie's neck and try to hang on. About 20 minutes later, Lizzie managed to pull herself and Nancy out of the rising water and onto a bit of high land, which seemed like a small island in the middle of a lake of white water. \n",
      "Even though it was about noon, the sky was so dark and the rain and lightning was so bad that it took rescuers more than two hours to discover Nancy. A man from a helicopter  lowered a rope, but Nancy couldn't catch it. A moment later, two men landed on the small island from a ladder in the helicopter. They raised her into the helicopter and took her to the school gym, where the Red Cross had set up an emergency shelter. \n",
      "When the flood disappeared two days later, Nancy immediately went back to the \"island.\" Lizzie was gone. She was one of 19 cows that Nancy had lost in the flood. \"I owe my life to her,\" said Nancy with tears. \n",
      "question:What did Nancy try to do before she fell over? \n",
      "options:\n",
      "A:Measure the depth of the river\n",
      "B:Look for a fallen tree trunk\n",
      "C:Protect her cows from being drowned\n",
      "D:Run away from the flooded farm. \n",
      "Please select the best option for the question based on the content in the context, answer:C\n",
      "\n",
      "the GT: C\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_checkpoint)\n",
    "model = PeftModel.from_pretrained(model, \"./results/GPT2-epoch5/checkpoint-13730\")\n",
    "\n",
    "respone = inference(raw_dataset['test'][0],model)\n",
    "print(respone)\n",
    "print(f\"\\nthe GT: {raw_dataset['test'][0]['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们写一个函数来测试一下准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, dataset):\n",
    "    right=0.\n",
    "    for idx,instance in enumerate(dataset):\n",
    "        gt = instance[\"answer\"].lower()\n",
    "        respone = inference(instance, model)\n",
    "        pred = respone[-1].lower()\n",
    "        if pred==gt:\n",
    "            right +=1\n",
    "    print(f\"accurancy: {right/len(dataset)}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accurancy: 0.2578268876611418\n"
     ]
    }
   ],
   "source": [
    "eval(model, raw_dataset[\"validation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 改进：\n",
    "1. COT\n",
    "2. 修改生成的参数\n",
    "3. 微调方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
