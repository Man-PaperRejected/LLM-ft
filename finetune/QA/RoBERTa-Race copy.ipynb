{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RACE数据集，英语阅读理解多项选择QA\n",
    "\n",
    "1. 使用RoBERTa，基于multichoice头，完成模型微调（不同类型的QA任务是不同的）\n",
    "2. 尝试更换为MLM任务进行模型微调\n",
    "3. 了解DataCollator\n",
    "4. 自定义Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载数据集\n",
    "使用 datasets 库加载 imdb 数据集。这个库会自动下载并缓存数据。\n",
    "也可以下载到本地，这样可以防止网络问题导致代码执行失败（尽管已经下载到缓存）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda3/envs/llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = \"/data/Dataset/LLM-dataset/race\"\n",
    "raw_dataset = load_dataset(dataset_name, \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['example_id', 'article', 'answer', 'question', 'options'],\n",
      "        num_rows: 4934\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['example_id', 'article', 'answer', 'question', 'options'],\n",
      "        num_rows: 87866\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['example_id', 'article', 'answer', 'question', 'options'],\n",
      "        num_rows: 4887\n",
      "    })\n",
      "})\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "Dataset({\n",
      "    features: ['example_id', 'article', 'answer', 'question', 'options'],\n",
      "    num_rows: 87866\n",
      "})\n",
      "{'example_id': 'high19088.txt', 'article': 'Last week I talked with some of my students about what they wanted to do after they graduated, and what kind of job prospects  they thought they had.\\nGiven that I teach students who are training to be doctors, I was surprised do find that most thought that they would not be able to get the jobs they wanted without \"outside help\". \"What kind of help is that?\" I asked, expecting them to tell me that they would need a   or family friend to help them out.\\n\"Surgery ,\" one replied.\\nI was pretty alarmed by that response. It seems that the graduates of today are increasingly willing to go under the knife to get ahead of others when it comes to getting a job .\\nOne girl told me that she was considering surgery to increase her height. \"They break your legs, put in special extending screws, and slowly expand the gap between the two ends of the bone as it re-grows, you can get at least 5 cm taller!\"\\nAt that point, I was shocked. I am short, I can\\'t deny that, but I don\\'t think I would put myself through months of agony just to be a few centimetres taller. I don\\'t even bother to wear shoes with thick soles, as I\\'m not trying to hide the fact that I am just not tall!\\nIt seems to me that there is a trend towards wanting \"perfection\" , and that is an ideal that just does not exist in reality.\\nNo one is born perfect, yet magazines, TV shows and movies present images of thin, tall, beautiful people as being the norm. Advertisements for slimming aids, beauty treatments and cosmetic surgery clinics fill the pages of newspapers, further creating an idea that \"perfection\" is a requirement, and that it must be purchased, no matter what the cost. In my opinion, skills, rather than appearance, should determine how successful a person is in his/her chosen career.', 'answer': 'C', 'question': 'We can know from the passage that the author works as a_.', 'options': ['doctor', 'model', 'teacher', 'reporter']}\n"
     ]
    }
   ],
   "source": [
    "'''可以直接print 看一下数据集的概况'''\n",
    "print(raw_dataset)\n",
    "\n",
    "'''由于返回的是train 和 test 两个split 也可以直接像字典一样进行索引'''\n",
    "print(type(raw_dataset['train']))\n",
    "print(raw_dataset['train'])\n",
    "\n",
    "print(raw_dataset['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理 (Tokenization)\n",
    "这里的处理稍微有所不一样，针对Decoder only的模型，我们期望其对答案进行生成\n",
    "1. 我们需要构造：article + question + options + answer的Prompt\n",
    "2. 对于labels，我们不计算answer之前的token预测loss，专注于答案生成的loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[1102, 16886, 25, 5956, 1285, 314, 6619, 351, 617, 286, 616, 2444, 546, 644, 484, 2227, 284, 466, 706, 484, 18303, 11, 290, 644, 1611, 286, 1693, 13285, 220, 484, 1807, 484, 550, 13, 198, 15056, 326, 314, 4545, 2444, 508, 389, 3047, 284, 307, 7519, 11, 314, 373, 6655, 466, 1064, 326, 749, 1807, 326, 484, 561, 407, 307, 1498, 284, 651, 262, 3946, 484, 2227, 1231, 366, 43435, 1037, 1911, 366, 2061, 1611, 286, 1037, 318, 326, 1701, 314, 1965, 11, 12451, 606, 284, 1560, 502, 326, 484, 561, 761, 257, 220, 220, 393, 1641, 1545, 284, 1037, 606, 503, 13, 198, 1, 14214, 7076, 42911, 530, 8712, 13, 198, 40, 373, 2495, 32064, 416, 326, 2882, 13, 632, 2331, 326, 262, 19087, 286, 1909, 389, 6481, 4684, 284, 467, 739, 262, 9845, 284, 651, 4058, 286, 1854, 618, 340, 2058, 284, 1972, 257, 1693, 764, 198, 3198, 2576, 1297, 502, 326, 673, 373, 6402, 8185, 284, 2620, 607, 6001, 13, 366, 2990, 2270, 534, 7405, 11, 1234, 287, 2041, 16610, 23742, 11, 290, 6364, 4292, 262, 7625, 1022, 262, 734, 5645, 286, 262, 9970, 355, 340, 302, 12, 70, 8516, 11, 345, 460, 651, 379, 1551, 642, 12067, 25242, 2474, 198, 2953, 326, 966, 11, 314, 373, 11472, 13, 314, 716, 1790, 11, 314, 460, 470, 10129, 326, 11, 475, 314, 836, 470, 892, 314, 561, 1234, 3589, 832, 1933, 286, 35358, 655, 284, 307, 257, 1178, 1247, 38813, 411, 25242, 13, 314, 836, 470, 772, 11393, 284, 5806, 10012, 351, 6546, 1540, 274, 11, 355, 314, 1101, 407, 2111, 284, 7808, 262, 1109, 326, 314, 716, 655, 407, 7331, 0, 198, 1026, 2331, 284, 502, 326, 612, 318, 257, 5182, 3371, 10291, 366, 25833, 295, 1, 837, 290, 326, 318, 281, 7306, 326, 655, 857, 407, 2152, 287, 3950, 13, 198, 2949, 530, 318, 4642, 2818, 11, 1865, 16695, 11, 3195, 2523, 290, 6918, 1944, 4263, 286, 7888, 11, 7331, 11, 4950, 661, 355, 852, 262, 2593, 13, 1215, 11371, 329, 18862, 2229, 31378, 11, 8737, 13820, 290, 27284, 8185, 21434, 6070, 262, 5468, 286, 14741, 11, 2252, 4441, 281, 2126, 326, 366, 25833, 295, 1, 318, 257, 9079, 11, 290, 326, 340, 1276, 307, 8155, 11, 645, 2300, 644, 262, 1575, 13, 554, 616, 4459, 11, 4678, 11, 2138, 621, 5585, 11, 815, 5004, 703, 4388, 257, 1048, 318, 287, 465, 14, 372, 7147, 3451, 13, 220, 198, 25652, 25, 1135, 460, 760, 422, 262, 10066, 326, 262, 1772, 2499, 355, 257, 44807, 220, 198, 25811, 25, 198, 32, 25, 35580, 198, 33, 25, 19849, 198, 34, 25, 660, 3493, 198, 35, 25, 260, 26634, 13, 220, 198, 5492, 2922, 262, 1266, 3038, 329, 262, 1808, 1912, 319, 262, 2695, 287, 262, 4732, 11, 3280, 25, 34], [1102, 16886, 25, 5956, 1285, 314, 6619, 351, 617, 286, 616, 2444, 546, 644, 484, 2227, 284, 466, 706, 484, 18303, 11, 290, 644, 1611, 286, 1693, 13285, 220, 484, 1807, 484, 550, 13, 198, 15056, 326, 314, 4545, 2444, 508, 389, 3047, 284, 307, 7519, 11, 314, 373, 6655, 466, 1064, 326, 749, 1807, 326, 484, 561, 407, 307, 1498, 284, 651, 262, 3946, 484, 2227, 1231, 366, 43435, 1037, 1911, 366, 2061, 1611, 286, 1037, 318, 326, 1701, 314, 1965, 11, 12451, 606, 284, 1560, 502, 326, 484, 561, 761, 257, 220, 220, 393, 1641, 1545, 284, 1037, 606, 503, 13, 198, 1, 14214, 7076, 42911, 530, 8712, 13, 198, 40, 373, 2495, 32064, 416, 326, 2882, 13, 632, 2331, 326, 262, 19087, 286, 1909, 389, 6481, 4684, 284, 467, 739, 262, 9845, 284, 651, 4058, 286, 1854, 618, 340, 2058, 284, 1972, 257, 1693, 764, 198, 3198, 2576, 1297, 502, 326, 673, 373, 6402, 8185, 284, 2620, 607, 6001, 13, 366, 2990, 2270, 534, 7405, 11, 1234, 287, 2041, 16610, 23742, 11, 290, 6364, 4292, 262, 7625, 1022, 262, 734, 5645, 286, 262, 9970, 355, 340, 302, 12, 70, 8516, 11, 345, 460, 651, 379, 1551, 642, 12067, 25242, 2474, 198, 2953, 326, 966, 11, 314, 373, 11472, 13, 314, 716, 1790, 11, 314, 460, 470, 10129, 326, 11, 475, 314, 836, 470, 892, 314, 561, 1234, 3589, 832, 1933, 286, 35358, 655, 284, 307, 257, 1178, 1247, 38813, 411, 25242, 13, 314, 836, 470, 772, 11393, 284, 5806, 10012, 351, 6546, 1540, 274, 11, 355, 314, 1101, 407, 2111, 284, 7808, 262, 1109, 326, 314, 716, 655, 407, 7331, 0, 198, 1026, 2331, 284, 502, 326, 612, 318, 257, 5182, 3371, 10291, 366, 25833, 295, 1, 837, 290, 326, 318, 281, 7306, 326, 655, 857, 407, 2152, 287, 3950, 13, 198, 2949, 530, 318, 4642, 2818, 11, 1865, 16695, 11, 3195, 2523, 290, 6918, 1944, 4263, 286, 7888, 11, 7331, 11, 4950, 661, 355, 852, 262, 2593, 13, 1215, 11371, 329, 18862, 2229, 31378, 11, 8737, 13820, 290, 27284, 8185, 21434, 6070, 262, 5468, 286, 14741, 11, 2252, 4441, 281, 2126, 326, 366, 25833, 295, 1, 318, 257, 9079, 11, 290, 326, 340, 1276, 307, 8155, 11, 645, 2300, 644, 262, 1575, 13, 554, 616, 4459, 11, 4678, 11, 2138, 621, 5585, 11, 815, 5004, 703, 4388, 257, 1048, 318, 287, 465, 14, 372, 7147, 3451, 13, 220, 198, 25652, 25, 7085, 19087, 1909, 1210, 284, 27284, 8185, 284, 44807, 220, 198, 25811, 25, 198, 32, 25, 3876, 563, 257, 1365, 582, 14, 8580, 198, 33, 25, 9423, 462, 257, 2746, 198, 34, 25, 1136, 281, 4621, 625, 1854, 287, 1693, 12, 20088, 889, 198, 35, 25, 1078, 974, 517, 21099, 3808, 13, 220, 198, 5492, 2922, 262, 1266, 3038, 329, 262, 1808, 1912, 319, 262, 2695, 287, 262, 4732, 11, 3280, 25, 34], [1102, 16886, 25, 5956, 1285, 314, 6619, 351, 617, 286, 616, 2444, 546, 644, 484, 2227, 284, 466, 706, 484, 18303, 11, 290, 644, 1611, 286, 1693, 13285, 220, 484, 1807, 484, 550, 13, 198, 15056, 326, 314, 4545, 2444, 508, 389, 3047, 284, 307, 7519, 11, 314, 373, 6655, 466, 1064, 326, 749, 1807, 326, 484, 561, 407, 307, 1498, 284, 651, 262, 3946, 484, 2227, 1231, 366, 43435, 1037, 1911, 366, 2061, 1611, 286, 1037, 318, 326, 1701, 314, 1965, 11, 12451, 606, 284, 1560, 502, 326, 484, 561, 761, 257, 220, 220, 393, 1641, 1545, 284, 1037, 606, 503, 13, 198, 1, 14214, 7076, 42911, 530, 8712, 13, 198, 40, 373, 2495, 32064, 416, 326, 2882, 13, 632, 2331, 326, 262, 19087, 286, 1909, 389, 6481, 4684, 284, 467, 739, 262, 9845, 284, 651, 4058, 286, 1854, 618, 340, 2058, 284, 1972, 257, 1693, 764, 198, 3198, 2576, 1297, 502, 326, 673, 373, 6402, 8185, 284, 2620, 607, 6001, 13, 366, 2990, 2270, 534, 7405, 11, 1234, 287, 2041, 16610, 23742, 11, 290, 6364, 4292, 262, 7625, 1022, 262, 734, 5645, 286, 262, 9970, 355, 340, 302, 12, 70, 8516, 11, 345, 460, 651, 379, 1551, 642, 12067, 25242, 2474, 198, 2953, 326, 966, 11, 314, 373, 11472, 13, 314, 716, 1790, 11, 314, 460, 470, 10129, 326, 11, 475, 314, 836, 470, 892, 314, 561, 1234, 3589, 832, 1933, 286, 35358, 655, 284, 307, 257, 1178, 1247, 38813, 411, 25242, 13, 314, 836, 470, 772, 11393, 284, 5806, 10012, 351, 6546, 1540, 274, 11, 355, 314, 1101, 407, 2111, 284, 7808, 262, 1109, 326, 314, 716, 655, 407, 7331, 0, 198, 1026, 2331, 284, 502, 326, 612, 318, 257, 5182, 3371, 10291, 366, 25833, 295, 1, 837, 290, 326, 318, 281, 7306, 326, 655, 857, 407, 2152, 287, 3950, 13, 198, 2949, 530, 318, 4642, 2818, 11, 1865, 16695, 11, 3195, 2523, 290, 6918, 1944, 4263, 286, 7888, 11, 7331, 11, 4950, 661, 355, 852, 262, 2593, 13, 1215, 11371, 329, 18862, 2229, 31378, 11, 8737, 13820, 290, 27284, 8185, 21434, 6070, 262, 5468, 286, 14741, 11, 2252, 4441, 281, 2126, 326, 366, 25833, 295, 1, 318, 257, 9079, 11, 290, 326, 340, 1276, 307, 8155, 11, 645, 2300, 644, 262, 1575, 13, 554, 616, 4459, 11, 4678, 11, 2138, 621, 5585, 11, 815, 5004, 703, 4388, 257, 1048, 318, 287, 465, 14, 372, 7147, 3451, 13, 220, 198, 25652, 25, 4821, 284, 262, 10066, 11, 262, 1772, 5804, 326, 44807, 220, 198, 25811, 25, 198, 32, 25, 47057, 815, 5001, 20187, 11, 4232, 262, 1575, 198, 33, 25, 270, 338, 826, 329, 19087, 284, 1265, 329, 1854, 284, 1037, 606, 503, 287, 10988, 329, 3946, 198, 34, 25, 270, 318, 530, 338, 5585, 2427, 286, 4678, 326, 1107, 6067, 287, 530, 338, 3451, 198, 35, 25, 11431, 389, 284, 8138, 329, 15850, 1862, 661, 287, 511, 6095, 329, 8185, 13, 220, 198, 5492, 2922, 262, 1266, 3038, 329, 262, 1808, 1912, 319, 262, 2695, 287, 262, 4732, 11, 3280, 25, 35], [1102, 16886, 25, 5956, 1285, 314, 6619, 351, 617, 286, 616, 2444, 546, 644, 484, 2227, 284, 466, 706, 484, 18303, 11, 290, 644, 1611, 286, 1693, 13285, 220, 484, 1807, 484, 550, 13, 198, 15056, 326, 314, 4545, 2444, 508, 389, 3047, 284, 307, 7519, 11, 314, 373, 6655, 466, 1064, 326, 749, 1807, 326, 484, 561, 407, 307, 1498, 284, 651, 262, 3946, 484, 2227, 1231, 366, 43435, 1037, 1911, 366, 2061, 1611, 286, 1037, 318, 326, 1701, 314, 1965, 11, 12451, 606, 284, 1560, 502, 326, 484, 561, 761, 257, 220, 220, 393, 1641, 1545, 284, 1037, 606, 503, 13, 198, 1, 14214, 7076, 42911, 530, 8712, 13, 198, 40, 373, 2495, 32064, 416, 326, 2882, 13, 632, 2331, 326, 262, 19087, 286, 1909, 389, 6481, 4684, 284, 467, 739, 262, 9845, 284, 651, 4058, 286, 1854, 618, 340, 2058, 284, 1972, 257, 1693, 764, 198, 3198, 2576, 1297, 502, 326, 673, 373, 6402, 8185, 284, 2620, 607, 6001, 13, 366, 2990, 2270, 534, 7405, 11, 1234, 287, 2041, 16610, 23742, 11, 290, 6364, 4292, 262, 7625, 1022, 262, 734, 5645, 286, 262, 9970, 355, 340, 302, 12, 70, 8516, 11, 345, 460, 651, 379, 1551, 642, 12067, 25242, 2474, 198, 2953, 326, 966, 11, 314, 373, 11472, 13, 314, 716, 1790, 11, 314, 460, 470, 10129, 326, 11, 475, 314, 836, 470, 892, 314, 561, 1234, 3589, 832, 1933, 286, 35358, 655, 284, 307, 257, 1178, 1247, 38813, 411, 25242, 13, 314, 836, 470, 772, 11393, 284, 5806, 10012, 351, 6546, 1540, 274, 11, 355, 314, 1101, 407, 2111, 284, 7808, 262, 1109, 326, 314, 716, 655, 407, 7331, 0, 198, 1026, 2331, 284, 502, 326, 612, 318, 257, 5182, 3371, 10291, 366, 25833, 295, 1, 837, 290, 326, 318, 281, 7306, 326, 655, 857, 407, 2152, 287, 3950, 13, 198, 2949, 530, 318, 4642, 2818, 11, 1865, 16695, 11, 3195, 2523, 290, 6918, 1944, 4263, 286, 7888, 11, 7331, 11, 4950, 661, 355, 852, 262, 2593, 13, 1215, 11371, 329, 18862, 2229, 31378, 11, 8737, 13820, 290, 27284, 8185, 21434, 6070, 262, 5468, 286, 14741, 11, 2252, 4441, 281, 2126, 326, 366, 25833, 295, 1, 318, 257, 9079, 11, 290, 326, 340, 1276, 307, 8155, 11, 645, 2300, 644, 262, 1575, 13, 554, 616, 4459, 11, 4678, 11, 2138, 621, 5585, 11, 815, 5004, 703, 4388, 257, 1048, 318, 287, 465, 14, 372, 7147, 3451, 13, 220, 198, 25652, 25, 13828, 6, 264, 262, 1266, 3670, 329, 262, 10066, 30, 13, 220, 198, 25811, 25, 198, 32, 25, 20917, 17701, 12632, 8192, 16038, 23600, 602, 198, 33, 25, 20917, 17701, 12632, 6803, 284, 39037, 329, 11625, 19161, 198, 34, 25, 20917, 17701, 12632, 6, 29525, 7994, 46161, 39037, 198, 35, 25, 20917, 17701, 12632, 15399, 257, 20615, 49465, 287, 15768, 12, 20088, 889, 13, 220, 198, 5492, 2922, 262, 1266, 3038, 329, 262, 1808, 1912, 319, 262, 2695, 287, 262, 4732, 11, 3280, 25, 33]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 34], [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 34], [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 35], [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 33]]}\n",
      "shape of input_ids :(4,462)\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "model_checkpoint = \"/data/Weights/gpt2/gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "\n",
    "\n",
    "def preprocess(instances):\n",
    "    article = instances[\"article\"]\n",
    "    question = instances[\"question\"]\n",
    "    options = instances[\"options\"]\n",
    "    answer = instances['answer']\n",
    "\n",
    "    results = {\n",
    "        'input_ids': [],\n",
    "        'attention_mask': [],\n",
    "        'labels': []\n",
    "    }\n",
    "    for art,ques,opt,ans in zip(article,question,options,answer):\n",
    "        opt_str = f\"\\nA:{opt[0]}\\nB:{opt[1]}\\nC:{opt[2]}\\nD:{opt[3]}\"\n",
    "        prompt = f\"contex:{art} \\nquestion:{ques} \\noptions:{opt_str}. \\nPlease select the best option for the question based on the content in the context, answer:\"        \n",
    "\n",
    "        tokenized_context = tokenizer(\n",
    "            prompt,\n",
    "            max_length=512,\n",
    "            padding=False,            \n",
    "            truncation=\"only_first\",  \n",
    "        ) \n",
    "        # answer之前的loss，不计算\n",
    "        labels = [-100]*len(tokenized_context['input_ids'])\n",
    "        \n",
    "        # 对Answer进行encode，并补充进labels\n",
    "        tokenized_ans = tokenizer(\n",
    "            ans,\n",
    "            max_length=512,\n",
    "            padding=False,            \n",
    "            truncation=\"only_first\",  \n",
    "        ) \n",
    "        labels.extend(tokenized_ans['input_ids'])\n",
    "        \n",
    "        for k in tokenized_context.keys():\n",
    "            tokenized_context[k].extend(tokenized_ans[k])\n",
    "            \n",
    "        results['input_ids'].append(tokenized_context['input_ids'])\n",
    "        results['attention_mask'].append(tokenized_context['attention_mask'])\n",
    "        results['labels'].append(labels)\n",
    "    \n",
    "    return results\n",
    "\n",
    "c = preprocess(raw_dataset[\"train\"][0:4])\n",
    "\n",
    "print(c)\n",
    "print(f\"shape of input_ids :({len(c['input_ids'])},{len(c['input_ids'][0])})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contex:Last week I talked with some of my students about what they wanted to do after they graduated, and what kind of job prospects  they thought they had.\n",
      "Given that I teach students who are training to be doctors, I was surprised do find that most thought that they would not be able to get the jobs they wanted without \"outside help\". \"What kind of help is that?\" I asked, expecting them to tell me that they would need a   or family friend to help them out.\n",
      "\"Surgery ,\" one replied.\n",
      "I was pretty alarmed by that response. It seems that the graduates of today are increasingly willing to go under the knife to get ahead of others when it comes to getting a job .\n",
      "One girl told me that she was considering surgery to increase her height. \"They break your legs, put in special extending screws, and slowly expand the gap between the two ends of the bone as it re-grows, you can get at least 5 cm taller!\"\n",
      "At that point, I was shocked. I am short, I can't deny that, but I don't think I would put myself through months of agony just to be a few centimetres taller. I don't even bother to wear shoes with thick soles, as I'm not trying to hide the fact that I am just not tall!\n",
      "It seems to me that there is a trend towards wanting \"perfection\" , and that is an ideal that just does not exist in reality.\n",
      "No one is born perfect, yet magazines, TV shows and movies present images of thin, tall, beautiful people as being the norm. Advertisements for slimming aids, beauty treatments and cosmetic surgery clinics fill the pages of newspapers, further creating an idea that \"perfection\" is a requirement, and that it must be purchased, no matter what the cost. In my opinion, skills, rather than appearance, should determine how successful a person is in his/her chosen career. \n",
      "question:We can know from the passage that the author works as a_. \n",
      "options:\n",
      "A:doctor\n",
      "B:model\n",
      "C:teacher\n",
      "D:reporter. \n",
      "Please select the best option for the question based on the content in the context, answer:C\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(c[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|██████████| 4934/4934 [00:04<00:00, 1160.78 examples/s]\n",
      "Map (num_proc=4): 100%|██████████| 87866/87866 [01:01<00:00, 1428.62 examples/s]\n",
      "Map (num_proc=4): 100%|██████████| 4887/4887 [00:04<00:00, 1179.60 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# 进行批处理，这里我们直接在map函数中将文本信息全部删除即可\n",
    "tokenized_dataset = raw_dataset.map(preprocess, batched=True,remove_columns=raw_dataset[\"train\"].column_names, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1102,\n",
       "  16886,\n",
       "  25,\n",
       "  5956,\n",
       "  1285,\n",
       "  314,\n",
       "  6619,\n",
       "  351,\n",
       "  617,\n",
       "  286,\n",
       "  616,\n",
       "  2444,\n",
       "  546,\n",
       "  644,\n",
       "  484,\n",
       "  2227,\n",
       "  284,\n",
       "  466,\n",
       "  706,\n",
       "  484,\n",
       "  18303,\n",
       "  11,\n",
       "  290,\n",
       "  644,\n",
       "  1611,\n",
       "  286,\n",
       "  1693,\n",
       "  13285,\n",
       "  220,\n",
       "  484,\n",
       "  1807,\n",
       "  484,\n",
       "  550,\n",
       "  13,\n",
       "  198,\n",
       "  15056,\n",
       "  326,\n",
       "  314,\n",
       "  4545,\n",
       "  2444,\n",
       "  508,\n",
       "  389,\n",
       "  3047,\n",
       "  284,\n",
       "  307,\n",
       "  7519,\n",
       "  11,\n",
       "  314,\n",
       "  373,\n",
       "  6655,\n",
       "  466,\n",
       "  1064,\n",
       "  326,\n",
       "  749,\n",
       "  1807,\n",
       "  326,\n",
       "  484,\n",
       "  561,\n",
       "  407,\n",
       "  307,\n",
       "  1498,\n",
       "  284,\n",
       "  651,\n",
       "  262,\n",
       "  3946,\n",
       "  484,\n",
       "  2227,\n",
       "  1231,\n",
       "  366,\n",
       "  43435,\n",
       "  1037,\n",
       "  1911,\n",
       "  366,\n",
       "  2061,\n",
       "  1611,\n",
       "  286,\n",
       "  1037,\n",
       "  318,\n",
       "  326,\n",
       "  1701,\n",
       "  314,\n",
       "  1965,\n",
       "  11,\n",
       "  12451,\n",
       "  606,\n",
       "  284,\n",
       "  1560,\n",
       "  502,\n",
       "  326,\n",
       "  484,\n",
       "  561,\n",
       "  761,\n",
       "  257,\n",
       "  220,\n",
       "  220,\n",
       "  393,\n",
       "  1641,\n",
       "  1545,\n",
       "  284,\n",
       "  1037,\n",
       "  606,\n",
       "  503,\n",
       "  13,\n",
       "  198,\n",
       "  1,\n",
       "  14214,\n",
       "  7076,\n",
       "  42911,\n",
       "  530,\n",
       "  8712,\n",
       "  13,\n",
       "  198,\n",
       "  40,\n",
       "  373,\n",
       "  2495,\n",
       "  32064,\n",
       "  416,\n",
       "  326,\n",
       "  2882,\n",
       "  13,\n",
       "  632,\n",
       "  2331,\n",
       "  326,\n",
       "  262,\n",
       "  19087,\n",
       "  286,\n",
       "  1909,\n",
       "  389,\n",
       "  6481,\n",
       "  4684,\n",
       "  284,\n",
       "  467,\n",
       "  739,\n",
       "  262,\n",
       "  9845,\n",
       "  284,\n",
       "  651,\n",
       "  4058,\n",
       "  286,\n",
       "  1854,\n",
       "  618,\n",
       "  340,\n",
       "  2058,\n",
       "  284,\n",
       "  1972,\n",
       "  257,\n",
       "  1693,\n",
       "  764,\n",
       "  198,\n",
       "  3198,\n",
       "  2576,\n",
       "  1297,\n",
       "  502,\n",
       "  326,\n",
       "  673,\n",
       "  373,\n",
       "  6402,\n",
       "  8185,\n",
       "  284,\n",
       "  2620,\n",
       "  607,\n",
       "  6001,\n",
       "  13,\n",
       "  366,\n",
       "  2990,\n",
       "  2270,\n",
       "  534,\n",
       "  7405,\n",
       "  11,\n",
       "  1234,\n",
       "  287,\n",
       "  2041,\n",
       "  16610,\n",
       "  23742,\n",
       "  11,\n",
       "  290,\n",
       "  6364,\n",
       "  4292,\n",
       "  262,\n",
       "  7625,\n",
       "  1022,\n",
       "  262,\n",
       "  734,\n",
       "  5645,\n",
       "  286,\n",
       "  262,\n",
       "  9970,\n",
       "  355,\n",
       "  340,\n",
       "  302,\n",
       "  12,\n",
       "  70,\n",
       "  8516,\n",
       "  11,\n",
       "  345,\n",
       "  460,\n",
       "  651,\n",
       "  379,\n",
       "  1551,\n",
       "  642,\n",
       "  12067,\n",
       "  25242,\n",
       "  2474,\n",
       "  198,\n",
       "  2953,\n",
       "  326,\n",
       "  966,\n",
       "  11,\n",
       "  314,\n",
       "  373,\n",
       "  11472,\n",
       "  13,\n",
       "  314,\n",
       "  716,\n",
       "  1790,\n",
       "  11,\n",
       "  314,\n",
       "  460,\n",
       "  470,\n",
       "  10129,\n",
       "  326,\n",
       "  11,\n",
       "  475,\n",
       "  314,\n",
       "  836,\n",
       "  470,\n",
       "  892,\n",
       "  314,\n",
       "  561,\n",
       "  1234,\n",
       "  3589,\n",
       "  832,\n",
       "  1933,\n",
       "  286,\n",
       "  35358,\n",
       "  655,\n",
       "  284,\n",
       "  307,\n",
       "  257,\n",
       "  1178,\n",
       "  1247,\n",
       "  38813,\n",
       "  411,\n",
       "  25242,\n",
       "  13,\n",
       "  314,\n",
       "  836,\n",
       "  470,\n",
       "  772,\n",
       "  11393,\n",
       "  284,\n",
       "  5806,\n",
       "  10012,\n",
       "  351,\n",
       "  6546,\n",
       "  1540,\n",
       "  274,\n",
       "  11,\n",
       "  355,\n",
       "  314,\n",
       "  1101,\n",
       "  407,\n",
       "  2111,\n",
       "  284,\n",
       "  7808,\n",
       "  262,\n",
       "  1109,\n",
       "  326,\n",
       "  314,\n",
       "  716,\n",
       "  655,\n",
       "  407,\n",
       "  7331,\n",
       "  0,\n",
       "  198,\n",
       "  1026,\n",
       "  2331,\n",
       "  284,\n",
       "  502,\n",
       "  326,\n",
       "  612,\n",
       "  318,\n",
       "  257,\n",
       "  5182,\n",
       "  3371,\n",
       "  10291,\n",
       "  366,\n",
       "  25833,\n",
       "  295,\n",
       "  1,\n",
       "  837,\n",
       "  290,\n",
       "  326,\n",
       "  318,\n",
       "  281,\n",
       "  7306,\n",
       "  326,\n",
       "  655,\n",
       "  857,\n",
       "  407,\n",
       "  2152,\n",
       "  287,\n",
       "  3950,\n",
       "  13,\n",
       "  198,\n",
       "  2949,\n",
       "  530,\n",
       "  318,\n",
       "  4642,\n",
       "  2818,\n",
       "  11,\n",
       "  1865,\n",
       "  16695,\n",
       "  11,\n",
       "  3195,\n",
       "  2523,\n",
       "  290,\n",
       "  6918,\n",
       "  1944,\n",
       "  4263,\n",
       "  286,\n",
       "  7888,\n",
       "  11,\n",
       "  7331,\n",
       "  11,\n",
       "  4950,\n",
       "  661,\n",
       "  355,\n",
       "  852,\n",
       "  262,\n",
       "  2593,\n",
       "  13,\n",
       "  1215,\n",
       "  11371,\n",
       "  329,\n",
       "  18862,\n",
       "  2229,\n",
       "  31378,\n",
       "  11,\n",
       "  8737,\n",
       "  13820,\n",
       "  290,\n",
       "  27284,\n",
       "  8185,\n",
       "  21434,\n",
       "  6070,\n",
       "  262,\n",
       "  5468,\n",
       "  286,\n",
       "  14741,\n",
       "  11,\n",
       "  2252,\n",
       "  4441,\n",
       "  281,\n",
       "  2126,\n",
       "  326,\n",
       "  366,\n",
       "  25833,\n",
       "  295,\n",
       "  1,\n",
       "  318,\n",
       "  257,\n",
       "  9079,\n",
       "  11,\n",
       "  290,\n",
       "  326,\n",
       "  340,\n",
       "  1276,\n",
       "  307,\n",
       "  8155,\n",
       "  11,\n",
       "  645,\n",
       "  2300,\n",
       "  644,\n",
       "  262,\n",
       "  1575,\n",
       "  13,\n",
       "  554,\n",
       "  616,\n",
       "  4459,\n",
       "  11,\n",
       "  4678,\n",
       "  11,\n",
       "  2138,\n",
       "  621,\n",
       "  5585,\n",
       "  11,\n",
       "  815,\n",
       "  5004,\n",
       "  703,\n",
       "  4388,\n",
       "  257,\n",
       "  1048,\n",
       "  318,\n",
       "  287,\n",
       "  465,\n",
       "  14,\n",
       "  372,\n",
       "  7147,\n",
       "  3451,\n",
       "  13,\n",
       "  220,\n",
       "  198,\n",
       "  25652,\n",
       "  25,\n",
       "  1135,\n",
       "  460,\n",
       "  760,\n",
       "  422,\n",
       "  262,\n",
       "  10066,\n",
       "  326,\n",
       "  262,\n",
       "  1772,\n",
       "  2499,\n",
       "  355,\n",
       "  257,\n",
       "  44807,\n",
       "  220,\n",
       "  198,\n",
       "  25811,\n",
       "  25,\n",
       "  198,\n",
       "  32,\n",
       "  25,\n",
       "  35580,\n",
       "  198,\n",
       "  33,\n",
       "  25,\n",
       "  19849,\n",
       "  198,\n",
       "  34,\n",
       "  25,\n",
       "  660,\n",
       "  3493,\n",
       "  198,\n",
       "  35,\n",
       "  25,\n",
       "  260,\n",
       "  26634,\n",
       "  13,\n",
       "  220,\n",
       "  198,\n",
       "  5492,\n",
       "  2922,\n",
       "  262,\n",
       "  1266,\n",
       "  3038,\n",
       "  329,\n",
       "  262,\n",
       "  1808,\n",
       "  1912,\n",
       "  319,\n",
       "  262,\n",
       "  2695,\n",
       "  287,\n",
       "  262,\n",
       "  4732,\n",
       "  11,\n",
       "  3280,\n",
       "  25,\n",
       "  34],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'labels': [-100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  34]}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置数据集格式为 PyTorch tensors (如果使用 TensorFlow 则设置为 \"tf\")\n",
    "tokenized_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DataCollatorForLanguageModeling` 是用于处理输入数据的动态填充（padding）和标签（labels）的生成，其会复制input_ids 作为labels，这里与我们的操作有所不一样，所有要自定义一个dataCollator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "class QADataCollator(DataCollatorForLanguageModeling):\n",
    "    def __call__(self, features):\n",
    "        batch = self.tokenizer.pad(\n",
    "            {\"input_ids\": [f[\"input_ids\"] for f in features]},\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        \n",
    "        labels = [f[\"labels\"] for f in features]\n",
    "        padded_labels = []\n",
    "        for i, label_seq in enumerate(labels):\n",
    "            # 计算需要填充的长度\n",
    "            pad_len = batch[\"input_ids\"].shape[1] - len(label_seq)\n",
    "            # 在 labels 的填充部分用 -100（PyTorch 会忽略这些位置的loss）\n",
    "            padded_seq = torch.cat([\n",
    "                torch.tensor(label_seq),\n",
    "                torch.full((pad_len,), -100, dtype=torch.long)\n",
    "            ])\n",
    "            padded_labels.append(padded_seq)\n",
    "        batch[\"labels\"] = torch.stack(padded_labels)\n",
    "        \n",
    "        return batch\n",
    "\n",
    "data_collator = QADataCollator(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    "    pad_to_multiple_of=8,\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_688971/2306962191.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(label_seq),\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 1102, 16886,    25,  ..., 50256, 50256, 50256],\n",
       "        [ 1102, 16886,    25,  ..., 50256, 50256, 50256],\n",
       "        [ 1102, 16886,    25,  ...,  3280,    25,    35],\n",
       "        [ 1102, 16886,    25,  ..., 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[-100, -100, -100,  ..., -100, -100, -100],\n",
       "        [-100, -100, -100,  ..., -100, -100, -100],\n",
       "        [-100, -100, -100,  ..., -100, -100,   35],\n",
       "        [-100, -100, -100,  ..., -100, -100, -100]])}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 传入四个数据进行查看\n",
    "batch = data_collator([tokenized_dataset['train'][i] for i in range(4)])\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  加载RoBERTa预训练模型\n",
    "加载带有适合下游任务头部的模型。对于文本分类，我们使用 AutoModelForSequenceClassification。其就是在最后加了层MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D(nf=2304, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=768)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D(nf=3072, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=3072)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_checkpoint)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(instances, model):\n",
    "    article = instances[\"article\"]\n",
    "    question = instances[\"question\"]\n",
    "    options = instances[\"options\"]\n",
    "    answer = instances['answer']\n",
    "    model = model.to('cuda')\n",
    "    \n",
    "    opt_str = f\"\\nA:{options[0]}\\nB:{options[1]}\\nC:{options[2]}\\nD:{options[3]}\"\n",
    "    prompt = f\"contex:{article} \\nquestion:{question} \\noptions:{opt_str}. \\nPlease select the best option for the question based on the content in the context, answer:\"     \n",
    "    results = tokenizer(\n",
    "        prompt,\n",
    "        max_length=512,\n",
    "        padding=False,            \n",
    "        truncation=\"only_first\",  \n",
    "        return_tensors=\"pt\"\n",
    "    )    \n",
    "    \n",
    "    outputs = model.generate(\n",
    "                        input_ids=results[\"input_ids\"].to(model.device),\n",
    "                        attention_mask=results[\"attention_mask\"].to(model.device),\n",
    "                        num_beams=1,\n",
    "                        max_new_tokens=1,  # 最大生成长度\n",
    "                        num_return_sequences=1,  # 返回1个候选\n",
    "                        do_sample=False,  # 启用随机采样\n",
    "                        top_p=1.0,                     \n",
    "                        temperature=1.0, \n",
    "                        pad_token_id = tokenizer.eos_token_id, \n",
    "                        eos_token_id = tokenizer.eos_token_id,\n",
    "                        )\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contex:Last week I talked with some of my students about what they wanted to do after they graduated, and what kind of job prospects  they thought they had.\n",
      "Given that I teach students who are training to be doctors, I was surprised do find that most thought that they would not be able to get the jobs they wanted without \"outside help\". \"What kind of help is that?\" I asked, expecting them to tell me that they would need a   or family friend to help them out.\n",
      "\"Surgery ,\" one replied.\n",
      "I was pretty alarmed by that response. It seems that the graduates of today are increasingly willing to go under the knife to get ahead of others when it comes to getting a job .\n",
      "One girl told me that she was considering surgery to increase her height. \"They break your legs, put in special extending screws, and slowly expand the gap between the two ends of the bone as it re-grows, you can get at least 5 cm taller!\"\n",
      "At that point, I was shocked. I am short, I can't deny that, but I don't think I would put myself through months of agony just to be a few centimetres taller. I don't even bother to wear shoes with thick soles, as I'm not trying to hide the fact that I am just not tall!\n",
      "It seems to me that there is a trend towards wanting \"perfection\" , and that is an ideal that just does not exist in reality.\n",
      "No one is born perfect, yet magazines, TV shows and movies present images of thin, tall, beautiful people as being the norm. Advertisements for slimming aids, beauty treatments and cosmetic surgery clinics fill the pages of newspapers, further creating an idea that \"perfection\" is a requirement, and that it must be purchased, no matter what the cost. In my opinion, skills, rather than appearance, should determine how successful a person is in his/her chosen career. \n",
      "question:According to the passage, the author believes that_. \n",
      "options:\n",
      "A:everyone should purchase perfection, whatever the cost\n",
      "B:it's right for graduates to ask for others to help them out in hunting for jobs\n",
      "C:it is one's appearance instead of skills that really matters in one's career\n",
      "D:media are to blame for misleading young people in their seeking for surgery. \n",
      "Please select the best option for the question based on the content in the context, answer:\n",
      "\n",
      "A\n"
     ]
    }
   ],
   "source": [
    "respone = inference(raw_dataset['train'][2],model)\n",
    "print(respone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 1102, 16886,    25,  5956,  1285,   314,  6619,   351,   617,   286,\n",
       "            616,  2444,   546,   644,   484,  2227,   284,   466,   706,   484,\n",
       "          18303,    11,   290,   644,  1611,   286,  1693, 13285,   220,   484,\n",
       "           1807,   484,   550,    13,   198, 15056,   326,   314,  4545,  2444,\n",
       "            508,   389,  3047,   284,   307,  7519,    11,   314,   373,  6655,\n",
       "            466,  1064,   326,   749,  1807,   326,   484,   561,   407,   307,\n",
       "           1498,   284,   651,   262,  3946,   484,  2227,  1231,   366, 43435,\n",
       "           1037,  1911,   366,  2061,  1611,   286,  1037,   318,   326,  1701,\n",
       "            314,  1965,    11, 12451,   606,   284,  1560,   502,   326,   484,\n",
       "            561,   761,   257,   220,   220,   393,  1641,  1545,   284,  1037,\n",
       "            606,   503,    13,   198,     1, 14214,  7076, 42911,   530,  8712,\n",
       "             13,   198,    40,   373,  2495, 32064,   416,   326,  2882,    13,\n",
       "            632,  2331,   326,   262, 19087,   286,  1909,   389,  6481,  4684,\n",
       "            284,   467,   739,   262,  9845,   284,   651,  4058,   286,  1854,\n",
       "            618,   340,  2058,   284,  1972,   257,  1693,   764,   198,  3198,\n",
       "           2576,  1297,   502,   326,   673,   373,  6402,  8185,   284,  2620,\n",
       "            607,  6001,    13,   366,  2990,  2270,   534,  7405,    11,  1234,\n",
       "            287,  2041, 16610, 23742,    11,   290,  6364,  4292,   262,  7625,\n",
       "           1022,   262,   734,  5645,   286,   262,  9970,   355,   340,   302,\n",
       "             12,    70,  8516,    11,   345,   460,   651,   379,  1551,   642,\n",
       "          12067, 25242,  2474,   198,  2953,   326,   966,    11,   314,   373,\n",
       "          11472,    13,   314,   716,  1790,    11,   314,   460,   470, 10129,\n",
       "            326,    11,   475,   314,   836,   470,   892,   314,   561,  1234,\n",
       "           3589,   832,  1933,   286, 35358,   655,   284,   307,   257,  1178,\n",
       "           1247, 38813,   411, 25242,    13,   314,   836,   470,   772, 11393,\n",
       "            284,  5806, 10012,   351,  6546,  1540,   274,    11,   355,   314,\n",
       "           1101,   407,  2111,   284,  7808,   262,  1109,   326,   314,   716,\n",
       "            655,   407,  7331,     0,   198,  1026,  2331,   284,   502,   326,\n",
       "            612,   318,   257,  5182,  3371, 10291,   366, 25833,   295,     1,\n",
       "            837,   290,   326,   318,   281,  7306,   326,   655,   857,   407,\n",
       "           2152,   287,  3950,    13,   198,  2949,   530,   318,  4642,  2818,\n",
       "             11,  1865, 16695,    11,  3195,  2523,   290,  6918,  1944,  4263,\n",
       "            286,  7888,    11,  7331,    11,  4950,   661,   355,   852,   262,\n",
       "           2593,    13,  1215, 11371,   329, 18862,  2229, 31378,    11,  8737,\n",
       "          13820,   290, 27284,  8185, 21434,  6070,   262,  5468,   286, 14741,\n",
       "             11,  2252,  4441,   281,  2126,   326,   366, 25833,   295,     1,\n",
       "            318,   257,  9079,    11,   290,   326,   340,  1276,   307,  8155,\n",
       "             11,   645,  2300,   644,   262,  1575,    13,   554,   616,  4459,\n",
       "             11,  4678,    11,  2138,   621,  5585,    11,   815,  5004,   703,\n",
       "           4388,   257,  1048,   318,   287,   465,    14,   372,  7147,  3451,\n",
       "             13,   220,   198, 25652,    25,  1135,   460,   760,   422,   262,\n",
       "          10066,   326,   262,  1772,  2499,   355,   257, 44807,   220,   198,\n",
       "          25811,    25,   198,    32,    25, 35580,   198,    33,    25, 19849,\n",
       "            198,    34,    25,   660,  3493,   198,    35,    25,   260, 26634,\n",
       "             13,   220,   198,  5492,  2922,   262,  1266,  3038,   329,   262,\n",
       "           1808,  1912,   319,   262,  2695,   287,   262,  4732,    11,  3280,\n",
       "             25,    34]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1]]),\n",
       " 'labels': tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100,   34]])}"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_input = tokenized_dataset['train'][0]\n",
    "for k, v in dummy_input.items():\n",
    "    dummy_input[k] = v.unsqueeze(0)\n",
    "dummy_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 462, 50257])\n"
     ]
    }
   ],
   "source": [
    "# 可以看看输出\n",
    "import torch\n",
    "\n",
    "dummy_input = tokenized_dataset['train'][0]\n",
    "for k, v in dummy_input.items():\n",
    "    dummy_input[k] = v.unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**dummy_input)\n",
    "\n",
    "print(outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  定义评估指标\n",
    "这里做的是语言建模，是不需要compute_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT2模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results/GPT2-epoch5\",              # 输出目录，保存模型和日志\n",
    "    eval_strategy=\"epoch\",         # 每个 epoch 结束后进行评估\n",
    "    save_strategy=\"epoch\",               # 每个 epoch 结束后保存模型\n",
    "    learning_rate=2e-5,                  # 学习率\n",
    "    per_device_train_batch_size=16,      # 训练批次大小\n",
    "    per_device_eval_batch_size=16,       # 评估批次大小\n",
    "    num_train_epochs=5,                  # 训练轮数\n",
    "    weight_decay=0.01,                   # 权重衰减\n",
    "    load_best_model_at_end=True,         # 训练结束后加载最佳模型\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    report_to=\"tensorboard\",             # 可以选择 tensorboard, wandb 等\n",
    "    save_total_limit=2,\n",
    "    label_names = [\"labels\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里我们使用P-tuning v2进行微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PrefixTuningConfig, get_peft_model, TaskType, PeftModel\n",
    "\n",
    "# 定义 Prefix Tuning 配置（P-Tuning v2）\n",
    "peft_config = PrefixTuningConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    num_virtual_tokens=20,         # prefix 长度，常用 10-50\n",
    "    encoder_hidden_size=768,       # GPT-2 hidden size\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# 记得解冻lm_head的梯度计算\n",
    "for name, param in model.named_parameters():\n",
    "    if 'lm_head' in name:\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): GPT2LMHeadModel(\n",
      "    (transformer): GPT2Model(\n",
      "      (wte): Embedding(50257, 768)\n",
      "      (wpe): Embedding(1024, 768)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "      (h): ModuleList(\n",
      "        (0-11): 12 x GPT2Block(\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): GPT2Attention(\n",
      "            (c_attn): Conv1D(nf=2304, nx=768)\n",
      "            (c_proj): Conv1D(nf=768, nx=768)\n",
      "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): GPT2MLP(\n",
      "            (c_fc): Conv1D(nf=3072, nx=768)\n",
      "            (c_proj): Conv1D(nf=768, nx=3072)\n",
      "            (act): NewGELUActivation()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      "  )\n",
      "  (prompt_encoder): ModuleDict(\n",
      "    (default): PrefixEncoder(\n",
      "      (embedding): Embedding(20, 18432)\n",
      "    )\n",
      "  )\n",
      "  (word_embeddings): Embedding(50257, 768)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_688971/2306962191.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(label_seq),\n",
      "/data/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='153' max='153' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [153/153 01:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.26106071472168, 'eval_runtime': 77.4211, 'eval_samples_per_second': 63.122, 'eval_steps_per_second': 1.976}\n"
     ]
    }
   ],
   "source": [
    "# 查看一下初始精度，并验证能否正常运行\n",
    "evalres = trainer.evaluate()\n",
    "print(evalres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_688971/2306962191.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(label_seq),\n",
      "/data/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2845' max='13730' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2845/13730 35:50 < 2:17:13, 1.32 it/s, Epoch 1.04/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.845700</td>\n",
       "      <td>1.590944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_688971/2306962191.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(label_seq),\n",
      "/data/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[224], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 4090 * 2 训练约3h\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/anaconda3/envs/llm/lib/python3.10/site-packages/transformers/trainer.py:2241\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2239\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/anaconda3/envs/llm/lib/python3.10/site-packages/transformers/trainer.py:2553\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2547\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m   2548\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[1;32m   2550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2551\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2552\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2553\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2554\u001b[0m ):\n\u001b[1;32m   2555\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2556\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2557\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 4090 * 2 训练约3h\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型推理\n",
    "\n",
    "这里测试一下能否正常生成答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contex:Little Tommy was doing very badly in math. His parents had tried everything--tutors, cards, special learning centers--in short, everything they could think of. Finally they took Tommy to a catholic  school.\n",
      "After the first day, little Tommy came home with a very serious look on his face. He didn't kiss his mother hello. Instead, he went straight to his room and started studying. Books and papers were spread out all over the room and little Tommy was hard at work. His mother was surprised. She called him down to dinner and as soon as he finished eating, he went back to his room, without a word. In no time he was back hitting the books as hard as before. This went on for some time, day after day while the mother tried to understand what was happening.\n",
      "Finally, little Tommy brought home his report card. He quietly put it on the table and went up to his room and hit the books. His mom looked at it and to her surprise, little Tommy got an A in math. She could no longer hold her curiosity. She went to his room and asked, \"Son, what was it? Was it the nuns ? \"\n",
      "Little Tommy looked at her and shook his head, \"No. \"\n",
      "\"Well then,\" she asked again. \"WHAT was it? \"\n",
      "Little Tommy looked at her and said, \"Well, on the first day of school, when I saw that man nailed to the plus sign , I knew they weren't joking. \" \n",
      "question:The last sentence in the passage shows that  _  . \n",
      "options:\n",
      "A:Tommy felt sorry for the man\n",
      "B:Tommy was afraid of being nailed\n",
      "C:Tommy didn't like the plus sign\n",
      "D:Tommy liked playing jokes on others. \n",
      "Please select the best option for the question based on the content in the context, answer:B\n",
      "\n",
      "the GT: B\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_checkpoint)\n",
    "model = PeftModel.from_pretrained(model, \"./results/GPT2-epoch5/checkpoint-2746\")\n",
    "\n",
    "respone = inference(raw_dataset['test'][8],model)\n",
    "print(respone)\n",
    "print(f\"\\nthe GT: {raw_dataset['test'][8]['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们写一个函数来测试一下准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, dataset):\n",
    "    right=0.\n",
    "    for idx,instance in enumerate(dataset):\n",
    "        gt = instance[\"answer\"].lower()\n",
    "        respone = inference(instance, model)\n",
    "        pred = respone[-1].lower()\n",
    "        if pred==gt:\n",
    "            right +=1\n",
    "    print(f\"accurancy: {right/len(dataset)}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accurancy: 0.25678962302391567\n"
     ]
    }
   ],
   "source": [
    "eval(model, raw_dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
