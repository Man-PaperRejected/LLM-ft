{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RACE数据集，英语阅读理解多项选择QA\n",
    "\n",
    "1. 使用RoBERTa，基于multichoice头，完成模型微调（不同类型的QA任务是不同的）\n",
    "2. 尝试更换为MLM任务进行模型微调\n",
    "3. 了解DataCollator\n",
    "4. 自定义Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载数据集\n",
    "使用 datasets 库加载 imdb 数据集。这个库会自动下载并缓存数据。\n",
    "也可以下载到本地，这样可以防止网络问题导致代码执行失败（尽管已经下载到缓存）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda3/envs/llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = \"/data/Dataset/LLM-dataset/race\"\n",
    "raw_dataset = load_dataset(dataset_name, \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['example_id', 'article', 'answer', 'question', 'options'],\n",
      "        num_rows: 4934\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['example_id', 'article', 'answer', 'question', 'options'],\n",
      "        num_rows: 87866\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['example_id', 'article', 'answer', 'question', 'options'],\n",
      "        num_rows: 4887\n",
      "    })\n",
      "})\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "Dataset({\n",
      "    features: ['example_id', 'article', 'answer', 'question', 'options'],\n",
      "    num_rows: 87866\n",
      "})\n",
      "{'example_id': 'high19088.txt', 'article': 'Last week I talked with some of my students about what they wanted to do after they graduated, and what kind of job prospects  they thought they had.\\nGiven that I teach students who are training to be doctors, I was surprised do find that most thought that they would not be able to get the jobs they wanted without \"outside help\". \"What kind of help is that?\" I asked, expecting them to tell me that they would need a   or family friend to help them out.\\n\"Surgery ,\" one replied.\\nI was pretty alarmed by that response. It seems that the graduates of today are increasingly willing to go under the knife to get ahead of others when it comes to getting a job .\\nOne girl told me that she was considering surgery to increase her height. \"They break your legs, put in special extending screws, and slowly expand the gap between the two ends of the bone as it re-grows, you can get at least 5 cm taller!\"\\nAt that point, I was shocked. I am short, I can\\'t deny that, but I don\\'t think I would put myself through months of agony just to be a few centimetres taller. I don\\'t even bother to wear shoes with thick soles, as I\\'m not trying to hide the fact that I am just not tall!\\nIt seems to me that there is a trend towards wanting \"perfection\" , and that is an ideal that just does not exist in reality.\\nNo one is born perfect, yet magazines, TV shows and movies present images of thin, tall, beautiful people as being the norm. Advertisements for slimming aids, beauty treatments and cosmetic surgery clinics fill the pages of newspapers, further creating an idea that \"perfection\" is a requirement, and that it must be purchased, no matter what the cost. In my opinion, skills, rather than appearance, should determine how successful a person is in his/her chosen career.', 'answer': 'C', 'question': 'We can know from the passage that the author works as a_.', 'options': ['doctor', 'model', 'teacher', 'reporter']}\n"
     ]
    }
   ],
   "source": [
    "'''可以直接print 看一下数据集的概况'''\n",
    "print(raw_dataset)\n",
    "\n",
    "'''由于返回的是train 和 test 两个split 也可以直接像字典一样进行索引'''\n",
    "print(type(raw_dataset['train']))\n",
    "print(raw_dataset['train'])\n",
    "\n",
    "print(raw_dataset['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理 (Tokenization)\n",
    "接着我们需要加载 模型对应的分词器对数据进行处理\n",
    "\n",
    "BERT类模型在预训练时，使用的两个特殊token：[CLS] [SEP]，前者是用于提取全局信息的，后者则是用于划分句子的\n",
    "\n",
    "这里我们需要了解Multiple Choice QA任务，encoder类模型所期待的输入格式：为每个选项构建一个独立的输入序列，并将它们组合起来。\n",
    "- 格式 (单个选项序列):\n",
    "  - [CLS] context [SEP] question + option_i [SEP]\n",
    "  - 当然，也可以分为三部分输入，但是bert类的tokenizer只能处理两个输入，只能自己重写tokenizer\n",
    "\n",
    "模型输入张量: (batch_size, num_choices, sequence_length)\n",
    "\n",
    "*步骤*：\n",
    "以下代码是基于batch 处理的\n",
    "1. 构建context列表，对每个instance的 article复制4次\n",
    "2. 构建question + option_i，i=0~3\n",
    "3. 先对所有输入进行展平，进行tokenize\n",
    "4. 重塑为 (nums_instances, nums_choices, max_length)返回"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[[0, 10285, 186, 38, 3244, 19, 103, 9, 127, 521, 59, 99, 51, 770, 7, 109, 71, 51, 8505, 6, 8, 99, 761, 9, 633, 5108, 1437, 51, 802, 51, 56, 4, 50118, 18377, 14, 38, 6396, 521, 54, 32, 1058, 7, 28, 3333, 6, 38, 21, 3911, 109, 465, 14, 144, 802, 14, 51, 74, 45, 28, 441, 7, 120, 5, 1315, 51, 770, 396, 22, 35301, 244, 845, 22, 2264, 761, 9, 244, 16, 14, 1917, 38, 553, 6, 4804, 106, 7, 1137, 162, 14, 51, 74, 240, 10, 1437, 1437, 50, 284, 1441, 7, 244, 106, 66, 4, 50118, 113, 27526, 21712, 41066, 65, 6849, 4, 50118, 100, 21, 1256, 23438, 30, 14, 1263, 4, 85, 1302, 14, 5, 11295, 9, 452, 32, 3150, 2882, 7, 213, 223, 5, 7023, 7, 120, 789, 9, 643, 77, 24, 606, 7, 562, 10, 633, 479, 50118, 3762, 1816, 174, 162, 14, 79, 21, 2811, 3012, 7, 712, 69, 6958, 4, 22, 1213, 1108, 110, 5856, 6, 342, 11, 780, 9148, 34242, 6, 8, 5764, 3003, 5, 4044, 227, 5, 80, 3587, 9, 5, 9013, 25, 24, 769, 12, 571, 13415, 6, 47, 64, 120, 23, 513, 195, 25434, 26647, 2901, 50118, 3750, 14, 477, 6, 38, 21, 6649, 4, 38, 524, 765, 6, 38, 64, 75, 7631, 14, 6, 53, 38, 218, 75, 206, 38, 74, 342, 2185, 149, 377, 9, 32248, 95, 7, 28, 10, 367, 715, 12965, 1535, 26647, 4, 38, 218, 75, 190, 15304, 7, 3568, 5582, 19, 7992, 9281, 293, 6, 25, 38, 437, 45, 667, 7, 7433, 5, 754, 14, 38, 524, 95, 45, 6764, 328, 50118, 243, 1302, 7, 162, 14, 89, 16, 10, 2904, 1567, 6923, 22, 20473, 1499, 113, 2156, 8, 14, 16, 41, 5631, 14, 95, 473, 45, 5152, 11, 2015, 4, 50118, 3084, 65, 16, 2421, 1969, 6, 648, 15829, 6, 1012, 924, 8, 4133, 1455, 3156, 9, 7174, 6, 6764, 6, 2721, 82, 25, 145, 5, 13071, 4, 1614, 31396, 13, 11875, 7059, 25842, 6, 4002, 8289, 8, 22411, 3012, 12538, 3300, 5, 6052, 9, 9911, 6, 617, 2351, 41, 1114, 14, 22, 20473, 1499, 113, 16, 10, 7404, 6, 8, 14, 24, 531, 28, 3584, 6, 117, 948, 99, 5, 701, 4, 96, 127, 2979, 6, 2417, 6, 1195, 87, 2772, 6, 197, 3094, 141, 1800, 10, 621, 16, 11, 39, 73, 1843, 4986, 756, 4, 2, 2, 170, 64, 216, 31, 5, 9078, 14, 5, 2730, 1364, 25, 10, 47426, 3299, 2], [0, 10285, 186, 38, 3244, 19, 103, 9, 127, 521, 59, 99, 51, 770, 7, 109, 71, 51, 8505, 6, 8, 99, 761, 9, 633, 5108, 1437, 51, 802, 51, 56, 4, 50118, 18377, 14, 38, 6396, 521, 54, 32, 1058, 7, 28, 3333, 6, 38, 21, 3911, 109, 465, 14, 144, 802, 14, 51, 74, 45, 28, 441, 7, 120, 5, 1315, 51, 770, 396, 22, 35301, 244, 845, 22, 2264, 761, 9, 244, 16, 14, 1917, 38, 553, 6, 4804, 106, 7, 1137, 162, 14, 51, 74, 240, 10, 1437, 1437, 50, 284, 1441, 7, 244, 106, 66, 4, 50118, 113, 27526, 21712, 41066, 65, 6849, 4, 50118, 100, 21, 1256, 23438, 30, 14, 1263, 4, 85, 1302, 14, 5, 11295, 9, 452, 32, 3150, 2882, 7, 213, 223, 5, 7023, 7, 120, 789, 9, 643, 77, 24, 606, 7, 562, 10, 633, 479, 50118, 3762, 1816, 174, 162, 14, 79, 21, 2811, 3012, 7, 712, 69, 6958, 4, 22, 1213, 1108, 110, 5856, 6, 342, 11, 780, 9148, 34242, 6, 8, 5764, 3003, 5, 4044, 227, 5, 80, 3587, 9, 5, 9013, 25, 24, 769, 12, 571, 13415, 6, 47, 64, 120, 23, 513, 195, 25434, 26647, 2901, 50118, 3750, 14, 477, 6, 38, 21, 6649, 4, 38, 524, 765, 6, 38, 64, 75, 7631, 14, 6, 53, 38, 218, 75, 206, 38, 74, 342, 2185, 149, 377, 9, 32248, 95, 7, 28, 10, 367, 715, 12965, 1535, 26647, 4, 38, 218, 75, 190, 15304, 7, 3568, 5582, 19, 7992, 9281, 293, 6, 25, 38, 437, 45, 667, 7, 7433, 5, 754, 14, 38, 524, 95, 45, 6764, 328, 50118, 243, 1302, 7, 162, 14, 89, 16, 10, 2904, 1567, 6923, 22, 20473, 1499, 113, 2156, 8, 14, 16, 41, 5631, 14, 95, 473, 45, 5152, 11, 2015, 4, 50118, 3084, 65, 16, 2421, 1969, 6, 648, 15829, 6, 1012, 924, 8, 4133, 1455, 3156, 9, 7174, 6, 6764, 6, 2721, 82, 25, 145, 5, 13071, 4, 1614, 31396, 13, 11875, 7059, 25842, 6, 4002, 8289, 8, 22411, 3012, 12538, 3300, 5, 6052, 9, 9911, 6, 617, 2351, 41, 1114, 14, 22, 20473, 1499, 113, 16, 10, 7404, 6, 8, 14, 24, 531, 28, 3584, 6, 117, 948, 99, 5, 701, 4, 96, 127, 2979, 6, 2417, 6, 1195, 87, 2772, 6, 197, 3094, 141, 1800, 10, 621, 16, 11, 39, 73, 1843, 4986, 756, 4, 2, 2, 170, 64, 216, 31, 5, 9078, 14, 5, 2730, 1364, 25, 10, 47426, 1421, 2], [0, 10285, 186, 38, 3244, 19, 103, 9, 127, 521, 59, 99, 51, 770, 7, 109, 71, 51, 8505, 6, 8, 99, 761, 9, 633, 5108, 1437, 51, 802, 51, 56, 4, 50118, 18377, 14, 38, 6396, 521, 54, 32, 1058, 7, 28, 3333, 6, 38, 21, 3911, 109, 465, 14, 144, 802, 14, 51, 74, 45, 28, 441, 7, 120, 5, 1315, 51, 770, 396, 22, 35301, 244, 845, 22, 2264, 761, 9, 244, 16, 14, 1917, 38, 553, 6, 4804, 106, 7, 1137, 162, 14, 51, 74, 240, 10, 1437, 1437, 50, 284, 1441, 7, 244, 106, 66, 4, 50118, 113, 27526, 21712, 41066, 65, 6849, 4, 50118, 100, 21, 1256, 23438, 30, 14, 1263, 4, 85, 1302, 14, 5, 11295, 9, 452, 32, 3150, 2882, 7, 213, 223, 5, 7023, 7, 120, 789, 9, 643, 77, 24, 606, 7, 562, 10, 633, 479, 50118, 3762, 1816, 174, 162, 14, 79, 21, 2811, 3012, 7, 712, 69, 6958, 4, 22, 1213, 1108, 110, 5856, 6, 342, 11, 780, 9148, 34242, 6, 8, 5764, 3003, 5, 4044, 227, 5, 80, 3587, 9, 5, 9013, 25, 24, 769, 12, 571, 13415, 6, 47, 64, 120, 23, 513, 195, 25434, 26647, 2901, 50118, 3750, 14, 477, 6, 38, 21, 6649, 4, 38, 524, 765, 6, 38, 64, 75, 7631, 14, 6, 53, 38, 218, 75, 206, 38, 74, 342, 2185, 149, 377, 9, 32248, 95, 7, 28, 10, 367, 715, 12965, 1535, 26647, 4, 38, 218, 75, 190, 15304, 7, 3568, 5582, 19, 7992, 9281, 293, 6, 25, 38, 437, 45, 667, 7, 7433, 5, 754, 14, 38, 524, 95, 45, 6764, 328, 50118, 243, 1302, 7, 162, 14, 89, 16, 10, 2904, 1567, 6923, 22, 20473, 1499, 113, 2156, 8, 14, 16, 41, 5631, 14, 95, 473, 45, 5152, 11, 2015, 4, 50118, 3084, 65, 16, 2421, 1969, 6, 648, 15829, 6, 1012, 924, 8, 4133, 1455, 3156, 9, 7174, 6, 6764, 6, 2721, 82, 25, 145, 5, 13071, 4, 1614, 31396, 13, 11875, 7059, 25842, 6, 4002, 8289, 8, 22411, 3012, 12538, 3300, 5, 6052, 9, 9911, 6, 617, 2351, 41, 1114, 14, 22, 20473, 1499, 113, 16, 10, 7404, 6, 8, 14, 24, 531, 28, 3584, 6, 117, 948, 99, 5, 701, 4, 96, 127, 2979, 6, 2417, 6, 1195, 87, 2772, 6, 197, 3094, 141, 1800, 10, 621, 16, 11, 39, 73, 1843, 4986, 756, 4, 2, 2, 170, 64, 216, 31, 5, 9078, 14, 5, 2730, 1364, 25, 10, 47426, 3254, 2], [0, 10285, 186, 38, 3244, 19, 103, 9, 127, 521, 59, 99, 51, 770, 7, 109, 71, 51, 8505, 6, 8, 99, 761, 9, 633, 5108, 1437, 51, 802, 51, 56, 4, 50118, 18377, 14, 38, 6396, 521, 54, 32, 1058, 7, 28, 3333, 6, 38, 21, 3911, 109, 465, 14, 144, 802, 14, 51, 74, 45, 28, 441, 7, 120, 5, 1315, 51, 770, 396, 22, 35301, 244, 845, 22, 2264, 761, 9, 244, 16, 14, 1917, 38, 553, 6, 4804, 106, 7, 1137, 162, 14, 51, 74, 240, 10, 1437, 1437, 50, 284, 1441, 7, 244, 106, 66, 4, 50118, 113, 27526, 21712, 41066, 65, 6849, 4, 50118, 100, 21, 1256, 23438, 30, 14, 1263, 4, 85, 1302, 14, 5, 11295, 9, 452, 32, 3150, 2882, 7, 213, 223, 5, 7023, 7, 120, 789, 9, 643, 77, 24, 606, 7, 562, 10, 633, 479, 50118, 3762, 1816, 174, 162, 14, 79, 21, 2811, 3012, 7, 712, 69, 6958, 4, 22, 1213, 1108, 110, 5856, 6, 342, 11, 780, 9148, 34242, 6, 8, 5764, 3003, 5, 4044, 227, 5, 80, 3587, 9, 5, 9013, 25, 24, 769, 12, 571, 13415, 6, 47, 64, 120, 23, 513, 195, 25434, 26647, 2901, 50118, 3750, 14, 477, 6, 38, 21, 6649, 4, 38, 524, 765, 6, 38, 64, 75, 7631, 14, 6, 53, 38, 218, 75, 206, 38, 74, 342, 2185, 149, 377, 9, 32248, 95, 7, 28, 10, 367, 715, 12965, 1535, 26647, 4, 38, 218, 75, 190, 15304, 7, 3568, 5582, 19, 7992, 9281, 293, 6, 25, 38, 437, 45, 667, 7, 7433, 5, 754, 14, 38, 524, 95, 45, 6764, 328, 50118, 243, 1302, 7, 162, 14, 89, 16, 10, 2904, 1567, 6923, 22, 20473, 1499, 113, 2156, 8, 14, 16, 41, 5631, 14, 95, 473, 45, 5152, 11, 2015, 4, 50118, 3084, 65, 16, 2421, 1969, 6, 648, 15829, 6, 1012, 924, 8, 4133, 1455, 3156, 9, 7174, 6, 6764, 6, 2721, 82, 25, 145, 5, 13071, 4, 1614, 31396, 13, 11875, 7059, 25842, 6, 4002, 8289, 8, 22411, 3012, 12538, 3300, 5, 6052, 9, 9911, 6, 617, 2351, 41, 1114, 14, 22, 20473, 1499, 113, 16, 10, 7404, 6, 8, 14, 24, 531, 28, 3584, 6, 117, 948, 99, 5, 701, 4, 96, 127, 2979, 6, 2417, 6, 1195, 87, 2772, 6, 197, 3094, 141, 1800, 10, 621, 16, 11, 39, 73, 1843, 4986, 756, 4, 2, 2, 170, 64, 216, 31, 5, 9078, 14, 5, 2730, 1364, 25, 10, 47426, 4439, 2]], [[0, 10285, 186, 38, 3244, 19, 103, 9, 127, 521, 59, 99, 51, 770, 7, 109, 71, 51, 8505, 6, 8, 99, 761, 9, 633, 5108, 1437, 51, 802, 51, 56, 4, 50118, 18377, 14, 38, 6396, 521, 54, 32, 1058, 7, 28, 3333, 6, 38, 21, 3911, 109, 465, 14, 144, 802, 14, 51, 74, 45, 28, 441, 7, 120, 5, 1315, 51, 770, 396, 22, 35301, 244, 845, 22, 2264, 761, 9, 244, 16, 14, 1917, 38, 553, 6, 4804, 106, 7, 1137, 162, 14, 51, 74, 240, 10, 1437, 1437, 50, 284, 1441, 7, 244, 106, 66, 4, 50118, 113, 27526, 21712, 41066, 65, 6849, 4, 50118, 100, 21, 1256, 23438, 30, 14, 1263, 4, 85, 1302, 14, 5, 11295, 9, 452, 32, 3150, 2882, 7, 213, 223, 5, 7023, 7, 120, 789, 9, 643, 77, 24, 606, 7, 562, 10, 633, 479, 50118, 3762, 1816, 174, 162, 14, 79, 21, 2811, 3012, 7, 712, 69, 6958, 4, 22, 1213, 1108, 110, 5856, 6, 342, 11, 780, 9148, 34242, 6, 8, 5764, 3003, 5, 4044, 227, 5, 80, 3587, 9, 5, 9013, 25, 24, 769, 12, 571, 13415, 6, 47, 64, 120, 23, 513, 195, 25434, 26647, 2901, 50118, 3750, 14, 477, 6, 38, 21, 6649, 4, 38, 524, 765, 6, 38, 64, 75, 7631, 14, 6, 53, 38, 218, 75, 206, 38, 74, 342, 2185, 149, 377, 9, 32248, 95, 7, 28, 10, 367, 715, 12965, 1535, 26647, 4, 38, 218, 75, 190, 15304, 7, 3568, 5582, 19, 7992, 9281, 293, 6, 25, 38, 437, 45, 667, 7, 7433, 5, 754, 14, 38, 524, 95, 45, 6764, 328, 50118, 243, 1302, 7, 162, 14, 89, 16, 10, 2904, 1567, 6923, 22, 20473, 1499, 113, 2156, 8, 14, 16, 41, 5631, 14, 95, 473, 45, 5152, 11, 2015, 4, 50118, 3084, 65, 16, 2421, 1969, 6, 648, 15829, 6, 1012, 924, 8, 4133, 1455, 3156, 9, 7174, 6, 6764, 6, 2721, 82, 25, 145, 5, 13071, 4, 1614, 31396, 13, 11875, 7059, 25842, 6, 4002, 8289, 8, 22411, 3012, 12538, 3300, 5, 6052, 9, 9911, 6, 617, 2351, 41, 1114, 14, 22, 20473, 1499, 113, 16, 10, 7404, 6, 8, 14, 24, 531, 28, 3584, 6, 117, 948, 99, 5, 701, 4, 96, 127, 2979, 6, 2417, 6, 1195, 87, 2772, 6, 197, 3094, 141, 1800, 10, 621, 16, 11, 39, 73, 1843, 4986, 756, 4, 2, 2, 10787, 11295, 452, 1004, 7, 22411, 3012, 7, 47426, 12908, 10, 357, 313, 73, 7760, 2], [0, 10285, 186, 38, 3244, 19, 103, 9, 127, 521, 59, 99, 51, 770, 7, 109, 71, 51, 8505, 6, 8, 99, 761, 9, 633, 5108, 1437, 51, 802, 51, 56, 4, 50118, 18377, 14, 38, 6396, 521, 54, 32, 1058, 7, 28, 3333, 6, 38, 21, 3911, 109, 465, 14, 144, 802, 14, 51, 74, 45, 28, 441, 7, 120, 5, 1315, 51, 770, 396, 22, 35301, 244, 845, 22, 2264, 761, 9, 244, 16, 14, 1917, 38, 553, 6, 4804, 106, 7, 1137, 162, 14, 51, 74, 240, 10, 1437, 1437, 50, 284, 1441, 7, 244, 106, 66, 4, 50118, 113, 27526, 21712, 41066, 65, 6849, 4, 50118, 100, 21, 1256, 23438, 30, 14, 1263, 4, 85, 1302, 14, 5, 11295, 9, 452, 32, 3150, 2882, 7, 213, 223, 5, 7023, 7, 120, 789, 9, 643, 77, 24, 606, 7, 562, 10, 633, 479, 50118, 3762, 1816, 174, 162, 14, 79, 21, 2811, 3012, 7, 712, 69, 6958, 4, 22, 1213, 1108, 110, 5856, 6, 342, 11, 780, 9148, 34242, 6, 8, 5764, 3003, 5, 4044, 227, 5, 80, 3587, 9, 5, 9013, 25, 24, 769, 12, 571, 13415, 6, 47, 64, 120, 23, 513, 195, 25434, 26647, 2901, 50118, 3750, 14, 477, 6, 38, 21, 6649, 4, 38, 524, 765, 6, 38, 64, 75, 7631, 14, 6, 53, 38, 218, 75, 206, 38, 74, 342, 2185, 149, 377, 9, 32248, 95, 7, 28, 10, 367, 715, 12965, 1535, 26647, 4, 38, 218, 75, 190, 15304, 7, 3568, 5582, 19, 7992, 9281, 293, 6, 25, 38, 437, 45, 667, 7, 7433, 5, 754, 14, 38, 524, 95, 45, 6764, 328, 50118, 243, 1302, 7, 162, 14, 89, 16, 10, 2904, 1567, 6923, 22, 20473, 1499, 113, 2156, 8, 14, 16, 41, 5631, 14, 95, 473, 45, 5152, 11, 2015, 4, 50118, 3084, 65, 16, 2421, 1969, 6, 648, 15829, 6, 1012, 924, 8, 4133, 1455, 3156, 9, 7174, 6, 6764, 6, 2721, 82, 25, 145, 5, 13071, 4, 1614, 31396, 13, 11875, 7059, 25842, 6, 4002, 8289, 8, 22411, 3012, 12538, 3300, 5, 6052, 9, 9911, 6, 617, 2351, 41, 1114, 14, 22, 20473, 1499, 113, 16, 10, 7404, 6, 8, 14, 24, 531, 28, 3584, 6, 117, 948, 99, 5, 701, 4, 96, 127, 2979, 6, 2417, 6, 1195, 87, 2772, 6, 197, 3094, 141, 1800, 10, 621, 16, 11, 39, 73, 1843, 4986, 756, 4, 2, 2, 10787, 11295, 452, 1004, 7, 22411, 3012, 7, 47426, 555, 10, 1421, 2], [0, 10285, 186, 38, 3244, 19, 103, 9, 127, 521, 59, 99, 51, 770, 7, 109, 71, 51, 8505, 6, 8, 99, 761, 9, 633, 5108, 1437, 51, 802, 51, 56, 4, 50118, 18377, 14, 38, 6396, 521, 54, 32, 1058, 7, 28, 3333, 6, 38, 21, 3911, 109, 465, 14, 144, 802, 14, 51, 74, 45, 28, 441, 7, 120, 5, 1315, 51, 770, 396, 22, 35301, 244, 845, 22, 2264, 761, 9, 244, 16, 14, 1917, 38, 553, 6, 4804, 106, 7, 1137, 162, 14, 51, 74, 240, 10, 1437, 1437, 50, 284, 1441, 7, 244, 106, 66, 4, 50118, 113, 27526, 21712, 41066, 65, 6849, 4, 50118, 100, 21, 1256, 23438, 30, 14, 1263, 4, 85, 1302, 14, 5, 11295, 9, 452, 32, 3150, 2882, 7, 213, 223, 5, 7023, 7, 120, 789, 9, 643, 77, 24, 606, 7, 562, 10, 633, 479, 50118, 3762, 1816, 174, 162, 14, 79, 21, 2811, 3012, 7, 712, 69, 6958, 4, 22, 1213, 1108, 110, 5856, 6, 342, 11, 780, 9148, 34242, 6, 8, 5764, 3003, 5, 4044, 227, 5, 80, 3587, 9, 5, 9013, 25, 24, 769, 12, 571, 13415, 6, 47, 64, 120, 23, 513, 195, 25434, 26647, 2901, 50118, 3750, 14, 477, 6, 38, 21, 6649, 4, 38, 524, 765, 6, 38, 64, 75, 7631, 14, 6, 53, 38, 218, 75, 206, 38, 74, 342, 2185, 149, 377, 9, 32248, 95, 7, 28, 10, 367, 715, 12965, 1535, 26647, 4, 38, 218, 75, 190, 15304, 7, 3568, 5582, 19, 7992, 9281, 293, 6, 25, 38, 437, 45, 667, 7, 7433, 5, 754, 14, 38, 524, 95, 45, 6764, 328, 50118, 243, 1302, 7, 162, 14, 89, 16, 10, 2904, 1567, 6923, 22, 20473, 1499, 113, 2156, 8, 14, 16, 41, 5631, 14, 95, 473, 45, 5152, 11, 2015, 4, 50118, 3084, 65, 16, 2421, 1969, 6, 648, 15829, 6, 1012, 924, 8, 4133, 1455, 3156, 9, 7174, 6, 6764, 6, 2721, 82, 25, 145, 5, 13071, 4, 1614, 31396, 13, 11875, 7059, 25842, 6, 4002, 8289, 8, 22411, 3012, 12538, 3300, 5, 6052, 9, 9911, 6, 617, 2351, 41, 1114, 14, 22, 20473, 1499, 113, 16, 10, 7404, 6, 8, 14, 24, 531, 28, 3584, 6, 117, 948, 99, 5, 701, 4, 96, 127, 2979, 6, 2417, 6, 1195, 87, 2772, 6, 197, 3094, 141, 1800, 10, 621, 16, 11, 39, 73, 1843, 4986, 756, 4, 2, 2, 10787, 11295, 452, 1004, 7, 22411, 3012, 7, 47426, 120, 41, 2093, 81, 643, 11, 633, 12, 18458, 2577, 2], [0, 10285, 186, 38, 3244, 19, 103, 9, 127, 521, 59, 99, 51, 770, 7, 109, 71, 51, 8505, 6, 8, 99, 761, 9, 633, 5108, 1437, 51, 802, 51, 56, 4, 50118, 18377, 14, 38, 6396, 521, 54, 32, 1058, 7, 28, 3333, 6, 38, 21, 3911, 109, 465, 14, 144, 802, 14, 51, 74, 45, 28, 441, 7, 120, 5, 1315, 51, 770, 396, 22, 35301, 244, 845, 22, 2264, 761, 9, 244, 16, 14, 1917, 38, 553, 6, 4804, 106, 7, 1137, 162, 14, 51, 74, 240, 10, 1437, 1437, 50, 284, 1441, 7, 244, 106, 66, 4, 50118, 113, 27526, 21712, 41066, 65, 6849, 4, 50118, 100, 21, 1256, 23438, 30, 14, 1263, 4, 85, 1302, 14, 5, 11295, 9, 452, 32, 3150, 2882, 7, 213, 223, 5, 7023, 7, 120, 789, 9, 643, 77, 24, 606, 7, 562, 10, 633, 479, 50118, 3762, 1816, 174, 162, 14, 79, 21, 2811, 3012, 7, 712, 69, 6958, 4, 22, 1213, 1108, 110, 5856, 6, 342, 11, 780, 9148, 34242, 6, 8, 5764, 3003, 5, 4044, 227, 5, 80, 3587, 9, 5, 9013, 25, 24, 769, 12, 571, 13415, 6, 47, 64, 120, 23, 513, 195, 25434, 26647, 2901, 50118, 3750, 14, 477, 6, 38, 21, 6649, 4, 38, 524, 765, 6, 38, 64, 75, 7631, 14, 6, 53, 38, 218, 75, 206, 38, 74, 342, 2185, 149, 377, 9, 32248, 95, 7, 28, 10, 367, 715, 12965, 1535, 26647, 4, 38, 218, 75, 190, 15304, 7, 3568, 5582, 19, 7992, 9281, 293, 6, 25, 38, 437, 45, 667, 7, 7433, 5, 754, 14, 38, 524, 95, 45, 6764, 328, 50118, 243, 1302, 7, 162, 14, 89, 16, 10, 2904, 1567, 6923, 22, 20473, 1499, 113, 2156, 8, 14, 16, 41, 5631, 14, 95, 473, 45, 5152, 11, 2015, 4, 50118, 3084, 65, 16, 2421, 1969, 6, 648, 15829, 6, 1012, 924, 8, 4133, 1455, 3156, 9, 7174, 6, 6764, 6, 2721, 82, 25, 145, 5, 13071, 4, 1614, 31396, 13, 11875, 7059, 25842, 6, 4002, 8289, 8, 22411, 3012, 12538, 3300, 5, 6052, 9, 9911, 6, 617, 2351, 41, 1114, 14, 22, 20473, 1499, 113, 16, 10, 7404, 6, 8, 14, 24, 531, 28, 3584, 6, 117, 948, 99, 5, 701, 4, 96, 127, 2979, 6, 2417, 6, 1195, 87, 2772, 6, 197, 3094, 141, 1800, 10, 621, 16, 11, 39, 73, 1843, 4986, 756, 4, 2, 2, 10787, 11295, 452, 1004, 7, 22411, 3012, 7, 47426, 5696, 55, 15865, 4926, 2]], [[0, 10285, 186, 38, 3244, 19, 103, 9, 127, 521, 59, 99, 51, 770, 7, 109, 71, 51, 8505, 6, 8, 99, 761, 9, 633, 5108, 1437, 51, 802, 51, 56, 4, 50118, 18377, 14, 38, 6396, 521, 54, 32, 1058, 7, 28, 3333, 6, 38, 21, 3911, 109, 465, 14, 144, 802, 14, 51, 74, 45, 28, 441, 7, 120, 5, 1315, 51, 770, 396, 22, 35301, 244, 845, 22, 2264, 761, 9, 244, 16, 14, 1917, 38, 553, 6, 4804, 106, 7, 1137, 162, 14, 51, 74, 240, 10, 1437, 1437, 50, 284, 1441, 7, 244, 106, 66, 4, 50118, 113, 27526, 21712, 41066, 65, 6849, 4, 50118, 100, 21, 1256, 23438, 30, 14, 1263, 4, 85, 1302, 14, 5, 11295, 9, 452, 32, 3150, 2882, 7, 213, 223, 5, 7023, 7, 120, 789, 9, 643, 77, 24, 606, 7, 562, 10, 633, 479, 50118, 3762, 1816, 174, 162, 14, 79, 21, 2811, 3012, 7, 712, 69, 6958, 4, 22, 1213, 1108, 110, 5856, 6, 342, 11, 780, 9148, 34242, 6, 8, 5764, 3003, 5, 4044, 227, 5, 80, 3587, 9, 5, 9013, 25, 24, 769, 12, 571, 13415, 6, 47, 64, 120, 23, 513, 195, 25434, 26647, 2901, 50118, 3750, 14, 477, 6, 38, 21, 6649, 4, 38, 524, 765, 6, 38, 64, 75, 7631, 14, 6, 53, 38, 218, 75, 206, 38, 74, 342, 2185, 149, 377, 9, 32248, 95, 7, 28, 10, 367, 715, 12965, 1535, 26647, 4, 38, 218, 75, 190, 15304, 7, 3568, 5582, 19, 7992, 9281, 293, 6, 25, 38, 437, 45, 667, 7, 7433, 5, 754, 14, 38, 524, 95, 45, 6764, 328, 50118, 243, 1302, 7, 162, 14, 89, 16, 10, 2904, 1567, 6923, 22, 20473, 1499, 113, 2156, 8, 14, 16, 41, 5631, 14, 95, 473, 45, 5152, 11, 2015, 4, 50118, 3084, 65, 16, 2421, 1969, 6, 648, 15829, 6, 1012, 924, 8, 4133, 1455, 3156, 9, 7174, 6, 6764, 6, 2721, 82, 25, 145, 5, 13071, 4, 1614, 31396, 13, 11875, 7059, 25842, 6, 4002, 8289, 8, 22411, 3012, 12538, 3300, 5, 6052, 9, 9911, 6, 617, 2351, 41, 1114, 14, 22, 20473, 1499, 113, 16, 10, 7404, 6, 8, 14, 24, 531, 28, 3584, 6, 117, 948, 99, 5, 701, 4, 96, 127, 2979, 6, 2417, 6, 1195, 87, 2772, 6, 197, 3094, 141, 1800, 10, 621, 16, 11, 39, 73, 1843, 4986, 756, 4, 2, 2, 14693, 7, 5, 9078, 6, 5, 2730, 2046, 14, 47426, 961, 197, 2229, 19858, 6, 3046, 5, 701, 2], [0, 10285, 186, 38, 3244, 19, 103, 9, 127, 521, 59, 99, 51, 770, 7, 109, 71, 51, 8505, 6, 8, 99, 761, 9, 633, 5108, 1437, 51, 802, 51, 56, 4, 50118, 18377, 14, 38, 6396, 521, 54, 32, 1058, 7, 28, 3333, 6, 38, 21, 3911, 109, 465, 14, 144, 802, 14, 51, 74, 45, 28, 441, 7, 120, 5, 1315, 51, 770, 396, 22, 35301, 244, 845, 22, 2264, 761, 9, 244, 16, 14, 1917, 38, 553, 6, 4804, 106, 7, 1137, 162, 14, 51, 74, 240, 10, 1437, 1437, 50, 284, 1441, 7, 244, 106, 66, 4, 50118, 113, 27526, 21712, 41066, 65, 6849, 4, 50118, 100, 21, 1256, 23438, 30, 14, 1263, 4, 85, 1302, 14, 5, 11295, 9, 452, 32, 3150, 2882, 7, 213, 223, 5, 7023, 7, 120, 789, 9, 643, 77, 24, 606, 7, 562, 10, 633, 479, 50118, 3762, 1816, 174, 162, 14, 79, 21, 2811, 3012, 7, 712, 69, 6958, 4, 22, 1213, 1108, 110, 5856, 6, 342, 11, 780, 9148, 34242, 6, 8, 5764, 3003, 5, 4044, 227, 5, 80, 3587, 9, 5, 9013, 25, 24, 769, 12, 571, 13415, 6, 47, 64, 120, 23, 513, 195, 25434, 26647, 2901, 50118, 3750, 14, 477, 6, 38, 21, 6649, 4, 38, 524, 765, 6, 38, 64, 75, 7631, 14, 6, 53, 38, 218, 75, 206, 38, 74, 342, 2185, 149, 377, 9, 32248, 95, 7, 28, 10, 367, 715, 12965, 1535, 26647, 4, 38, 218, 75, 190, 15304, 7, 3568, 5582, 19, 7992, 9281, 293, 6, 25, 38, 437, 45, 667, 7, 7433, 5, 754, 14, 38, 524, 95, 45, 6764, 328, 50118, 243, 1302, 7, 162, 14, 89, 16, 10, 2904, 1567, 6923, 22, 20473, 1499, 113, 2156, 8, 14, 16, 41, 5631, 14, 95, 473, 45, 5152, 11, 2015, 4, 50118, 3084, 65, 16, 2421, 1969, 6, 648, 15829, 6, 1012, 924, 8, 4133, 1455, 3156, 9, 7174, 6, 6764, 6, 2721, 82, 25, 145, 5, 13071, 4, 1614, 31396, 13, 11875, 7059, 25842, 6, 4002, 8289, 8, 22411, 3012, 12538, 3300, 5, 6052, 9, 9911, 6, 617, 2351, 41, 1114, 14, 22, 20473, 1499, 113, 16, 10, 7404, 6, 8, 14, 24, 531, 28, 3584, 6, 117, 948, 99, 5, 701, 4, 96, 127, 2979, 6, 2417, 6, 1195, 87, 2772, 6, 197, 3094, 141, 1800, 10, 621, 16, 11, 39, 73, 1843, 4986, 756, 4, 2, 2, 14693, 7, 5, 9078, 6, 5, 2730, 2046, 14, 47426, 24, 18, 235, 13, 11295, 7, 1394, 13, 643, 7, 244, 106, 66, 11, 8217, 13, 1315, 2], [0, 10285, 186, 38, 3244, 19, 103, 9, 127, 521, 59, 99, 51, 770, 7, 109, 71, 51, 8505, 6, 8, 99, 761, 9, 633, 5108, 1437, 51, 802, 51, 56, 4, 50118, 18377, 14, 38, 6396, 521, 54, 32, 1058, 7, 28, 3333, 6, 38, 21, 3911, 109, 465, 14, 144, 802, 14, 51, 74, 45, 28, 441, 7, 120, 5, 1315, 51, 770, 396, 22, 35301, 244, 845, 22, 2264, 761, 9, 244, 16, 14, 1917, 38, 553, 6, 4804, 106, 7, 1137, 162, 14, 51, 74, 240, 10, 1437, 1437, 50, 284, 1441, 7, 244, 106, 66, 4, 50118, 113, 27526, 21712, 41066, 65, 6849, 4, 50118, 100, 21, 1256, 23438, 30, 14, 1263, 4, 85, 1302, 14, 5, 11295, 9, 452, 32, 3150, 2882, 7, 213, 223, 5, 7023, 7, 120, 789, 9, 643, 77, 24, 606, 7, 562, 10, 633, 479, 50118, 3762, 1816, 174, 162, 14, 79, 21, 2811, 3012, 7, 712, 69, 6958, 4, 22, 1213, 1108, 110, 5856, 6, 342, 11, 780, 9148, 34242, 6, 8, 5764, 3003, 5, 4044, 227, 5, 80, 3587, 9, 5, 9013, 25, 24, 769, 12, 571, 13415, 6, 47, 64, 120, 23, 513, 195, 25434, 26647, 2901, 50118, 3750, 14, 477, 6, 38, 21, 6649, 4, 38, 524, 765, 6, 38, 64, 75, 7631, 14, 6, 53, 38, 218, 75, 206, 38, 74, 342, 2185, 149, 377, 9, 32248, 95, 7, 28, 10, 367, 715, 12965, 1535, 26647, 4, 38, 218, 75, 190, 15304, 7, 3568, 5582, 19, 7992, 9281, 293, 6, 25, 38, 437, 45, 667, 7, 7433, 5, 754, 14, 38, 524, 95, 45, 6764, 328, 50118, 243, 1302, 7, 162, 14, 89, 16, 10, 2904, 1567, 6923, 22, 20473, 1499, 113, 2156, 8, 14, 16, 41, 5631, 14, 95, 473, 45, 5152, 11, 2015, 4, 50118, 3084, 65, 16, 2421, 1969, 6, 648, 15829, 6, 1012, 924, 8, 4133, 1455, 3156, 9, 7174, 6, 6764, 6, 2721, 82, 25, 145, 5, 13071, 4, 1614, 31396, 13, 11875, 7059, 25842, 6, 4002, 8289, 8, 22411, 3012, 12538, 3300, 5, 6052, 9, 9911, 6, 617, 2351, 41, 1114, 14, 22, 20473, 1499, 113, 16, 10, 7404, 6, 8, 14, 24, 531, 28, 3584, 6, 117, 948, 99, 5, 701, 4, 96, 127, 2979, 6, 2417, 6, 1195, 87, 2772, 6, 197, 3094, 141, 1800, 10, 621, 16, 11, 39, 73, 1843, 4986, 756, 4, 2, 2, 14693, 7, 5, 9078, 6, 5, 2730, 2046, 14, 47426, 24, 16, 65, 18, 2772, 1386, 9, 2417, 14, 269, 3510, 11, 65, 18, 756, 2], [0, 10285, 186, 38, 3244, 19, 103, 9, 127, 521, 59, 99, 51, 770, 7, 109, 71, 51, 8505, 6, 8, 99, 761, 9, 633, 5108, 1437, 51, 802, 51, 56, 4, 50118, 18377, 14, 38, 6396, 521, 54, 32, 1058, 7, 28, 3333, 6, 38, 21, 3911, 109, 465, 14, 144, 802, 14, 51, 74, 45, 28, 441, 7, 120, 5, 1315, 51, 770, 396, 22, 35301, 244, 845, 22, 2264, 761, 9, 244, 16, 14, 1917, 38, 553, 6, 4804, 106, 7, 1137, 162, 14, 51, 74, 240, 10, 1437, 1437, 50, 284, 1441, 7, 244, 106, 66, 4, 50118, 113, 27526, 21712, 41066, 65, 6849, 4, 50118, 100, 21, 1256, 23438, 30, 14, 1263, 4, 85, 1302, 14, 5, 11295, 9, 452, 32, 3150, 2882, 7, 213, 223, 5, 7023, 7, 120, 789, 9, 643, 77, 24, 606, 7, 562, 10, 633, 479, 50118, 3762, 1816, 174, 162, 14, 79, 21, 2811, 3012, 7, 712, 69, 6958, 4, 22, 1213, 1108, 110, 5856, 6, 342, 11, 780, 9148, 34242, 6, 8, 5764, 3003, 5, 4044, 227, 5, 80, 3587, 9, 5, 9013, 25, 24, 769, 12, 571, 13415, 6, 47, 64, 120, 23, 513, 195, 25434, 26647, 2901, 50118, 3750, 14, 477, 6, 38, 21, 6649, 4, 38, 524, 765, 6, 38, 64, 75, 7631, 14, 6, 53, 38, 218, 75, 206, 38, 74, 342, 2185, 149, 377, 9, 32248, 95, 7, 28, 10, 367, 715, 12965, 1535, 26647, 4, 38, 218, 75, 190, 15304, 7, 3568, 5582, 19, 7992, 9281, 293, 6, 25, 38, 437, 45, 667, 7, 7433, 5, 754, 14, 38, 524, 95, 45, 6764, 328, 50118, 243, 1302, 7, 162, 14, 89, 16, 10, 2904, 1567, 6923, 22, 20473, 1499, 113, 2156, 8, 14, 16, 41, 5631, 14, 95, 473, 45, 5152, 11, 2015, 4, 50118, 3084, 65, 16, 2421, 1969, 6, 648, 15829, 6, 1012, 924, 8, 4133, 1455, 3156, 9, 7174, 6, 6764, 6, 2721, 82, 25, 145, 5, 13071, 4, 1614, 31396, 13, 11875, 7059, 25842, 6, 4002, 8289, 8, 22411, 3012, 12538, 3300, 5, 6052, 9, 9911, 6, 617, 2351, 41, 1114, 14, 22, 20473, 1499, 113, 16, 10, 7404, 6, 8, 14, 24, 531, 28, 3584, 6, 117, 948, 99, 5, 701, 4, 96, 127, 2979, 6, 2417, 6, 1195, 87, 2772, 6, 197, 3094, 141, 1800, 10, 621, 16, 11, 39, 73, 1843, 4986, 756, 4, 2, 2, 14693, 7, 5, 9078, 6, 5, 2730, 2046, 14, 47426, 433, 32, 7, 4887, 13, 12030, 664, 82, 11, 49, 1818, 13, 3012, 2]], [[0, 10285, 186, 38, 3244, 19, 103, 9, 127, 521, 59, 99, 51, 770, 7, 109, 71, 51, 8505, 6, 8, 99, 761, 9, 633, 5108, 1437, 51, 802, 51, 56, 4, 50118, 18377, 14, 38, 6396, 521, 54, 32, 1058, 7, 28, 3333, 6, 38, 21, 3911, 109, 465, 14, 144, 802, 14, 51, 74, 45, 28, 441, 7, 120, 5, 1315, 51, 770, 396, 22, 35301, 244, 845, 22, 2264, 761, 9, 244, 16, 14, 1917, 38, 553, 6, 4804, 106, 7, 1137, 162, 14, 51, 74, 240, 10, 1437, 1437, 50, 284, 1441, 7, 244, 106, 66, 4, 50118, 113, 27526, 21712, 41066, 65, 6849, 4, 50118, 100, 21, 1256, 23438, 30, 14, 1263, 4, 85, 1302, 14, 5, 11295, 9, 452, 32, 3150, 2882, 7, 213, 223, 5, 7023, 7, 120, 789, 9, 643, 77, 24, 606, 7, 562, 10, 633, 479, 50118, 3762, 1816, 174, 162, 14, 79, 21, 2811, 3012, 7, 712, 69, 6958, 4, 22, 1213, 1108, 110, 5856, 6, 342, 11, 780, 9148, 34242, 6, 8, 5764, 3003, 5, 4044, 227, 5, 80, 3587, 9, 5, 9013, 25, 24, 769, 12, 571, 13415, 6, 47, 64, 120, 23, 513, 195, 25434, 26647, 2901, 50118, 3750, 14, 477, 6, 38, 21, 6649, 4, 38, 524, 765, 6, 38, 64, 75, 7631, 14, 6, 53, 38, 218, 75, 206, 38, 74, 342, 2185, 149, 377, 9, 32248, 95, 7, 28, 10, 367, 715, 12965, 1535, 26647, 4, 38, 218, 75, 190, 15304, 7, 3568, 5582, 19, 7992, 9281, 293, 6, 25, 38, 437, 45, 667, 7, 7433, 5, 754, 14, 38, 524, 95, 45, 6764, 328, 50118, 243, 1302, 7, 162, 14, 89, 16, 10, 2904, 1567, 6923, 22, 20473, 1499, 113, 2156, 8, 14, 16, 41, 5631, 14, 95, 473, 45, 5152, 11, 2015, 4, 50118, 3084, 65, 16, 2421, 1969, 6, 648, 15829, 6, 1012, 924, 8, 4133, 1455, 3156, 9, 7174, 6, 6764, 6, 2721, 82, 25, 145, 5, 13071, 4, 1614, 31396, 13, 11875, 7059, 25842, 6, 4002, 8289, 8, 22411, 3012, 12538, 3300, 5, 6052, 9, 9911, 6, 617, 2351, 41, 1114, 14, 22, 20473, 1499, 113, 16, 10, 7404, 6, 8, 14, 24, 531, 28, 3584, 6, 117, 948, 99, 5, 701, 4, 96, 127, 2979, 6, 2417, 6, 1195, 87, 2772, 6, 197, 3094, 141, 1800, 10, 621, 16, 11, 39, 73, 1843, 4986, 756, 4, 2, 2, 32251, 108, 579, 5, 275, 1270, 13, 5, 9078, 116, 4, 2880, 24257, 24597, 6319, 13620, 12809, 1635, 2], [0, 10285, 186, 38, 3244, 19, 103, 9, 127, 521, 59, 99, 51, 770, 7, 109, 71, 51, 8505, 6, 8, 99, 761, 9, 633, 5108, 1437, 51, 802, 51, 56, 4, 50118, 18377, 14, 38, 6396, 521, 54, 32, 1058, 7, 28, 3333, 6, 38, 21, 3911, 109, 465, 14, 144, 802, 14, 51, 74, 45, 28, 441, 7, 120, 5, 1315, 51, 770, 396, 22, 35301, 244, 845, 22, 2264, 761, 9, 244, 16, 14, 1917, 38, 553, 6, 4804, 106, 7, 1137, 162, 14, 51, 74, 240, 10, 1437, 1437, 50, 284, 1441, 7, 244, 106, 66, 4, 50118, 113, 27526, 21712, 41066, 65, 6849, 4, 50118, 100, 21, 1256, 23438, 30, 14, 1263, 4, 85, 1302, 14, 5, 11295, 9, 452, 32, 3150, 2882, 7, 213, 223, 5, 7023, 7, 120, 789, 9, 643, 77, 24, 606, 7, 562, 10, 633, 479, 50118, 3762, 1816, 174, 162, 14, 79, 21, 2811, 3012, 7, 712, 69, 6958, 4, 22, 1213, 1108, 110, 5856, 6, 342, 11, 780, 9148, 34242, 6, 8, 5764, 3003, 5, 4044, 227, 5, 80, 3587, 9, 5, 9013, 25, 24, 769, 12, 571, 13415, 6, 47, 64, 120, 23, 513, 195, 25434, 26647, 2901, 50118, 3750, 14, 477, 6, 38, 21, 6649, 4, 38, 524, 765, 6, 38, 64, 75, 7631, 14, 6, 53, 38, 218, 75, 206, 38, 74, 342, 2185, 149, 377, 9, 32248, 95, 7, 28, 10, 367, 715, 12965, 1535, 26647, 4, 38, 218, 75, 190, 15304, 7, 3568, 5582, 19, 7992, 9281, 293, 6, 25, 38, 437, 45, 667, 7, 7433, 5, 754, 14, 38, 524, 95, 45, 6764, 328, 50118, 243, 1302, 7, 162, 14, 89, 16, 10, 2904, 1567, 6923, 22, 20473, 1499, 113, 2156, 8, 14, 16, 41, 5631, 14, 95, 473, 45, 5152, 11, 2015, 4, 50118, 3084, 65, 16, 2421, 1969, 6, 648, 15829, 6, 1012, 924, 8, 4133, 1455, 3156, 9, 7174, 6, 6764, 6, 2721, 82, 25, 145, 5, 13071, 4, 1614, 31396, 13, 11875, 7059, 25842, 6, 4002, 8289, 8, 22411, 3012, 12538, 3300, 5, 6052, 9, 9911, 6, 617, 2351, 41, 1114, 14, 22, 20473, 1499, 113, 16, 10, 7404, 6, 8, 14, 24, 531, 28, 3584, 6, 117, 948, 99, 5, 701, 4, 96, 127, 2979, 6, 2417, 6, 1195, 87, 2772, 6, 197, 3094, 141, 1800, 10, 621, 16, 11, 39, 73, 1843, 4986, 756, 4, 2, 2, 32251, 108, 579, 5, 275, 1270, 13, 5, 9078, 116, 4, 2880, 24257, 24597, 6893, 7, 26793, 13, 11238, 13499, 2], [0, 10285, 186, 38, 3244, 19, 103, 9, 127, 521, 59, 99, 51, 770, 7, 109, 71, 51, 8505, 6, 8, 99, 761, 9, 633, 5108, 1437, 51, 802, 51, 56, 4, 50118, 18377, 14, 38, 6396, 521, 54, 32, 1058, 7, 28, 3333, 6, 38, 21, 3911, 109, 465, 14, 144, 802, 14, 51, 74, 45, 28, 441, 7, 120, 5, 1315, 51, 770, 396, 22, 35301, 244, 845, 22, 2264, 761, 9, 244, 16, 14, 1917, 38, 553, 6, 4804, 106, 7, 1137, 162, 14, 51, 74, 240, 10, 1437, 1437, 50, 284, 1441, 7, 244, 106, 66, 4, 50118, 113, 27526, 21712, 41066, 65, 6849, 4, 50118, 100, 21, 1256, 23438, 30, 14, 1263, 4, 85, 1302, 14, 5, 11295, 9, 452, 32, 3150, 2882, 7, 213, 223, 5, 7023, 7, 120, 789, 9, 643, 77, 24, 606, 7, 562, 10, 633, 479, 50118, 3762, 1816, 174, 162, 14, 79, 21, 2811, 3012, 7, 712, 69, 6958, 4, 22, 1213, 1108, 110, 5856, 6, 342, 11, 780, 9148, 34242, 6, 8, 5764, 3003, 5, 4044, 227, 5, 80, 3587, 9, 5, 9013, 25, 24, 769, 12, 571, 13415, 6, 47, 64, 120, 23, 513, 195, 25434, 26647, 2901, 50118, 3750, 14, 477, 6, 38, 21, 6649, 4, 38, 524, 765, 6, 38, 64, 75, 7631, 14, 6, 53, 38, 218, 75, 206, 38, 74, 342, 2185, 149, 377, 9, 32248, 95, 7, 28, 10, 367, 715, 12965, 1535, 26647, 4, 38, 218, 75, 190, 15304, 7, 3568, 5582, 19, 7992, 9281, 293, 6, 25, 38, 437, 45, 667, 7, 7433, 5, 754, 14, 38, 524, 95, 45, 6764, 328, 50118, 243, 1302, 7, 162, 14, 89, 16, 10, 2904, 1567, 6923, 22, 20473, 1499, 113, 2156, 8, 14, 16, 41, 5631, 14, 95, 473, 45, 5152, 11, 2015, 4, 50118, 3084, 65, 16, 2421, 1969, 6, 648, 15829, 6, 1012, 924, 8, 4133, 1455, 3156, 9, 7174, 6, 6764, 6, 2721, 82, 25, 145, 5, 13071, 4, 1614, 31396, 13, 11875, 7059, 25842, 6, 4002, 8289, 8, 22411, 3012, 12538, 3300, 5, 6052, 9, 9911, 6, 617, 2351, 41, 1114, 14, 22, 20473, 1499, 113, 16, 10, 7404, 6, 8, 14, 24, 531, 28, 3584, 6, 117, 948, 99, 5, 701, 4, 96, 127, 2979, 6, 2417, 6, 1195, 87, 2772, 6, 197, 3094, 141, 1800, 10, 621, 16, 11, 39, 73, 1843, 4986, 756, 4, 2, 2, 32251, 108, 579, 5, 275, 1270, 13, 5, 9078, 116, 4, 2880, 24257, 24597, 108, 20253, 1936, 43549, 26793, 2], [0, 10285, 186, 38, 3244, 19, 103, 9, 127, 521, 59, 99, 51, 770, 7, 109, 71, 51, 8505, 6, 8, 99, 761, 9, 633, 5108, 1437, 51, 802, 51, 56, 4, 50118, 18377, 14, 38, 6396, 521, 54, 32, 1058, 7, 28, 3333, 6, 38, 21, 3911, 109, 465, 14, 144, 802, 14, 51, 74, 45, 28, 441, 7, 120, 5, 1315, 51, 770, 396, 22, 35301, 244, 845, 22, 2264, 761, 9, 244, 16, 14, 1917, 38, 553, 6, 4804, 106, 7, 1137, 162, 14, 51, 74, 240, 10, 1437, 1437, 50, 284, 1441, 7, 244, 106, 66, 4, 50118, 113, 27526, 21712, 41066, 65, 6849, 4, 50118, 100, 21, 1256, 23438, 30, 14, 1263, 4, 85, 1302, 14, 5, 11295, 9, 452, 32, 3150, 2882, 7, 213, 223, 5, 7023, 7, 120, 789, 9, 643, 77, 24, 606, 7, 562, 10, 633, 479, 50118, 3762, 1816, 174, 162, 14, 79, 21, 2811, 3012, 7, 712, 69, 6958, 4, 22, 1213, 1108, 110, 5856, 6, 342, 11, 780, 9148, 34242, 6, 8, 5764, 3003, 5, 4044, 227, 5, 80, 3587, 9, 5, 9013, 25, 24, 769, 12, 571, 13415, 6, 47, 64, 120, 23, 513, 195, 25434, 26647, 2901, 50118, 3750, 14, 477, 6, 38, 21, 6649, 4, 38, 524, 765, 6, 38, 64, 75, 7631, 14, 6, 53, 38, 218, 75, 206, 38, 74, 342, 2185, 149, 377, 9, 32248, 95, 7, 28, 10, 367, 715, 12965, 1535, 26647, 4, 38, 218, 75, 190, 15304, 7, 3568, 5582, 19, 7992, 9281, 293, 6, 25, 38, 437, 45, 667, 7, 7433, 5, 754, 14, 38, 524, 95, 45, 6764, 328, 50118, 243, 1302, 7, 162, 14, 89, 16, 10, 2904, 1567, 6923, 22, 20473, 1499, 113, 2156, 8, 14, 16, 41, 5631, 14, 95, 473, 45, 5152, 11, 2015, 4, 50118, 3084, 65, 16, 2421, 1969, 6, 648, 15829, 6, 1012, 924, 8, 4133, 1455, 3156, 9, 7174, 6, 6764, 6, 2721, 82, 25, 145, 5, 13071, 4, 1614, 31396, 13, 11875, 7059, 25842, 6, 4002, 8289, 8, 22411, 3012, 12538, 3300, 5, 6052, 9, 9911, 6, 617, 2351, 41, 1114, 14, 22, 20473, 1499, 113, 16, 10, 7404, 6, 8, 14, 24, 531, 28, 3584, 6, 117, 948, 99, 5, 701, 4, 96, 127, 2979, 6, 2417, 6, 1195, 87, 2772, 6, 197, 3094, 141, 1800, 10, 621, 16, 11, 39, 73, 1843, 4986, 756, 4, 2, 2, 32251, 108, 579, 5, 275, 1270, 13, 5, 9078, 116, 4, 2880, 24257, 24597, 12346, 10, 22248, 34346, 11, 13576, 12, 18458, 2577, 2]]], 'attention_mask': [[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]], 'labels': [2, 2, 3, 1]}\n",
      "shape of input_ids :(4,4,416)\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "\n",
    "model_checkpoint = \"/data/Weights/roberta/roberta-base\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "\n",
    "def preprocess(instances):\n",
    "    context = [[article] * 4 for article in instances[\"article\"]]\n",
    "    questions = [q for q in instances[\"question\"]]\n",
    "    \n",
    "    question_choice = [\n",
    "        [f\"{ques} {instances['options'][i][j]}\" for j in range(4)] \n",
    "        for i, ques in enumerate(questions)\n",
    "    ]\n",
    "    \n",
    "    context = sum(context, [])                  #len = (len(instances) * 4)\n",
    "    question_choice = sum(question_choice, [])  \n",
    "\n",
    "    tokenized_instances = tokenizer(\n",
    "        context,\n",
    "        question_choice,\n",
    "        max_length=512,\n",
    "        padding=False,            # 这里我们之后使用DataCollator进行batch内的动态填充\n",
    "        truncation=\"only_first\",  # 优先截断第一个序列(文章)\n",
    "    )\n",
    "    '''返回字典\n",
    "    dict_keys(['input_ids', 'attention_mask'])\n",
    "    'input_ids' shape:(16, max_length)\n",
    "    '''\n",
    "    \n",
    "    # 创建标签列\n",
    "    labels = [ord(l)-ord('A') for l in instances['answer']]\n",
    "    \n",
    "    # reshape -> (nums_instances, nums_choices, max_length)\n",
    "    results = {\n",
    "        k: [v[i:i+4] for i in range(0, len(v), 4)] \n",
    "        for k, v in tokenized_instances.items()\n",
    "    }\n",
    "    results[\"labels\"] = labels\n",
    "    \n",
    "    return results\n",
    "\n",
    "c = preprocess(raw_dataset[\"train\"][0:4])\n",
    "\n",
    "print(c)\n",
    "print(f\"shape of input_ids :({len(c['input_ids'])},{len(c['input_ids'][0])},{len(c['input_ids'][0][0])})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行批处理，这里我们直接在map函数中将文本信息全部删除即可\n",
    "tokenized_dataset = raw_dataset.map(preprocess, batched=True,remove_columns=raw_dataset[\"train\"].column_names, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0,\n",
       "   10285,\n",
       "   186,\n",
       "   38,\n",
       "   3244,\n",
       "   19,\n",
       "   103,\n",
       "   9,\n",
       "   127,\n",
       "   521,\n",
       "   59,\n",
       "   99,\n",
       "   51,\n",
       "   770,\n",
       "   7,\n",
       "   109,\n",
       "   71,\n",
       "   51,\n",
       "   8505,\n",
       "   6,\n",
       "   8,\n",
       "   99,\n",
       "   761,\n",
       "   9,\n",
       "   633,\n",
       "   5108,\n",
       "   1437,\n",
       "   51,\n",
       "   802,\n",
       "   51,\n",
       "   56,\n",
       "   4,\n",
       "   50118,\n",
       "   18377,\n",
       "   14,\n",
       "   38,\n",
       "   6396,\n",
       "   521,\n",
       "   54,\n",
       "   32,\n",
       "   1058,\n",
       "   7,\n",
       "   28,\n",
       "   3333,\n",
       "   6,\n",
       "   38,\n",
       "   21,\n",
       "   3911,\n",
       "   109,\n",
       "   465,\n",
       "   14,\n",
       "   144,\n",
       "   802,\n",
       "   14,\n",
       "   51,\n",
       "   74,\n",
       "   45,\n",
       "   28,\n",
       "   441,\n",
       "   7,\n",
       "   120,\n",
       "   5,\n",
       "   1315,\n",
       "   51,\n",
       "   770,\n",
       "   396,\n",
       "   22,\n",
       "   35301,\n",
       "   244,\n",
       "   845,\n",
       "   22,\n",
       "   2264,\n",
       "   761,\n",
       "   9,\n",
       "   244,\n",
       "   16,\n",
       "   14,\n",
       "   1917,\n",
       "   38,\n",
       "   553,\n",
       "   6,\n",
       "   4804,\n",
       "   106,\n",
       "   7,\n",
       "   1137,\n",
       "   162,\n",
       "   14,\n",
       "   51,\n",
       "   74,\n",
       "   240,\n",
       "   10,\n",
       "   1437,\n",
       "   1437,\n",
       "   50,\n",
       "   284,\n",
       "   1441,\n",
       "   7,\n",
       "   244,\n",
       "   106,\n",
       "   66,\n",
       "   4,\n",
       "   50118,\n",
       "   113,\n",
       "   27526,\n",
       "   21712,\n",
       "   41066,\n",
       "   65,\n",
       "   6849,\n",
       "   4,\n",
       "   50118,\n",
       "   100,\n",
       "   21,\n",
       "   1256,\n",
       "   23438,\n",
       "   30,\n",
       "   14,\n",
       "   1263,\n",
       "   4,\n",
       "   85,\n",
       "   1302,\n",
       "   14,\n",
       "   5,\n",
       "   11295,\n",
       "   9,\n",
       "   452,\n",
       "   32,\n",
       "   3150,\n",
       "   2882,\n",
       "   7,\n",
       "   213,\n",
       "   223,\n",
       "   5,\n",
       "   7023,\n",
       "   7,\n",
       "   120,\n",
       "   789,\n",
       "   9,\n",
       "   643,\n",
       "   77,\n",
       "   24,\n",
       "   606,\n",
       "   7,\n",
       "   562,\n",
       "   10,\n",
       "   633,\n",
       "   479,\n",
       "   50118,\n",
       "   3762,\n",
       "   1816,\n",
       "   174,\n",
       "   162,\n",
       "   14,\n",
       "   79,\n",
       "   21,\n",
       "   2811,\n",
       "   3012,\n",
       "   7,\n",
       "   712,\n",
       "   69,\n",
       "   6958,\n",
       "   4,\n",
       "   22,\n",
       "   1213,\n",
       "   1108,\n",
       "   110,\n",
       "   5856,\n",
       "   6,\n",
       "   342,\n",
       "   11,\n",
       "   780,\n",
       "   9148,\n",
       "   34242,\n",
       "   6,\n",
       "   8,\n",
       "   5764,\n",
       "   3003,\n",
       "   5,\n",
       "   4044,\n",
       "   227,\n",
       "   5,\n",
       "   80,\n",
       "   3587,\n",
       "   9,\n",
       "   5,\n",
       "   9013,\n",
       "   25,\n",
       "   24,\n",
       "   769,\n",
       "   12,\n",
       "   571,\n",
       "   13415,\n",
       "   6,\n",
       "   47,\n",
       "   64,\n",
       "   120,\n",
       "   23,\n",
       "   513,\n",
       "   195,\n",
       "   25434,\n",
       "   26647,\n",
       "   2901,\n",
       "   50118,\n",
       "   3750,\n",
       "   14,\n",
       "   477,\n",
       "   6,\n",
       "   38,\n",
       "   21,\n",
       "   6649,\n",
       "   4,\n",
       "   38,\n",
       "   524,\n",
       "   765,\n",
       "   6,\n",
       "   38,\n",
       "   64,\n",
       "   75,\n",
       "   7631,\n",
       "   14,\n",
       "   6,\n",
       "   53,\n",
       "   38,\n",
       "   218,\n",
       "   75,\n",
       "   206,\n",
       "   38,\n",
       "   74,\n",
       "   342,\n",
       "   2185,\n",
       "   149,\n",
       "   377,\n",
       "   9,\n",
       "   32248,\n",
       "   95,\n",
       "   7,\n",
       "   28,\n",
       "   10,\n",
       "   367,\n",
       "   715,\n",
       "   12965,\n",
       "   1535,\n",
       "   26647,\n",
       "   4,\n",
       "   38,\n",
       "   218,\n",
       "   75,\n",
       "   190,\n",
       "   15304,\n",
       "   7,\n",
       "   3568,\n",
       "   5582,\n",
       "   19,\n",
       "   7992,\n",
       "   9281,\n",
       "   293,\n",
       "   6,\n",
       "   25,\n",
       "   38,\n",
       "   437,\n",
       "   45,\n",
       "   667,\n",
       "   7,\n",
       "   7433,\n",
       "   5,\n",
       "   754,\n",
       "   14,\n",
       "   38,\n",
       "   524,\n",
       "   95,\n",
       "   45,\n",
       "   6764,\n",
       "   328,\n",
       "   50118,\n",
       "   243,\n",
       "   1302,\n",
       "   7,\n",
       "   162,\n",
       "   14,\n",
       "   89,\n",
       "   16,\n",
       "   10,\n",
       "   2904,\n",
       "   1567,\n",
       "   6923,\n",
       "   22,\n",
       "   20473,\n",
       "   1499,\n",
       "   113,\n",
       "   2156,\n",
       "   8,\n",
       "   14,\n",
       "   16,\n",
       "   41,\n",
       "   5631,\n",
       "   14,\n",
       "   95,\n",
       "   473,\n",
       "   45,\n",
       "   5152,\n",
       "   11,\n",
       "   2015,\n",
       "   4,\n",
       "   50118,\n",
       "   3084,\n",
       "   65,\n",
       "   16,\n",
       "   2421,\n",
       "   1969,\n",
       "   6,\n",
       "   648,\n",
       "   15829,\n",
       "   6,\n",
       "   1012,\n",
       "   924,\n",
       "   8,\n",
       "   4133,\n",
       "   1455,\n",
       "   3156,\n",
       "   9,\n",
       "   7174,\n",
       "   6,\n",
       "   6764,\n",
       "   6,\n",
       "   2721,\n",
       "   82,\n",
       "   25,\n",
       "   145,\n",
       "   5,\n",
       "   13071,\n",
       "   4,\n",
       "   1614,\n",
       "   31396,\n",
       "   13,\n",
       "   11875,\n",
       "   7059,\n",
       "   25842,\n",
       "   6,\n",
       "   4002,\n",
       "   8289,\n",
       "   8,\n",
       "   22411,\n",
       "   3012,\n",
       "   12538,\n",
       "   3300,\n",
       "   5,\n",
       "   6052,\n",
       "   9,\n",
       "   9911,\n",
       "   6,\n",
       "   617,\n",
       "   2351,\n",
       "   41,\n",
       "   1114,\n",
       "   14,\n",
       "   22,\n",
       "   20473,\n",
       "   1499,\n",
       "   113,\n",
       "   16,\n",
       "   10,\n",
       "   7404,\n",
       "   6,\n",
       "   8,\n",
       "   14,\n",
       "   24,\n",
       "   531,\n",
       "   28,\n",
       "   3584,\n",
       "   6,\n",
       "   117,\n",
       "   948,\n",
       "   99,\n",
       "   5,\n",
       "   701,\n",
       "   4,\n",
       "   96,\n",
       "   127,\n",
       "   2979,\n",
       "   6,\n",
       "   2417,\n",
       "   6,\n",
       "   1195,\n",
       "   87,\n",
       "   2772,\n",
       "   6,\n",
       "   197,\n",
       "   3094,\n",
       "   141,\n",
       "   1800,\n",
       "   10,\n",
       "   621,\n",
       "   16,\n",
       "   11,\n",
       "   39,\n",
       "   73,\n",
       "   1843,\n",
       "   4986,\n",
       "   756,\n",
       "   4,\n",
       "   2,\n",
       "   2,\n",
       "   170,\n",
       "   64,\n",
       "   216,\n",
       "   31,\n",
       "   5,\n",
       "   9078,\n",
       "   14,\n",
       "   5,\n",
       "   2730,\n",
       "   1364,\n",
       "   25,\n",
       "   10,\n",
       "   47426,\n",
       "   3299,\n",
       "   2],\n",
       "  [0,\n",
       "   10285,\n",
       "   186,\n",
       "   38,\n",
       "   3244,\n",
       "   19,\n",
       "   103,\n",
       "   9,\n",
       "   127,\n",
       "   521,\n",
       "   59,\n",
       "   99,\n",
       "   51,\n",
       "   770,\n",
       "   7,\n",
       "   109,\n",
       "   71,\n",
       "   51,\n",
       "   8505,\n",
       "   6,\n",
       "   8,\n",
       "   99,\n",
       "   761,\n",
       "   9,\n",
       "   633,\n",
       "   5108,\n",
       "   1437,\n",
       "   51,\n",
       "   802,\n",
       "   51,\n",
       "   56,\n",
       "   4,\n",
       "   50118,\n",
       "   18377,\n",
       "   14,\n",
       "   38,\n",
       "   6396,\n",
       "   521,\n",
       "   54,\n",
       "   32,\n",
       "   1058,\n",
       "   7,\n",
       "   28,\n",
       "   3333,\n",
       "   6,\n",
       "   38,\n",
       "   21,\n",
       "   3911,\n",
       "   109,\n",
       "   465,\n",
       "   14,\n",
       "   144,\n",
       "   802,\n",
       "   14,\n",
       "   51,\n",
       "   74,\n",
       "   45,\n",
       "   28,\n",
       "   441,\n",
       "   7,\n",
       "   120,\n",
       "   5,\n",
       "   1315,\n",
       "   51,\n",
       "   770,\n",
       "   396,\n",
       "   22,\n",
       "   35301,\n",
       "   244,\n",
       "   845,\n",
       "   22,\n",
       "   2264,\n",
       "   761,\n",
       "   9,\n",
       "   244,\n",
       "   16,\n",
       "   14,\n",
       "   1917,\n",
       "   38,\n",
       "   553,\n",
       "   6,\n",
       "   4804,\n",
       "   106,\n",
       "   7,\n",
       "   1137,\n",
       "   162,\n",
       "   14,\n",
       "   51,\n",
       "   74,\n",
       "   240,\n",
       "   10,\n",
       "   1437,\n",
       "   1437,\n",
       "   50,\n",
       "   284,\n",
       "   1441,\n",
       "   7,\n",
       "   244,\n",
       "   106,\n",
       "   66,\n",
       "   4,\n",
       "   50118,\n",
       "   113,\n",
       "   27526,\n",
       "   21712,\n",
       "   41066,\n",
       "   65,\n",
       "   6849,\n",
       "   4,\n",
       "   50118,\n",
       "   100,\n",
       "   21,\n",
       "   1256,\n",
       "   23438,\n",
       "   30,\n",
       "   14,\n",
       "   1263,\n",
       "   4,\n",
       "   85,\n",
       "   1302,\n",
       "   14,\n",
       "   5,\n",
       "   11295,\n",
       "   9,\n",
       "   452,\n",
       "   32,\n",
       "   3150,\n",
       "   2882,\n",
       "   7,\n",
       "   213,\n",
       "   223,\n",
       "   5,\n",
       "   7023,\n",
       "   7,\n",
       "   120,\n",
       "   789,\n",
       "   9,\n",
       "   643,\n",
       "   77,\n",
       "   24,\n",
       "   606,\n",
       "   7,\n",
       "   562,\n",
       "   10,\n",
       "   633,\n",
       "   479,\n",
       "   50118,\n",
       "   3762,\n",
       "   1816,\n",
       "   174,\n",
       "   162,\n",
       "   14,\n",
       "   79,\n",
       "   21,\n",
       "   2811,\n",
       "   3012,\n",
       "   7,\n",
       "   712,\n",
       "   69,\n",
       "   6958,\n",
       "   4,\n",
       "   22,\n",
       "   1213,\n",
       "   1108,\n",
       "   110,\n",
       "   5856,\n",
       "   6,\n",
       "   342,\n",
       "   11,\n",
       "   780,\n",
       "   9148,\n",
       "   34242,\n",
       "   6,\n",
       "   8,\n",
       "   5764,\n",
       "   3003,\n",
       "   5,\n",
       "   4044,\n",
       "   227,\n",
       "   5,\n",
       "   80,\n",
       "   3587,\n",
       "   9,\n",
       "   5,\n",
       "   9013,\n",
       "   25,\n",
       "   24,\n",
       "   769,\n",
       "   12,\n",
       "   571,\n",
       "   13415,\n",
       "   6,\n",
       "   47,\n",
       "   64,\n",
       "   120,\n",
       "   23,\n",
       "   513,\n",
       "   195,\n",
       "   25434,\n",
       "   26647,\n",
       "   2901,\n",
       "   50118,\n",
       "   3750,\n",
       "   14,\n",
       "   477,\n",
       "   6,\n",
       "   38,\n",
       "   21,\n",
       "   6649,\n",
       "   4,\n",
       "   38,\n",
       "   524,\n",
       "   765,\n",
       "   6,\n",
       "   38,\n",
       "   64,\n",
       "   75,\n",
       "   7631,\n",
       "   14,\n",
       "   6,\n",
       "   53,\n",
       "   38,\n",
       "   218,\n",
       "   75,\n",
       "   206,\n",
       "   38,\n",
       "   74,\n",
       "   342,\n",
       "   2185,\n",
       "   149,\n",
       "   377,\n",
       "   9,\n",
       "   32248,\n",
       "   95,\n",
       "   7,\n",
       "   28,\n",
       "   10,\n",
       "   367,\n",
       "   715,\n",
       "   12965,\n",
       "   1535,\n",
       "   26647,\n",
       "   4,\n",
       "   38,\n",
       "   218,\n",
       "   75,\n",
       "   190,\n",
       "   15304,\n",
       "   7,\n",
       "   3568,\n",
       "   5582,\n",
       "   19,\n",
       "   7992,\n",
       "   9281,\n",
       "   293,\n",
       "   6,\n",
       "   25,\n",
       "   38,\n",
       "   437,\n",
       "   45,\n",
       "   667,\n",
       "   7,\n",
       "   7433,\n",
       "   5,\n",
       "   754,\n",
       "   14,\n",
       "   38,\n",
       "   524,\n",
       "   95,\n",
       "   45,\n",
       "   6764,\n",
       "   328,\n",
       "   50118,\n",
       "   243,\n",
       "   1302,\n",
       "   7,\n",
       "   162,\n",
       "   14,\n",
       "   89,\n",
       "   16,\n",
       "   10,\n",
       "   2904,\n",
       "   1567,\n",
       "   6923,\n",
       "   22,\n",
       "   20473,\n",
       "   1499,\n",
       "   113,\n",
       "   2156,\n",
       "   8,\n",
       "   14,\n",
       "   16,\n",
       "   41,\n",
       "   5631,\n",
       "   14,\n",
       "   95,\n",
       "   473,\n",
       "   45,\n",
       "   5152,\n",
       "   11,\n",
       "   2015,\n",
       "   4,\n",
       "   50118,\n",
       "   3084,\n",
       "   65,\n",
       "   16,\n",
       "   2421,\n",
       "   1969,\n",
       "   6,\n",
       "   648,\n",
       "   15829,\n",
       "   6,\n",
       "   1012,\n",
       "   924,\n",
       "   8,\n",
       "   4133,\n",
       "   1455,\n",
       "   3156,\n",
       "   9,\n",
       "   7174,\n",
       "   6,\n",
       "   6764,\n",
       "   6,\n",
       "   2721,\n",
       "   82,\n",
       "   25,\n",
       "   145,\n",
       "   5,\n",
       "   13071,\n",
       "   4,\n",
       "   1614,\n",
       "   31396,\n",
       "   13,\n",
       "   11875,\n",
       "   7059,\n",
       "   25842,\n",
       "   6,\n",
       "   4002,\n",
       "   8289,\n",
       "   8,\n",
       "   22411,\n",
       "   3012,\n",
       "   12538,\n",
       "   3300,\n",
       "   5,\n",
       "   6052,\n",
       "   9,\n",
       "   9911,\n",
       "   6,\n",
       "   617,\n",
       "   2351,\n",
       "   41,\n",
       "   1114,\n",
       "   14,\n",
       "   22,\n",
       "   20473,\n",
       "   1499,\n",
       "   113,\n",
       "   16,\n",
       "   10,\n",
       "   7404,\n",
       "   6,\n",
       "   8,\n",
       "   14,\n",
       "   24,\n",
       "   531,\n",
       "   28,\n",
       "   3584,\n",
       "   6,\n",
       "   117,\n",
       "   948,\n",
       "   99,\n",
       "   5,\n",
       "   701,\n",
       "   4,\n",
       "   96,\n",
       "   127,\n",
       "   2979,\n",
       "   6,\n",
       "   2417,\n",
       "   6,\n",
       "   1195,\n",
       "   87,\n",
       "   2772,\n",
       "   6,\n",
       "   197,\n",
       "   3094,\n",
       "   141,\n",
       "   1800,\n",
       "   10,\n",
       "   621,\n",
       "   16,\n",
       "   11,\n",
       "   39,\n",
       "   73,\n",
       "   1843,\n",
       "   4986,\n",
       "   756,\n",
       "   4,\n",
       "   2,\n",
       "   2,\n",
       "   170,\n",
       "   64,\n",
       "   216,\n",
       "   31,\n",
       "   5,\n",
       "   9078,\n",
       "   14,\n",
       "   5,\n",
       "   2730,\n",
       "   1364,\n",
       "   25,\n",
       "   10,\n",
       "   47426,\n",
       "   1421,\n",
       "   2],\n",
       "  [0,\n",
       "   10285,\n",
       "   186,\n",
       "   38,\n",
       "   3244,\n",
       "   19,\n",
       "   103,\n",
       "   9,\n",
       "   127,\n",
       "   521,\n",
       "   59,\n",
       "   99,\n",
       "   51,\n",
       "   770,\n",
       "   7,\n",
       "   109,\n",
       "   71,\n",
       "   51,\n",
       "   8505,\n",
       "   6,\n",
       "   8,\n",
       "   99,\n",
       "   761,\n",
       "   9,\n",
       "   633,\n",
       "   5108,\n",
       "   1437,\n",
       "   51,\n",
       "   802,\n",
       "   51,\n",
       "   56,\n",
       "   4,\n",
       "   50118,\n",
       "   18377,\n",
       "   14,\n",
       "   38,\n",
       "   6396,\n",
       "   521,\n",
       "   54,\n",
       "   32,\n",
       "   1058,\n",
       "   7,\n",
       "   28,\n",
       "   3333,\n",
       "   6,\n",
       "   38,\n",
       "   21,\n",
       "   3911,\n",
       "   109,\n",
       "   465,\n",
       "   14,\n",
       "   144,\n",
       "   802,\n",
       "   14,\n",
       "   51,\n",
       "   74,\n",
       "   45,\n",
       "   28,\n",
       "   441,\n",
       "   7,\n",
       "   120,\n",
       "   5,\n",
       "   1315,\n",
       "   51,\n",
       "   770,\n",
       "   396,\n",
       "   22,\n",
       "   35301,\n",
       "   244,\n",
       "   845,\n",
       "   22,\n",
       "   2264,\n",
       "   761,\n",
       "   9,\n",
       "   244,\n",
       "   16,\n",
       "   14,\n",
       "   1917,\n",
       "   38,\n",
       "   553,\n",
       "   6,\n",
       "   4804,\n",
       "   106,\n",
       "   7,\n",
       "   1137,\n",
       "   162,\n",
       "   14,\n",
       "   51,\n",
       "   74,\n",
       "   240,\n",
       "   10,\n",
       "   1437,\n",
       "   1437,\n",
       "   50,\n",
       "   284,\n",
       "   1441,\n",
       "   7,\n",
       "   244,\n",
       "   106,\n",
       "   66,\n",
       "   4,\n",
       "   50118,\n",
       "   113,\n",
       "   27526,\n",
       "   21712,\n",
       "   41066,\n",
       "   65,\n",
       "   6849,\n",
       "   4,\n",
       "   50118,\n",
       "   100,\n",
       "   21,\n",
       "   1256,\n",
       "   23438,\n",
       "   30,\n",
       "   14,\n",
       "   1263,\n",
       "   4,\n",
       "   85,\n",
       "   1302,\n",
       "   14,\n",
       "   5,\n",
       "   11295,\n",
       "   9,\n",
       "   452,\n",
       "   32,\n",
       "   3150,\n",
       "   2882,\n",
       "   7,\n",
       "   213,\n",
       "   223,\n",
       "   5,\n",
       "   7023,\n",
       "   7,\n",
       "   120,\n",
       "   789,\n",
       "   9,\n",
       "   643,\n",
       "   77,\n",
       "   24,\n",
       "   606,\n",
       "   7,\n",
       "   562,\n",
       "   10,\n",
       "   633,\n",
       "   479,\n",
       "   50118,\n",
       "   3762,\n",
       "   1816,\n",
       "   174,\n",
       "   162,\n",
       "   14,\n",
       "   79,\n",
       "   21,\n",
       "   2811,\n",
       "   3012,\n",
       "   7,\n",
       "   712,\n",
       "   69,\n",
       "   6958,\n",
       "   4,\n",
       "   22,\n",
       "   1213,\n",
       "   1108,\n",
       "   110,\n",
       "   5856,\n",
       "   6,\n",
       "   342,\n",
       "   11,\n",
       "   780,\n",
       "   9148,\n",
       "   34242,\n",
       "   6,\n",
       "   8,\n",
       "   5764,\n",
       "   3003,\n",
       "   5,\n",
       "   4044,\n",
       "   227,\n",
       "   5,\n",
       "   80,\n",
       "   3587,\n",
       "   9,\n",
       "   5,\n",
       "   9013,\n",
       "   25,\n",
       "   24,\n",
       "   769,\n",
       "   12,\n",
       "   571,\n",
       "   13415,\n",
       "   6,\n",
       "   47,\n",
       "   64,\n",
       "   120,\n",
       "   23,\n",
       "   513,\n",
       "   195,\n",
       "   25434,\n",
       "   26647,\n",
       "   2901,\n",
       "   50118,\n",
       "   3750,\n",
       "   14,\n",
       "   477,\n",
       "   6,\n",
       "   38,\n",
       "   21,\n",
       "   6649,\n",
       "   4,\n",
       "   38,\n",
       "   524,\n",
       "   765,\n",
       "   6,\n",
       "   38,\n",
       "   64,\n",
       "   75,\n",
       "   7631,\n",
       "   14,\n",
       "   6,\n",
       "   53,\n",
       "   38,\n",
       "   218,\n",
       "   75,\n",
       "   206,\n",
       "   38,\n",
       "   74,\n",
       "   342,\n",
       "   2185,\n",
       "   149,\n",
       "   377,\n",
       "   9,\n",
       "   32248,\n",
       "   95,\n",
       "   7,\n",
       "   28,\n",
       "   10,\n",
       "   367,\n",
       "   715,\n",
       "   12965,\n",
       "   1535,\n",
       "   26647,\n",
       "   4,\n",
       "   38,\n",
       "   218,\n",
       "   75,\n",
       "   190,\n",
       "   15304,\n",
       "   7,\n",
       "   3568,\n",
       "   5582,\n",
       "   19,\n",
       "   7992,\n",
       "   9281,\n",
       "   293,\n",
       "   6,\n",
       "   25,\n",
       "   38,\n",
       "   437,\n",
       "   45,\n",
       "   667,\n",
       "   7,\n",
       "   7433,\n",
       "   5,\n",
       "   754,\n",
       "   14,\n",
       "   38,\n",
       "   524,\n",
       "   95,\n",
       "   45,\n",
       "   6764,\n",
       "   328,\n",
       "   50118,\n",
       "   243,\n",
       "   1302,\n",
       "   7,\n",
       "   162,\n",
       "   14,\n",
       "   89,\n",
       "   16,\n",
       "   10,\n",
       "   2904,\n",
       "   1567,\n",
       "   6923,\n",
       "   22,\n",
       "   20473,\n",
       "   1499,\n",
       "   113,\n",
       "   2156,\n",
       "   8,\n",
       "   14,\n",
       "   16,\n",
       "   41,\n",
       "   5631,\n",
       "   14,\n",
       "   95,\n",
       "   473,\n",
       "   45,\n",
       "   5152,\n",
       "   11,\n",
       "   2015,\n",
       "   4,\n",
       "   50118,\n",
       "   3084,\n",
       "   65,\n",
       "   16,\n",
       "   2421,\n",
       "   1969,\n",
       "   6,\n",
       "   648,\n",
       "   15829,\n",
       "   6,\n",
       "   1012,\n",
       "   924,\n",
       "   8,\n",
       "   4133,\n",
       "   1455,\n",
       "   3156,\n",
       "   9,\n",
       "   7174,\n",
       "   6,\n",
       "   6764,\n",
       "   6,\n",
       "   2721,\n",
       "   82,\n",
       "   25,\n",
       "   145,\n",
       "   5,\n",
       "   13071,\n",
       "   4,\n",
       "   1614,\n",
       "   31396,\n",
       "   13,\n",
       "   11875,\n",
       "   7059,\n",
       "   25842,\n",
       "   6,\n",
       "   4002,\n",
       "   8289,\n",
       "   8,\n",
       "   22411,\n",
       "   3012,\n",
       "   12538,\n",
       "   3300,\n",
       "   5,\n",
       "   6052,\n",
       "   9,\n",
       "   9911,\n",
       "   6,\n",
       "   617,\n",
       "   2351,\n",
       "   41,\n",
       "   1114,\n",
       "   14,\n",
       "   22,\n",
       "   20473,\n",
       "   1499,\n",
       "   113,\n",
       "   16,\n",
       "   10,\n",
       "   7404,\n",
       "   6,\n",
       "   8,\n",
       "   14,\n",
       "   24,\n",
       "   531,\n",
       "   28,\n",
       "   3584,\n",
       "   6,\n",
       "   117,\n",
       "   948,\n",
       "   99,\n",
       "   5,\n",
       "   701,\n",
       "   4,\n",
       "   96,\n",
       "   127,\n",
       "   2979,\n",
       "   6,\n",
       "   2417,\n",
       "   6,\n",
       "   1195,\n",
       "   87,\n",
       "   2772,\n",
       "   6,\n",
       "   197,\n",
       "   3094,\n",
       "   141,\n",
       "   1800,\n",
       "   10,\n",
       "   621,\n",
       "   16,\n",
       "   11,\n",
       "   39,\n",
       "   73,\n",
       "   1843,\n",
       "   4986,\n",
       "   756,\n",
       "   4,\n",
       "   2,\n",
       "   2,\n",
       "   170,\n",
       "   64,\n",
       "   216,\n",
       "   31,\n",
       "   5,\n",
       "   9078,\n",
       "   14,\n",
       "   5,\n",
       "   2730,\n",
       "   1364,\n",
       "   25,\n",
       "   10,\n",
       "   47426,\n",
       "   3254,\n",
       "   2],\n",
       "  [0,\n",
       "   10285,\n",
       "   186,\n",
       "   38,\n",
       "   3244,\n",
       "   19,\n",
       "   103,\n",
       "   9,\n",
       "   127,\n",
       "   521,\n",
       "   59,\n",
       "   99,\n",
       "   51,\n",
       "   770,\n",
       "   7,\n",
       "   109,\n",
       "   71,\n",
       "   51,\n",
       "   8505,\n",
       "   6,\n",
       "   8,\n",
       "   99,\n",
       "   761,\n",
       "   9,\n",
       "   633,\n",
       "   5108,\n",
       "   1437,\n",
       "   51,\n",
       "   802,\n",
       "   51,\n",
       "   56,\n",
       "   4,\n",
       "   50118,\n",
       "   18377,\n",
       "   14,\n",
       "   38,\n",
       "   6396,\n",
       "   521,\n",
       "   54,\n",
       "   32,\n",
       "   1058,\n",
       "   7,\n",
       "   28,\n",
       "   3333,\n",
       "   6,\n",
       "   38,\n",
       "   21,\n",
       "   3911,\n",
       "   109,\n",
       "   465,\n",
       "   14,\n",
       "   144,\n",
       "   802,\n",
       "   14,\n",
       "   51,\n",
       "   74,\n",
       "   45,\n",
       "   28,\n",
       "   441,\n",
       "   7,\n",
       "   120,\n",
       "   5,\n",
       "   1315,\n",
       "   51,\n",
       "   770,\n",
       "   396,\n",
       "   22,\n",
       "   35301,\n",
       "   244,\n",
       "   845,\n",
       "   22,\n",
       "   2264,\n",
       "   761,\n",
       "   9,\n",
       "   244,\n",
       "   16,\n",
       "   14,\n",
       "   1917,\n",
       "   38,\n",
       "   553,\n",
       "   6,\n",
       "   4804,\n",
       "   106,\n",
       "   7,\n",
       "   1137,\n",
       "   162,\n",
       "   14,\n",
       "   51,\n",
       "   74,\n",
       "   240,\n",
       "   10,\n",
       "   1437,\n",
       "   1437,\n",
       "   50,\n",
       "   284,\n",
       "   1441,\n",
       "   7,\n",
       "   244,\n",
       "   106,\n",
       "   66,\n",
       "   4,\n",
       "   50118,\n",
       "   113,\n",
       "   27526,\n",
       "   21712,\n",
       "   41066,\n",
       "   65,\n",
       "   6849,\n",
       "   4,\n",
       "   50118,\n",
       "   100,\n",
       "   21,\n",
       "   1256,\n",
       "   23438,\n",
       "   30,\n",
       "   14,\n",
       "   1263,\n",
       "   4,\n",
       "   85,\n",
       "   1302,\n",
       "   14,\n",
       "   5,\n",
       "   11295,\n",
       "   9,\n",
       "   452,\n",
       "   32,\n",
       "   3150,\n",
       "   2882,\n",
       "   7,\n",
       "   213,\n",
       "   223,\n",
       "   5,\n",
       "   7023,\n",
       "   7,\n",
       "   120,\n",
       "   789,\n",
       "   9,\n",
       "   643,\n",
       "   77,\n",
       "   24,\n",
       "   606,\n",
       "   7,\n",
       "   562,\n",
       "   10,\n",
       "   633,\n",
       "   479,\n",
       "   50118,\n",
       "   3762,\n",
       "   1816,\n",
       "   174,\n",
       "   162,\n",
       "   14,\n",
       "   79,\n",
       "   21,\n",
       "   2811,\n",
       "   3012,\n",
       "   7,\n",
       "   712,\n",
       "   69,\n",
       "   6958,\n",
       "   4,\n",
       "   22,\n",
       "   1213,\n",
       "   1108,\n",
       "   110,\n",
       "   5856,\n",
       "   6,\n",
       "   342,\n",
       "   11,\n",
       "   780,\n",
       "   9148,\n",
       "   34242,\n",
       "   6,\n",
       "   8,\n",
       "   5764,\n",
       "   3003,\n",
       "   5,\n",
       "   4044,\n",
       "   227,\n",
       "   5,\n",
       "   80,\n",
       "   3587,\n",
       "   9,\n",
       "   5,\n",
       "   9013,\n",
       "   25,\n",
       "   24,\n",
       "   769,\n",
       "   12,\n",
       "   571,\n",
       "   13415,\n",
       "   6,\n",
       "   47,\n",
       "   64,\n",
       "   120,\n",
       "   23,\n",
       "   513,\n",
       "   195,\n",
       "   25434,\n",
       "   26647,\n",
       "   2901,\n",
       "   50118,\n",
       "   3750,\n",
       "   14,\n",
       "   477,\n",
       "   6,\n",
       "   38,\n",
       "   21,\n",
       "   6649,\n",
       "   4,\n",
       "   38,\n",
       "   524,\n",
       "   765,\n",
       "   6,\n",
       "   38,\n",
       "   64,\n",
       "   75,\n",
       "   7631,\n",
       "   14,\n",
       "   6,\n",
       "   53,\n",
       "   38,\n",
       "   218,\n",
       "   75,\n",
       "   206,\n",
       "   38,\n",
       "   74,\n",
       "   342,\n",
       "   2185,\n",
       "   149,\n",
       "   377,\n",
       "   9,\n",
       "   32248,\n",
       "   95,\n",
       "   7,\n",
       "   28,\n",
       "   10,\n",
       "   367,\n",
       "   715,\n",
       "   12965,\n",
       "   1535,\n",
       "   26647,\n",
       "   4,\n",
       "   38,\n",
       "   218,\n",
       "   75,\n",
       "   190,\n",
       "   15304,\n",
       "   7,\n",
       "   3568,\n",
       "   5582,\n",
       "   19,\n",
       "   7992,\n",
       "   9281,\n",
       "   293,\n",
       "   6,\n",
       "   25,\n",
       "   38,\n",
       "   437,\n",
       "   45,\n",
       "   667,\n",
       "   7,\n",
       "   7433,\n",
       "   5,\n",
       "   754,\n",
       "   14,\n",
       "   38,\n",
       "   524,\n",
       "   95,\n",
       "   45,\n",
       "   6764,\n",
       "   328,\n",
       "   50118,\n",
       "   243,\n",
       "   1302,\n",
       "   7,\n",
       "   162,\n",
       "   14,\n",
       "   89,\n",
       "   16,\n",
       "   10,\n",
       "   2904,\n",
       "   1567,\n",
       "   6923,\n",
       "   22,\n",
       "   20473,\n",
       "   1499,\n",
       "   113,\n",
       "   2156,\n",
       "   8,\n",
       "   14,\n",
       "   16,\n",
       "   41,\n",
       "   5631,\n",
       "   14,\n",
       "   95,\n",
       "   473,\n",
       "   45,\n",
       "   5152,\n",
       "   11,\n",
       "   2015,\n",
       "   4,\n",
       "   50118,\n",
       "   3084,\n",
       "   65,\n",
       "   16,\n",
       "   2421,\n",
       "   1969,\n",
       "   6,\n",
       "   648,\n",
       "   15829,\n",
       "   6,\n",
       "   1012,\n",
       "   924,\n",
       "   8,\n",
       "   4133,\n",
       "   1455,\n",
       "   3156,\n",
       "   9,\n",
       "   7174,\n",
       "   6,\n",
       "   6764,\n",
       "   6,\n",
       "   2721,\n",
       "   82,\n",
       "   25,\n",
       "   145,\n",
       "   5,\n",
       "   13071,\n",
       "   4,\n",
       "   1614,\n",
       "   31396,\n",
       "   13,\n",
       "   11875,\n",
       "   7059,\n",
       "   25842,\n",
       "   6,\n",
       "   4002,\n",
       "   8289,\n",
       "   8,\n",
       "   22411,\n",
       "   3012,\n",
       "   12538,\n",
       "   3300,\n",
       "   5,\n",
       "   6052,\n",
       "   9,\n",
       "   9911,\n",
       "   6,\n",
       "   617,\n",
       "   2351,\n",
       "   41,\n",
       "   1114,\n",
       "   14,\n",
       "   22,\n",
       "   20473,\n",
       "   1499,\n",
       "   113,\n",
       "   16,\n",
       "   10,\n",
       "   7404,\n",
       "   6,\n",
       "   8,\n",
       "   14,\n",
       "   24,\n",
       "   531,\n",
       "   28,\n",
       "   3584,\n",
       "   6,\n",
       "   117,\n",
       "   948,\n",
       "   99,\n",
       "   5,\n",
       "   701,\n",
       "   4,\n",
       "   96,\n",
       "   127,\n",
       "   2979,\n",
       "   6,\n",
       "   2417,\n",
       "   6,\n",
       "   1195,\n",
       "   87,\n",
       "   2772,\n",
       "   6,\n",
       "   197,\n",
       "   3094,\n",
       "   141,\n",
       "   1800,\n",
       "   10,\n",
       "   621,\n",
       "   16,\n",
       "   11,\n",
       "   39,\n",
       "   73,\n",
       "   1843,\n",
       "   4986,\n",
       "   756,\n",
       "   4,\n",
       "   2,\n",
       "   2,\n",
       "   170,\n",
       "   64,\n",
       "   216,\n",
       "   31,\n",
       "   5,\n",
       "   9078,\n",
       "   14,\n",
       "   5,\n",
       "   2730,\n",
       "   1364,\n",
       "   25,\n",
       "   10,\n",
       "   47426,\n",
       "   4439,\n",
       "   2]],\n",
       " 'attention_mask': [[1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1]],\n",
       " 'labels': 2}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置数据集格式为 PyTorch tensors (如果使用 TensorFlow 则设置为 \"tf\")\n",
    "tokenized_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里稍微介绍一下DataCollator进行padding和在tokenizer进行padding的区别：\n",
    "使用 DataCollator 进行**动态填充（Dynamic Padding）**通常比在 Tokenizer 阶段进行**全局填充（Global Padding）**更高效、更灵活。其会根据**batch内的最长长度进行填充**，而不是全局的max_length\n",
    "\n",
    "但是因为DataCollatorWithPadding是针对一个一维序列数据，即 (bs, seq_len)，而我们的数据是二维序列 -> (bs, choices, seq_len)，所以需要自定义一个DataCollator\n",
    "\n",
    "最后会介绍一下其他常用的DataCollator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# 选择DataCollatorWithPadding作为父类，修改其call的实现\n",
    "class MultipleChoiceDataCollator(DataCollatorWithPadding):\n",
    "    def __call__(self, features):\n",
    "        # 展开所有选项到批次维度\n",
    "        flattened_features = [\n",
    "            {k: v[i] for k, v in feature.items() if k != \"labels\"}\n",
    "            for feature in features\n",
    "            for i in range(len(feature[\"input_ids\"])) \n",
    "        ]\n",
    "        \n",
    "        # 使用父类call进行一维序列的填充\n",
    "        batch = super().__call__(flattened_features)\n",
    "        \n",
    "        # 恢复选项维度 (batch_size, num_choices, seq_len)\n",
    "        batch = {k: v.view(len(features), -1, v.size(-1)) for k, v in batch.items()}\n",
    "        \n",
    "        # 添加labels\n",
    "        if \"labels\" in features[0]:\n",
    "            batch[\"labels\"] = torch.tensor([f[\"labels\"] for f in features])\n",
    "            \n",
    "        return batch\n",
    "\n",
    "data_collator = MultipleChoiceDataCollator(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=\"longest\",\n",
    "    pad_to_multiple_of=8,\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[[    0, 10285,   186,  ...,     1,     1,     1],\n",
       "          [    0, 10285,   186,  ...,     1,     1,     1],\n",
       "          [    0, 10285,   186,  ...,     1,     1,     1],\n",
       "          [    0, 10285,   186,  ...,     1,     1,     1]],\n",
       " \n",
       "         [[    0, 10285,   186,  ...,     1,     1,     1],\n",
       "          [    0, 10285,   186,  ...,     1,     1,     1],\n",
       "          [    0, 10285,   186,  ...,     1,     1,     1],\n",
       "          [    0, 10285,   186,  ...,     1,     1,     1]],\n",
       " \n",
       "         [[    0, 10285,   186,  ...,     1,     1,     1],\n",
       "          [    0, 10285,   186,  ...,     1,     1,     1],\n",
       "          [    0, 10285,   186,  ...,     1,     1,     1],\n",
       "          [    0, 10285,   186,  ...,     1,     1,     1]],\n",
       " \n",
       "         [[    0, 10285,   186,  ...,     1,     1,     1],\n",
       "          [    0, 10285,   186,  ...,     1,     1,     1],\n",
       "          [    0, 10285,   186,  ...,     1,     1,     1],\n",
       "          [    0, 10285,   186,  ...,     1,     1,     1]]]),\n",
       " 'attention_mask': tensor([[[1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0]],\n",
       " \n",
       "         [[1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0]],\n",
       " \n",
       "         [[1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0]],\n",
       " \n",
       "         [[1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0]]]),\n",
       " 'labels': tensor([2, 2, 3, 1])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 传入四个数据进行查看\n",
    "batch = data_collator([tokenized_dataset['train'][i] for i in range(4)])\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  加载RoBERTa预训练模型\n",
    "加载带有适合下游任务头部的模型。对于文本分类，我们使用 AutoModelForSequenceClassification。其就是在最后加了层MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForMultipleChoice were not initialized from the model checkpoint at /data/Weights/roberta/roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaForMultipleChoice(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): RobertaPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaForMultipleChoice, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "model = RobertaForMultipleChoice.from_pretrained(model_checkpoint)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultipleChoiceModelOutput(loss=tensor(1.3860), logits=tensor([[0.1880, 0.1879, 0.1881, 0.1872]]), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "# 可以看看输出\n",
    "import torch\n",
    "\n",
    "dummy_input = tokenized_dataset['train'][0]\n",
    "for k, v in dummy_input.items():\n",
    "    dummy_input[k] = v.unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**dummy_input)\n",
    "\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  定义评估指标\n",
    "选择适合任务的评估指标。对于多项选择的QA任务，可以看做一个四分类问题，使用accuracy即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == eval_pred.label_ids).mean()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RoBERTa模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results/roberta-epoch5\",              # 输出目录，保存模型和日志\n",
    "    eval_strategy=\"epoch\",         # 每个 epoch 结束后进行评估\n",
    "    save_strategy=\"epoch\",               # 每个 epoch 结束后保存模型\n",
    "    learning_rate=2e-5,                  # 学习率\n",
    "    per_device_train_batch_size=16,      # 训练批次大小\n",
    "    per_device_eval_batch_size=16,       # 评估批次大小\n",
    "    num_train_epochs=5,                  # 训练轮数\n",
    "    weight_decay=0.01,                   # 权重衰减\n",
    "    load_best_model_at_end=True,         # 训练结束后加载最佳模型\n",
    "    metric_for_best_model=\"accuracy\",    # 以 accuracy 指标判断最佳模型 (需与 compute_metrics 返回的 key 匹配)\n",
    "    report_to=\"tensorboard\",             # 可以选择 tensorboard, wandb 等\n",
    "    save_total_limit=2,\n",
    "    label_names=[\"labels\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里吸取之前的教训，我们不进行全量微调，采用LORA微调，这里要注意使用get_peft_model后会冻结其他层，包括classifier，所以需要手动解冻一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "        r=16,                        # LoRA 的秩 (Rank)\n",
    "        lora_alpha=32,               # LoRA 缩放因子\n",
    "        lora_dropout=0.1,            # LoRA 层的 Dropout\n",
    "        target_modules=[\"query\", \"value\"], \n",
    "        bias=\"none\",                \n",
    "    )\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# 记得解冻classifier 和 pooler的梯度计算\n",
    "for name, param in model.named_parameters():\n",
    "    if 'classifier' in name or 'pooler' in name:\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-07 14:34:59,620] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda3/envs/llm/compiler_compat/ld: cannot find -lcufile: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='153' max='153' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [153/153 00:50]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3862849473953247, 'eval_accuracy': 0.2596685082872928, 'eval_runtime': 51.3462, 'eval_samples_per_second': 95.178, 'eval_steps_per_second': 2.98}\n"
     ]
    }
   ],
   "source": [
    "# 查看一下初始精度，并验证能否正常运行\n",
    "evalres = trainer.evaluate()\n",
    "print(evalres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13730' max='13730' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13730/13730 2:46:22, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.386800</td>\n",
       "      <td>1.352846</td>\n",
       "      <td>0.403520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.141900</td>\n",
       "      <td>1.051512</td>\n",
       "      <td>0.562922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.085700</td>\n",
       "      <td>0.998876</td>\n",
       "      <td>0.588909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.055000</td>\n",
       "      <td>0.973867</td>\n",
       "      <td>0.602005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.050700</td>\n",
       "      <td>0.967764</td>\n",
       "      <td>0.605689</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/data/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/data/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/data/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/data/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=13730, training_loss=1.1581273611670242, metrics={'train_runtime': 9983.9119, 'train_samples_per_second': 44.004, 'train_steps_per_second': 1.375, 'total_flos': 4.6460041954632e+17, 'train_loss': 1.1581273611670242, 'epoch': 5.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4090 * 2 训练约3h\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型推理\n",
    "\n",
    "这里使用hf 的pipeline来进行模型的推理。对于使用标准 hf 训练得到的模型权重，直接传入目录即可。如果是经过其他包装的模型需要重新定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForMultipleChoice were not initialized from the model checkpoint at /data/Weights/roberta/roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModel(\n",
      "  (base_model): LoraModel(\n",
      "    (model): RobertaForMultipleChoice(\n",
      "      (roberta): RobertaModel(\n",
      "        (embeddings): RobertaEmbeddings(\n",
      "          (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "          (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "          (token_type_embeddings): Embedding(1, 768)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (encoder): RobertaEncoder(\n",
      "          (layer): ModuleList(\n",
      "            (0-11): 12 x RobertaLayer(\n",
      "              (attention): RobertaAttention(\n",
      "                (self): RobertaSdpaSelfAttention(\n",
      "                  (query): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): RobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): RobertaIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): RobertaOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (pooler): RobertaPooler(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (activation): Tanh()\n",
      "        )\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = RobertaForMultipleChoice.from_pretrained(\"/data/Weights/roberta/roberta-base\")\n",
    "model = PeftModel.from_pretrained(model, \"./results/roberta-epoch10/checkpoint-27460\")\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merge_and_unload() 是对部分支持权重合并的peft模型，将微调的权重整合到原模型，并移除相关的peft设置\n",
    "\n",
    "支持的peft方式有：\n",
    "- LORA\n",
    "- QLORA\n",
    "- $IA^{3}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaForMultipleChoice(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): RobertaPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model= model.merge_and_unload()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于没有multiple-choices的pipeline，所以这里我们随便学习一下如何自定义pipeline\n",
    "\n",
    "使用 `Pipeline` 类创建一个问答多项选择任务的 pipeline 时，它的**主要输入**是一个或多个包含**问题 (question)** ，**上下文 (context)** 和 **选项（options）** 的字典。\n",
    "\n",
    "具体来说，输入形式：\n",
    "\n",
    "1.  **多个问答对（用于批量处理）：**\n",
    "    一个列表，列表中的每个元素都是一个如上所示的字典。\n",
    "    ```python\n",
    "    [\n",
    "        {\n",
    "            'question': '第一个问题',\n",
    "            'article': '第一个上下文',\n",
    "            'options': ['opt1','opt2','opt3','opt4']\n",
    "        },\n",
    "        {\n",
    "            'question': '第二个问题',\n",
    "            'article': '第二个上下文',\n",
    "            'options': ['opt1','opt2','opt3','opt4']\n",
    "        },\n",
    "        # ... \n",
    "    ]\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Pipeline`的参数\n",
    "\n",
    "\n",
    "| 参数名称          | 类型                                                                 | 默认值                                                              | 描述                                                                                                                                                                                             |\n",
    "| :---------------- | :------------------------------------------------------------------- | :------------------------------------------------------------------ | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| `model`           | `PreTrainedModel` or `TFPreTrainedModel`                             | 无                                                                  | 用于进行预测的模型。必须继承自 PyTorch 的 `PreTrainedModel` 或 TensorFlow 的 `TFPreTrainedModel`。                                                                                                |\n",
    "| `tokenizer`       | `PreTrainedTokenizer`                                                | 无                                                                  | 用于对模型输入数据进行编码的 Tokenizer。必须继承自 `PreTrainedTokenizer`。                                                                                                                        |\n",
    "| `feature_extractor` | `SequenceFeatureExtractor` (可选)                                    | 无                                                                  | 用于对模型输入数据进行编码的 Feature Extractor。必须继承自 `SequenceFeatureExtractor`。                                                                                                            |\n",
    "| `image_processor` | `BaseImageProcessor` (可选)                                          | 无                                                                  | 用于对模型输入图像数据进行编码的 Image Processor。必须继承自 `BaseImageProcessor`。                                                                                                                |\n",
    "| `processor`       | `ProcessorMixin` (可选)                                              | 无                                                                  | 用于对模型输入数据进行编码的 Processor。必须继承自 `ProcessorMixin`。Processor 是一个复合对象，可能包含 `tokenizer`, `feature_extractor` 和 `image_processor`。                                    |\n",
    "| `modelcard`       | `str` or `ModelCard` (可选)                                          | 可选 (无特定默认值对象)                                               | 与此 Pipeline 相关的模型卡。                                                                                                                                                                     |\n",
    "| `framework`       | `str` (可选)                                                         | 自动检测（基于安装的框架，或模型的框架，或默认为 \"pt\"）                | 要使用的框架，可以是 \"pt\" (PyTorch) 或 \"tf\" (TensorFlow)。指定的框架必须已安装。如果未指定，将根据当前安装的框架自动确定。如果安装了两个框架且未指定模型，则默认为 PyTorch。                           |\n",
    "| `task`            | `str` (可选)                                                         | `\"\"`                                                                | Pipeline 的任务标识符。                                                                                                                                                                          |\n",
    "| `num_workers`     | `int` (可选)                                                         | `8`                                                                 | 当 Pipeline 使用 DataLoader 时 (传递数据集时，或 PyTorch 模型在 GPU 上运行时)，要使用的 worker 数量。                                                                                              |\n",
    "| `batch_size`      | `int` (可选)                                                         | `1`                                                                 | 当 Pipeline 使用 DataLoader 时 (传递数据集时，或 PyTorch 模型在 GPU 上运行时)，要使用的批次大小。请阅读关于 Pipeline 批处理的文档了解更多信息。                                                    |\n",
    "| `args_parser`     | `ArgumentHandler` (可选)                                             | 无                                                                  | 负责解析提供的 Pipeline 参数的对象引用。                                                                                                                                                           |\n",
    "| `device`          | `int` or `torch.device` or `str` (可选)                              | `-1`                                                                | CPU/GPU 支持的设备序号。设置为 -1 使用 CPU，正数使用对应的 CUDA 设备 ID。您也可以传递原生的 `torch.device` 对象或字符串。                                                                            |\n",
    "| `torch_dtype`     | `str` or `torch.dtype` (可选)                                        | 无                                                                  | 直接作为 `model_kwargs` 传递 (只是一个更简单的快捷方式)，用于指定模型使用的精度 (例如 `torch.float16`, `torch.bfloat16` 或 `\"auto\"`)。                                                            |\n",
    "| `binary_output`   | `bool` (可选)                                                        | `False`                                                             | 标志，指示 Pipeline 的输出应该是序列化格式 (例如 pickle) 还是原始输出数据 (例如文本)。                                                                                                            |\n",
    "\n",
    "**总结：**\n",
    "\n",
    "*   `model`, `tokenizer`, `feature_extractor`, `image_processor`, `processor` 是 Pipeline 的核心组件，您通常需要至少提供 `model` 和一个或多个处理输入数据的组件 (`tokenizer` 用于文本，`feature_extractor`/`image_processor` 用于图像/音频等，或使用复合的 `processor`)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **创建自定义 Pipeline 的核心步骤：**\n",
    "\n",
    "自定义 Pipeline 通常涉及到继承 Hugging Face Transformers 库中的 `Pipeline` 基类，并重写其核心方法。以下是关键步骤和主要方法：\n",
    "\n",
    "1.  **重写核心方法**:\n",
    "    * **`_init_(model, tokenizer, **kwargs)`**:\n",
    "        * **作用**: 没有啥特殊的初始化要求也可以不写，如果写了记得要调用父类的 `__init__` 方法，并加载你的模型和分词器 (或其他必要的组件)。\n",
    "\n",
    "    * **`_sanitize_parameters(**kwargs)`**:\n",
    "        * **作用**: 这个方法在 `preprocess` 之前被调用。它的主要职责是验证和清理传递给 Pipeline 的参数 (`__call__` 方法的参数)。\n",
    "        * **返回**: 一个包含预处理、前向传播和后处理所需参数的字典。\n",
    "        * **示例**:\n",
    "            ```python\n",
    "            def _sanitize_parameters(self, **kwargs):\n",
    "                preprocess_kwargs = {}\n",
    "                if \"my_custom_arg\" in kwargs:\n",
    "                    preprocess_kwargs[\"my_custom_arg\"] = kwargs[\"my_custom_arg\"]\n",
    "                return preprocess_kwargs, {}, {} # (preprocess_kwargs, forward_kwargs, postprocess_kwargs)\n",
    "            ```\n",
    "\n",
    "    * **`preprocess(self, inputs, **preprocess_kwargs)`**:\n",
    "        * **作用**: 这个方法接收原始输入 (例如文本、图像路径等)，并将其转换为模型可以理解的格式 (通常是 PyTorch 或 TensorFlow 张量)。\n",
    "        * **`inputs`**: Pipeline 调用时传入的原始数据。\n",
    "        * **`**preprocess_kwargs`**: 从 `_sanitize_parameters` 返回的预处理参数。\n",
    "        * **返回**: 一个包含模型输入的字典，键通常是模型期望的输入名称 (如 `input_ids`, `attention_mask`)。\n",
    "        * **示例 (文本分类)**:\n",
    "            ```python\n",
    "            def preprocess(self, text_input, **preprocess_kwargs):\n",
    "                # 使用分词器处理文本\n",
    "                return self.tokenizer(text_input, return_tensors=self.framework, truncation=True, padding=True)\n",
    "            ```\n",
    "\n",
    "    * **`_forward(self, model_inputs, **forward_kwargs)`**:\n",
    "        * **作用**: 这个方法接收 `preprocess` 方法的输出，并将其传递给模型进行实际的推理。\n",
    "        * **`model_inputs`**: `preprocess` 方法返回的模型输入。\n",
    "        * **`**forward_kwargs`**: 从 `_sanitize_parameters` 返回的前向传播参数。\n",
    "        * **返回**: 模型的原始输出。\n",
    "        * **示例**:\n",
    "            ```python\n",
    "            def _forward(self, model_inputs, **forward_kwargs):\n",
    "                # 将输入传递给模型\n",
    "                return self.model(**model_inputs)\n",
    "            ```\n",
    "\n",
    "    * **`postprocess(self, model_outputs, **postprocess_kwargs)`**:\n",
    "        * **作用**: 这个方法接收模型的原始输出，并将其转换为用户友好的、可理解的格式。\n",
    "        * **`model_outputs`**: `_forward` 方法返回的模型输出。\n",
    "        * **`**postprocess_kwargs`**: 从 `_sanitize_parameters` 返回的后处理参数。\n",
    "        * **返回**: Pipeline 的最终输出。\n",
    "        * **示例 (文本分类，返回标签和分数)**:\n",
    "            ```python\n",
    "            def postprocess(self, model_outputs, **postprocess_kwargs):\n",
    "                logits = model_outputs.logits\n",
    "                probabilities = torch.softmax(logits, dim=-1)\n",
    "                scores = probabilities.tolist()[0] # 假设批处理大小为1\n",
    "                # 假设你有 id 到标签的映射 self.model.config.id2label\n",
    "                results = []\n",
    "                for i, score in enumerate(scores):\n",
    "                    results.append({\"label\": self.model.config.id2label[i], \"score\": score})\n",
    "                return sorted(results, key=lambda x: x[\"score\"], reverse=True) # 按分数排序\n",
    "            ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Pipeline\n",
    "device = \"cuda:0\"\n",
    "\n",
    "# 简单化，默认都是4个选项\n",
    "\n",
    "class MultipleChoicesPipeline(Pipeline):\n",
    "    def __init__(self, model, tokenizer = None, feature_extractor = None, image_processor = None, processor = None, modelcard = None, framework = None, task = \"\", args_parser = None, device = None, torch_dtype = None, binary_output = False, **kwargs):\n",
    "        self.labels = None\n",
    "        self.options = None\n",
    "        super().__init__(model, tokenizer, feature_extractor, image_processor, processor, modelcard, framework, task, args_parser, device, torch_dtype, binary_output, **kwargs)\n",
    "    \n",
    "    def _sanitize_parameters(self, **kwargs):\n",
    "        \"\"\"\n",
    "        我们没有自定义参数，直接返回空字典。\n",
    "        \"\"\"\n",
    "        return {}, {}, {}\n",
    "    \n",
    "    # 参照之前数据集处理的写法即可\n",
    "    def preprocess(self, input_, **preprocess_parameters):\n",
    "        \n",
    "        if isinstance(input_, dict):\n",
    "            input_ = [input_]\n",
    "            \n",
    "        article = list(map(lambda x:x[\"article\"], input_))\n",
    "        question = list(map(lambda x:x[\"question\"], input_))\n",
    "        options = list(map(lambda x:x[\"options\"], input_))\n",
    "        answer = list(map(lambda x:x[\"answer\"], input_))\n",
    "        \n",
    "        self.labels = [ord(l)-ord('A') for l in answer]\n",
    "        self.options = options\n",
    "        \n",
    "        context = [[article_] * 4 for article_ in article]\n",
    "        question_choice = [\n",
    "                [f\"{ques} {options[i][j]}\" for j in range(4)] \n",
    "                for i, ques in enumerate(question)\n",
    "            ]\n",
    "        \n",
    "        \n",
    "        context = sum(context, [])                  #len = (len(instances) * 4)\n",
    "        question_choice = sum(question_choice, []) \n",
    "        \n",
    "        tokenized_instances = tokenizer(\n",
    "                                    context,\n",
    "                                    question_choice,\n",
    "                                    max_length=512,\n",
    "                                    padding=True,            \n",
    "                                    truncation=\"only_first\",  # 优先截断第一个序列(文章)\n",
    "                                    return_tensors=\"pt\",\n",
    "                                    )\n",
    "        \n",
    "        # 因为后续没有datacollator，所以这里要将输出都变为tensor\n",
    "        results = {\n",
    "                k: torch.stack([v[i:i+4] for i in range(0, len(v), 4)])\n",
    "                for k, v in tokenized_instances.items()\n",
    "                }\n",
    "        return results\n",
    "    \n",
    "    def _forward(self, model_inputs, **forward_kwargs):\n",
    "                return self.model(**model_inputs)\n",
    "    \n",
    "    def postprocess(self, model_outputs, **postprocess_parameters):\n",
    "        \n",
    "        logits = model_outputs.logits\n",
    "        scores = torch.softmax(logits, dim=-1)\n",
    "        \n",
    "        batch_size = logits.shape[0]\n",
    "        results = []\n",
    "        for i in range(batch_size):\n",
    "            question_scores = scores[i] # Shape (num_options,)\n",
    "\n",
    "            max_score = torch.max(question_scores).item()\n",
    "            predicted_index = torch.argmax(question_scores).item()\n",
    "            label = self.labels[i]\n",
    "\n",
    "            original_options = self.options[i]\n",
    "            predicted_option_text = original_options[predicted_index] \n",
    "\n",
    "            result = {\n",
    "                \"predicted_option_index\": predicted_index,\n",
    "                \"label\": label,\n",
    "                \"predicted_option_text\": predicted_option_text,\n",
    "                \"score\": max_score,\n",
    "            }\n",
    "\n",
    "            results.append(result)\n",
    "        return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[{'predicted_option_index': 0,\n",
       "   'label': 2,\n",
       "   'predicted_option_text': 'Measure the depth of the river',\n",
       "   'score': 0.29531028866767883}],\n",
       " [{'predicted_option_index': 3,\n",
       "   'label': 3,\n",
       "   'predicted_option_text': 'Nancy took hold of the rope and climbed into the helicopter.',\n",
       "   'score': 0.28053829073905945}],\n",
       " [{'predicted_option_index': 1,\n",
       "   'label': 0,\n",
       "   'predicted_option_text': 'They used helicopters to help carry cows.',\n",
       "   'score': 0.2699262797832489}],\n",
       " [{'predicted_option_index': 0,\n",
       "   'label': 1,\n",
       "   'predicted_option_text': 'our values and lifestyles are in no field of human activity',\n",
       "   'score': 0.28568899631500244}]]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = MultipleChoicesPipeline(model, tokenizer)\n",
    "\n",
    "input_ = [raw_dataset[\"test\"][i] for i in range(4)]\n",
    "res = pipeline(input_)\n",
    "res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataCollator\n",
    "\n",
    "| **Data Collator 类型**               | **主要用途**                     | **核心功能**                                                                 | **关键参数**                                                                 | **适用任务举例**                                                                 |\n",
    "|--------------------------------------|----------------------------------|-----------------------------------------------------------------------------|-----------------------------------------------------------------------------|--------------------------------------------------------------------------------|\n",
    "| **DataCollatorWithPadding**          | 通用序列填充                     | 动态填充 `input_ids`, `attention_mask` 等，使批次内序列长度一致                          | `tokenizer`（必需）, `padding=\"longest\"`, `pad_to_multiple_of`               | 文本分类、抽取式问答、多项选择任务                                                  |\n",
    "| **DataCollatorForTokenClassification** | 词元级分类任务                   | 填充 `input_ids` 和 `labels`，并用 `-100` 忽略填充位置的损失计算                         | `tokenizer`, `label_pad_token_id=-100`                                      | 命名实体识别 (NER)、词性标注 (POS)                                                 |\n",
    "| **DataCollatorForSeq2Seq**           | 序列到序列任务                   | 分别填充编码器输入和解码器标签，自动生成 `decoder_input_ids`（需提供 `model`）               | `tokenizer`, `model`（推荐）, `label_pad_token_id=-100`                     | 机器翻译、文本摘要、生成式问答                                                     |\n",
    "| **DataCollatorForLanguageModeling**  | 语言模型预训练                   | 动态执行掩码语言建模 (MLM)，随机替换 15% 的 token 并生成 `labels`                          | `tokenizer`, `mlm=True`, `mlm_probability=0.15`                             | BERT/RoBERTa 预训练、领域自适应训练                                                 |\n",
    "| **DataCollatorForWholeWordMask**     | 全词掩码预训练                   | 类似 `DataCollatorForLanguageModeling`，但以整个词为单位进行掩码                             | 同 `DataCollatorForLanguageModeling`                                        | 需要更高质量掩码的预训练任务                                                        |\n",
    "| **DefaultDataCollator**              | 通用默认处理                     | 仅将数据转换为张量，不进行填充或特殊处理（需提前确保数据形状一致）                               | 无特殊参数                                                                   | 非序列数据（如图像特征）、已手动处理填充的数据                                         |\n",
    "\n",
    "\n",
    "### **关键注意事项**\n",
    "1. **动态填充效率**：优先使用 `padding=\"longest\"`（而非 `max_length`），避免不必要的计算浪费。\n",
    "2. **模型参数传递**：对 Seq2Seq 任务，务必向 `DataCollatorForSeq2Seq` 传递 `model` 以正确处理 `decoder_input_ids`。\n",
    "3. **标签填充值**：分类任务中 `-100` 是默认的忽略索引（PyTorch 的 `CrossEntropyLoss` 会自动跳过）。\n",
    "4. **调试技巧**：通过 `print(batch.keys())` 和 `print(batch[\"input_ids\"].shape)` 检查批次格式是否符合预期。\n",
    "\n",
    "根据任务类型选择匹配的 Collator，能显著减少预处理代码的复杂性并提升训练效率。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
